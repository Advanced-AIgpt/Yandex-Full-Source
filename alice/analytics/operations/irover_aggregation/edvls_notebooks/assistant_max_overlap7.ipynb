{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils.word_transition_network as wtn_module\n",
    "from utils.rover import RoverVotingScheme\n",
    "from utils.word_transition_network import *\n",
    "\n",
    "from collections import Counter\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yt.wrapper as yt\n",
    "yt.config.set_proxy(\"hahn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AggregationResult = collections.namedtuple('AggregationResult', 'text confidence cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_prod(raw_data):\n",
    "    \"\"\"\n",
    "       aggregation from prod\n",
    "    \"\"\"\n",
    "    cost = 2\n",
    "    while cost < 5:\n",
    "        cost += 1\n",
    "        answers = [(x[\"text\"], x[\"speech\"]) for x in raw_data[:cost]]\n",
    "        answers = Counter(answers)\n",
    "        if answers.most_common(1)[0][1] >= 3:\n",
    "            break\n",
    "\n",
    "    texts = Counter()\n",
    "    speechs = Counter()\n",
    "    for text, speech in [(x[\"text\"], x[\"speech\"]) for x in raw_data[:cost]]:\n",
    "        if speech != \"BAD\" and text:\n",
    "            text = text.lower().replace('ё', 'е')\n",
    "        else:\n",
    "            text = \"\"\n",
    "        speechs.update([speech])\n",
    "        texts.update([text])\n",
    "    \n",
    "    \n",
    "    text, text_rate = max(texts.items(), key=lambda x: (x[1], x[0] != \"\"))\n",
    "    if text != \"\" and text_rate >= 2:\n",
    "        conf = text_rate * 1.0 / sum(texts.values())\n",
    "    else:\n",
    "        text = None\n",
    "        conf = 0\n",
    "    common = speechs.most_common(2)\n",
    "    speech, speech_rate = common[0]\n",
    "    if speech == \"BAD\" and len(common) >= 2 and common[1][1] == speech_rate:\n",
    "        speech = common[1][0]\n",
    "\n",
    "    # conf = text_rate / sum(texts.values())\n",
    "    return AggregationResult(text, conf, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(data, field, algorithm, treshhold=0, cluster_refernces=None, print_=True):\n",
    "    errors = 0\n",
    "    total_length = 0\n",
    "    aggregated = 0\n",
    "    total_items = 0\n",
    "    correct = 0\n",
    "    cost = 0\n",
    "    words_count = 0\n",
    "    for row in data:\n",
    "        if row[\"mark\"] != \"TEST\":\n",
    "            continue\n",
    "        total_items += 1\n",
    "        hyp = algorithm(sorted(row[field], key=lambda x: x[\"submit_ts\"]))\n",
    "        cost += hyp.cost\n",
    "        if (hyp.text is None) or (hyp.confidence < treshhold):\n",
    "            continue\n",
    "        hyp = hyp.text\n",
    "        aggregated += 1\n",
    "        words_count += len(row[\"text\"].split())\n",
    "        _, e, l = calculate_wer(row[\"text\"], hyp, cluster_refernces)\n",
    "        errors += e\n",
    "        if e == 0:\n",
    "            correct += 1\n",
    "        total_length += l\n",
    "\n",
    "    accuracy = correct / aggregated\n",
    "    wer = errors / total_length\n",
    "    aggregated_part = aggregated / total_items\n",
    "    cost = cost / total_items\n",
    "    if print_:\n",
    "        print(\"Aggregated: {:.4%}\\nWER: {:.4%}\\nAccuracy: {:.4%}\\nMean overlap: {:.4}\".format(\n",
    "            aggregated_part, wer, accuracy, cost\n",
    "        ))\n",
    "    return aggregated_part, wer, accuracy, cost, words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.resources import ClusterReference\n",
    "\n",
    "crf = ClusterReference(\"../linguistics/cluster_references/ru-RU/cluster_references.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_rover(raw_data, rover_class=RoverVotingScheme, treshhold=0.51, cluster_references=None):\n",
    "    \"\"\"\n",
    "       aggregation from prod\n",
    "    \"\"\"\n",
    "    cost = 2\n",
    "    while cost < 5:\n",
    "        cost += 1\n",
    "        answers = [(x[\"text\"], x[\"speech\"]) for x in raw_data[:cost]]\n",
    "        answers = Counter(answers)\n",
    "        if answers.most_common(1)[0][1] >= 3:\n",
    "            break\n",
    "\n",
    "    hyps = []\n",
    "    for text, speech, worker_id in [(x[\"text\"], x[\"speech\"], x[\"worker_id\"]) for x in raw_data[:cost]]:\n",
    "        if speech != \"BAD\" and text:\n",
    "            text = text.lower().replace('ё', 'е')\n",
    "        else:\n",
    "            text = \"\"\n",
    "        hyps.append(TextHyp(\"1\", worker_id, text))\n",
    "    rover = rover_class(\"1\", hyps, cluster_references)\n",
    "    rover_result = rover.get_result()\n",
    "    text = \" \".join(value for value, score in rover_result if value != \"\")\n",
    "    if text == \"\":\n",
    "        conf = 0\n",
    "        text = None\n",
    "    elif any(score < treshhold for value, score in rover_result):\n",
    "        conf = 0\n",
    "        text = None\n",
    "    else:\n",
    "        conf = 1\n",
    "    \n",
    "    return AggregationResult(text, conf, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RoverVotingSchemeEmptyDecreased(WordTransitionNetwork):\n",
    "    def get_result(self):\n",
    "        result = []\n",
    "        empty_correction = 0.66\n",
    "        for edges in self.edges:\n",
    "            score, _, value = max((len(set(x.sources)) * (1 if x.value else empty_correction), \n",
    "                                   len(x.value), \n",
    "                                   x.value) for x in edges.values())\n",
    "            score = float(score)\n",
    "            if value == \"\":\n",
    "                score /= empty_correction\n",
    "            score /= sum(len(set(x.sources)) for x in edges.values())\n",
    "            result.append((value, score))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NUM_ANSWERS = 7\n",
    "NO_SOURCE_ID = \"NO WORKER\"\n",
    "\n",
    "\n",
    "def extract_prefeatures(data, field_name):\n",
    "    prefeatures = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(dict)))\n",
    "    for row in data:\n",
    "        toloka_answers = sorted(row[field_name], key=lambda x: x[\"submit_ts\"])\n",
    "        id_ = row[\"mds_key\"]\n",
    "        for cost in range(1, len(toloka_answers) + 1):\n",
    "            hyps = []\n",
    "            for text, speech, worker_id in [(x[\"text\"], x[\"speech\"], x[\"worker_id\"]) for x in toloka_answers[:cost]]:\n",
    "                if speech != \"BAD\" and text:\n",
    "                    text = text.lower().replace('ё', 'е')\n",
    "                else:\n",
    "                    text = \"\"\n",
    "                hyps.append(TextHyp(id_, worker_id, text))\n",
    "            wtn = WordTransitionNetwork(object_id=id_, hypotheses=hyps)\n",
    "            ref_text = row.get(\"text\", \"\")\n",
    "            ref = WordTransitionNetwork(object_id=id_, hypotheses=[TextHyp(id_, \"reference\", ref_text)])\n",
    "            alignment, actions = wtn._align(wtn.edges, ref.edges, wtn.hypotheses_sources, ref.hypotheses_sources)\n",
    "            expanded_alignment = []  # формируем дополнительные ребра на местах где не было вставок\n",
    "            skip_next = False\n",
    "            for item, action in zip(alignment, actions): \n",
    "                if action == \"I\":\n",
    "                    if skip_next:\n",
    "                        continue  # берем только первую вставку из нескольких так как они индентичны\n",
    "                    expanded_alignment.append(item)\n",
    "                    skip_next = True\n",
    "                elif skip_next:\n",
    "                    expanded_alignment.append(item)\n",
    "                    skip_next = False\n",
    "                else:\n",
    "                    expanded_alignment += [\n",
    "                        {\"\": WTNEdge(\"\", \n",
    "                                     None, \n",
    "                                     wtn.hypotheses_sources + [\"reference\"], \n",
    "                                     [None for _ in wtn.hypotheses_sources + [\"reference\"]])}, item\n",
    "                    ]\n",
    "            if not skip_next:\n",
    "                expanded_alignment.append(\n",
    "                    {\n",
    "                        \"\": WTNEdge(\n",
    "                            \"\", \n",
    "                            None, \n",
    "                            wtn.hypotheses_sources + [\"reference\"], \n",
    "                            [None for _ in wtn.hypotheses_sources + [\"reference\"]]\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "            for position, edges in enumerate(expanded_alignment):\n",
    "                correct_word = None\n",
    "                fixed_edges = {}\n",
    "                for word, edge in edges.items():\n",
    "                    if \"reference\" in edge.sources:\n",
    "                        edge = WTNEdge(edge[0], edge[1], edge[2][:-1], edge[3][:-1])\n",
    "                        assert \"reference\" not in edge.sources\n",
    "                        assert correct_word is None\n",
    "                        correct_word = edge.value\n",
    "                    if len(edge.sources) != 0:\n",
    "                        fixed_edges[word] = edge\n",
    "                assert correct_word is not None\n",
    "                all_submissions = [(edge.value, edge.score, source, original_position)\n",
    "                                   for edge in fixed_edges.values() \n",
    "                                   for source, original_position in zip(edge.sources, edge.original_positions) \n",
    "                                   ]\n",
    "                for word in fixed_edges:\n",
    "                    submissions_sorted = sorted(\n",
    "                        (calculate_wer(list(word), list(sub[0]))[1], *sub) for sub in all_submissions\n",
    "                    )\n",
    "                    prefeatures[id_][cost][position][word] = (submissions_sorted, word == correct_word)\n",
    "    return prefeatures    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "eng_letter = re.compile(\"[a-zA-Z]\")\n",
    "\n",
    "def count_eng_letters(text):\n",
    "    x = re.findall(eng_letter, text)\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_features(word):\n",
    "    if word is None:\n",
    "        return [-100] * 3\n",
    "    result = [\n",
    "        len(word), count_eng_letters(word), len(word) - count_eng_letters(word)\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "def get_word_to_word_features(word, hyp_word):\n",
    "    if word is None or hyp_word is None:\n",
    "        return [-100] * 3\n",
    "    result = [\n",
    "        calculate_wer(list(word), list(hyp_word))[1], \n",
    "        len(word) - len(hyp_word),\n",
    "        count_eng_letters(word) - count_eng_letters(hyp_word)\n",
    "    ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_task_features(row):\n",
    "    mds_key = row[\"mds_key\"]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_source_features(data, field_name): # , predresult)\n",
    "    N_FEATURES = 8\n",
    "    sources_stats = collections.defaultdict(lambda: np.zeros(N_FEATURES, dtype=np.int32))\n",
    "    for row in data:\n",
    "        mds_key = row[\"mds_key\"]\n",
    "        texts = []\n",
    "        for cost, assignment in enumerate(sorted(row[field_name], key=lambda x: x[\"submit_ts\"])):\n",
    "            cost += 1\n",
    "            source_id = assignment[\"worker_id\"]\n",
    "            text = assignment[\"text\"]\n",
    "            texts.append(text)\n",
    "            speech = assignment[\"speech\"]\n",
    "            if speech != \"BAD\" and text:\n",
    "                text = text.lower().replace('ё', 'е')\n",
    "            else:\n",
    "                text = \"\"\n",
    "            number_of_speakers = assignment[\"number_of_speakers\"]\n",
    "            submit_ts = assignment[\"submit_ts\"]\n",
    "            for i in range(cost, MAX_NUM_ANSWERS + 1):\n",
    "                sources_stats[(source_id, i)] += np.array([\n",
    "                                                      1, \n",
    "                                                      text == \"\", \n",
    "                                                      len(text.split()), \n",
    "                                                      len(text), \n",
    "                                                      number_of_speakers==\"many\", \n",
    "                                                      count_eng_letters(text),\n",
    "                                                      count_eng_letters(text) != 0,\n",
    "                                                      sum(1 for x in texts if x == text)\n",
    "                                                     ])\n",
    "    sources_stats[NO_SOURCE_ID] = np.full(fill_value=-1, shape=N_FEATURES)\n",
    "    for i, value in sources_stats.items():\n",
    "        n = value[0]\n",
    "        value = np.concatenate((\n",
    "            np.array([n], dtype=np.float),\n",
    "            value[1:] / n\n",
    "        ))\n",
    "        sources_stats[i] = list(value)\n",
    "    for i in range(1, MAX_NUM_ANSWERS + 1):\n",
    "        sources_stats[(NO_SOURCE_ID, i)] = [-1] * N_FEATURES\n",
    "    return dict(sources_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_eng_words(text):\n",
    "    return sum(1 for word in text.split() if count_eng_letters(word) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_task_and_source_features(data, field_name):\n",
    "    stats = dict()\n",
    "    for row in data:\n",
    "        mds_key = row[\"mds_key\"]\n",
    "        assignments = sorted(row[field_name], key=lambda x: x[\"submit_ts\"])\n",
    "        for cost in range(1, len(assignments) + 1):\n",
    "            texts = []\n",
    "            for assignment in assignments[:cost]:\n",
    "                text = assignment[\"text\"]\n",
    "                speech = assignment[\"speech\"]\n",
    "                if speech != \"BAD\" and text:\n",
    "                    text = text.lower().replace('ё', 'е')\n",
    "                else:\n",
    "                    text = \"\"\n",
    "                texts.append(text)\n",
    "            for assignment, text in zip(assignments[:cost], texts):\n",
    "                source_id = assignment[\"worker_id\"]\n",
    "                speech = assignment[\"speech\"]\n",
    "                number_of_speakers = assignment[\"number_of_speakers\"]\n",
    "                submit_ts = assignment[\"submit_ts\"]\n",
    "                stats[(mds_key, source_id, cost)] = [\n",
    "                    len(text.split()), \n",
    "                    len(text), \n",
    "                    number_of_speakers == \"many\", \n",
    "                    count_eng_letters(text),\n",
    "                    count_eng_words(text),\n",
    "                    sum(1 for x in texts if x == text)\n",
    "                ]\n",
    "        \n",
    "        for i in range(1, MAX_NUM_ANSWERS + 1):\n",
    "            stats[(mds_key, NO_SOURCE_ID, i)] = [\n",
    "                    -1, \n",
    "                    -1, \n",
    "                    -1, \n",
    "                    -1,\n",
    "                    -1,\n",
    "                    -1\n",
    "                ]\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_task_and_source_word_features(data, field_name):\n",
    "    N_FEATURES = 5\n",
    "    stats = dict()\n",
    "    for row in data:\n",
    "        mds_key = row[\"mds_key\"]\n",
    "        for assignment in row[field_name]:\n",
    "            source_id = assignment[\"worker_id\"]\n",
    "            text = assignment[\"text\"]\n",
    "            speech = assignment[\"speech\"]\n",
    "            if speech != \"BAD\" and text:\n",
    "                text = text.lower().replace('ё', 'е')\n",
    "            else:\n",
    "                text = \"\"\n",
    "            number_of_speakers = assignment[\"number_of_speakers\"]\n",
    "            submit_ts = assignment[\"submit_ts\"]\n",
    "            text = text.split()\n",
    "            text_len = len(text)\n",
    "            for pos, word in enumerate(text):\n",
    "                stats[(mds_key, source_id, pos)] = [\n",
    "                    pos, \n",
    "                    len(text[pos-1]) if pos > 0 else -1, \n",
    "                    len(text[pos+1]) if pos + 1 < text_len else -1,\n",
    "                    count_eng_letters(text[pos-1]) if pos > 0 else -1, \n",
    "                    count_eng_letters(text[pos+1]) if pos + 1 < text_len else -1\n",
    "                ]\n",
    "            stats[(mds_key, source_id, None)] = [-1] * N_FEATURES\n",
    "        stats[(mds_key, NO_SOURCE_ID, None)] = [-1] * N_FEATURES\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(data, field_name):\n",
    "    task_features = dict((row[\"mds_key\"], extract_task_features(row)) for row in data)\n",
    "    source_features = extract_source_features(data, field_name)\n",
    "    task_and_source_features = extract_task_and_source_features(data, field_name)\n",
    "    task_and_source_word_features = extract_task_and_source_word_features(data, field_name)\n",
    "    \n",
    "    features = extract_prefeatures(data, field_name)\n",
    "    for task_id, item0 in features.items():\n",
    "        for cost, item1 in item0.items():\n",
    "            for position, item2 in item1.items():\n",
    "                for word, item3 in item2.items():\n",
    "                    prefeature, y = item3\n",
    "                    prefeature += [(-1, None, None, NO_SOURCE_ID, None) for _ in range(MAX_NUM_ANSWERS - len(prefeature))]\n",
    "                    if len(prefeature) != MAX_NUM_ANSWERS:\n",
    "                        print(prefeature)\n",
    "                        assert False\n",
    "                    x = [position] + task_features[task_id] + get_word_features(word) \n",
    "                    for edit_distance, hyp_word, score, source_id, original_position in prefeature:\n",
    "                        x += get_word_features(hyp_word)\n",
    "                        x += get_word_to_word_features(word, hyp_word)\n",
    "                        x += source_features[(source_id, cost)]\n",
    "                        x += task_and_source_features[(task_id, source_id, cost)]\n",
    "                        x += task_and_source_word_features[(task_id, source_id, original_position)]\n",
    "                    features[task_id][cost][position][word] = (x, y)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_array(features):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for task_id, item0 in features.items():\n",
    "        for cost, item1 in item0.items():\n",
    "            for position, item2 in item1.items():\n",
    "                for word, item3 in item2.items():\n",
    "                    x, y = item3\n",
    "                    X.append(np.array(x))\n",
    "                    Y.append(1 if y else -1)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preaggregate_with_clf(features, clf):\n",
    "    matching = dict()\n",
    "    X = list()\n",
    "    for task_id, item0 in features.items():\n",
    "        for cost, item1 in item0.items():\n",
    "            for position, item2 in item1.items():\n",
    "                for word, item3 in item2.items():\n",
    "                    x, y = item3\n",
    "                    X.append(np.array(x))\n",
    "                    matching[(task_id, cost, position, word)] = len(X) - 1\n",
    "    probabilites = clf.predict_proba(X)[:, 1]\n",
    "    results = dict()\n",
    "    for task_id, item0 in features.items():\n",
    "        results[task_id] = dict()\n",
    "        for cost, item1 in sorted(item0.items()):\n",
    "            results[task_id][cost] = list()\n",
    "            for position, item2 in sorted(item1.items()):\n",
    "                words = list(item2.keys())\n",
    "                probs = np.array([probabilites[matching[(task_id, cost, position, word)]] for word in words])\n",
    "                pos = np.argmax(probs)\n",
    "                score = probs[pos]\n",
    "                word = words[pos]\n",
    "                results[task_id][cost].append((word, score))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics_from_dict(data, aggregation_results, treshhold=0, cluster_refernces=None, print_=True):\n",
    "    errors = 0\n",
    "    total_length = 0\n",
    "    aggregated = 0\n",
    "    total_items = 0\n",
    "    correct = 0\n",
    "    cost = 0\n",
    "    words_count = 0\n",
    "    false_empty = 0.0\n",
    "    \n",
    "    for row in data:\n",
    "        if row[\"mark\"] != \"TEST\":\n",
    "            continue\n",
    "        total_items += 1\n",
    "        hyp = aggregation_results[row[\"mds_key\"]]\n",
    "        cost += hyp.cost\n",
    "        if (hyp.text is None) or (hyp.confidence < treshhold):\n",
    "            continue\n",
    "        hyp = hyp.text\n",
    "        aggregated += 1\n",
    "        words_count += len(row[\"text\"].split())\n",
    "        _, e, l = calculate_wer(row[\"text\"], hyp, cluster_refernces)\n",
    "        errors += e\n",
    "        if e == 0:\n",
    "            correct += 1\n",
    "        total_length += l\n",
    "        if (row['text'] != '' and hyp == ''):\n",
    "            false_empty += 1\n",
    "\n",
    "    accuracy = correct / aggregated\n",
    "    wer = errors / total_length\n",
    "    aggregated_part = aggregated / total_items\n",
    "    cost = cost / total_items\n",
    "    false_empty /= aggregated\n",
    "    if print_:\n",
    "        print(\"Aggregated: {:.4%}\\nWER: {:.4%}\\nAccuracy: {:.4%}\\nMean overlap: {:.4}\".format(\n",
    "            aggregated_part, wer, accuracy, cost\n",
    "        ))\n",
    "    return aggregated_part, wer, accuracy, cost, words_count, false_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_rover_with_clf_(rows, features, field_name, clf, treshhold=0.51, min_cost=3, max_cost=10, cluster_references=None):\n",
    "    \"\"\"\n",
    "       aggregation from prod\n",
    "    \"\"\"\n",
    "    clf_results = preaggregate_with_clf(features, clf)\n",
    "    results = dict()\n",
    "    for task_id, item0 in clf_results.items():\n",
    "        for cost, item1 in sorted(item0.items()):\n",
    "            if cost < min_cost or cost > max_cost: \n",
    "                continue\n",
    "            text = \" \".join(value for value, score in item1 if value != \"\")\n",
    "            score = sum(score for value, score in item1) / len(item1)\n",
    "            if score >= treshhold or cost == max_cost: #MAX_NUM_ANSWERS:\n",
    "                if text == \"\":\n",
    "                    conf = 0\n",
    "                    text = None\n",
    "                elif score < treshhold:\n",
    "                    conf = 0\n",
    "                    text = None\n",
    "                else:\n",
    "                    conf = score\n",
    "                results[task_id] = AggregationResult(text, conf, cost)\n",
    "                break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics_clf_(data, \n",
    "                          features, \n",
    "                          field_name, \n",
    "                          clf, \n",
    "                          treshhold=0.51, \n",
    "                          min_cost=3, \n",
    "                          max_cost=10, \n",
    "                          cluster_refernces=None, \n",
    "                          print_=True):\n",
    "    aggregation_results = aggregate_rover_with_clf_(data, \n",
    "                                                    features, \n",
    "                                                    field_name, \n",
    "                                                    clf, \n",
    "                                                    treshhold, \n",
    "                                                    min_cost, \n",
    "                                                    max_cost, \n",
    "                                                    cluster_refernces)\n",
    "    return evaluate_metrics_from_dict(data, aggregation_results, treshhold, cluster_refernces, print_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_assistant_train = yt.read_table(\"//home/voice/vklyukin/va-893/assistant/train\")\n",
    "data_assistant_test = yt.read_table(\"//home/voice/vklyukin/va-893/assistant/test\")\n",
    "data_assistant_valid = list(data_assistant_train) + list(data_assistant_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7022"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_assistant_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toloka_results\n",
      "Prod: \n",
      "Aggregated: 67.9862%\n",
      "WER: 7.2800%\n",
      "Accuracy: 79.0295%\n",
      "Mean overlap: 3.833\n",
      "\n",
      "ROVER:\n",
      "Aggregated: 69.7074%\n",
      "WER: 9.9717%\n",
      "Accuracy: 74.3621%\n",
      "Mean overlap: 3.833\n"
     ]
    }
   ],
   "source": [
    "print('toloka_results')\n",
    "print('Prod: ')\n",
    "prod_agg_metrics = evaluate_metrics(data_assistant_valid, 'toloka_results', aggregate_prod)\n",
    "print('\\nROVER:')\n",
    "rover_agg_metrics = evaluate_metrics(data_assistant_valid, 'toloka_results', aggregate_rover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_assistant_train = yt.read_table(\"//home/voice/vklyukin/va-893/assistant/train\")\n",
    "data_assistant_test = yt.read_table(\"//home/voice/vklyukin/va-893/assistant/test\")\n",
    "data_assistant_train = list(data_assistant_train)\n",
    "data_assistant_test = list(data_assistant_test)\n",
    "for row in data_assistant_test:\n",
    "    if 'text' in row:\n",
    "        del row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '', None, '05616ccdbce241371a9bad9171d9134d', None), (0, '', None, '3e008d251f2b59f172e73de044f6a68e', None), (0, '', None, '8e131452720305e11e6c6038dac334d7', None), (0, '', None, '93b85c02189eb869dee4b6fff82d9b2e', None), (0, '', None, 'a0341f19580095f77aacf480c1094922', None), (0, '', None, 'c8107540e8e65d02069a5bd58a7bcfa2', None), (0, '', None, 'cb7ed9cc9638ce1c5d00388e6e81702f', None), (0, '', None, 'fc1a19a37ff03d137bf4464f8055f805', None)]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-569b27cba80e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_assistant_new_test_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_assistant_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toloka_results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-b95c6e4405f1>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(data, field_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefeature\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAX_NUM_ANSWERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                         \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtask_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_word_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0medit_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_position\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprefeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_assistant_new_test_features = extract_features(data_assistant_test, 'toloka_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_assistant_new_train_features = extract_features(data_assistant_train, 'toloka_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_assistant_test_valid = yt.read_table(\"//home/voice/vklyukin/va-893/assistant/test\")\n",
    "data_assistant_test_valid = list(data_assistant_test_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('toloka_results')\n",
    "print('Prod: ')\n",
    "prod_agg_metrics = evaluate_metrics(data_assistant_test_valid, 'toloka_results', aggregate_prod)\n",
    "print('\\nROVER:')\n",
    "rover_agg_metrics = evaluate_metrics(data_assistant_test_valid, 'toloka_results', aggregate_rover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = convert_to_array(data_assistant_new_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_new = CatBoostClassifier(iterations=19000, depth=6, thread_count=26, random_seed=42)\n",
    "clf_new.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf_assistant_new_max_cost_7 = []\n",
    "\n",
    "for treshhold in np.linspace(0.7, 0.98, 29):\n",
    "    print(treshhold)\n",
    "    metrics = evaluate_metrics_clf_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    treshhold,\n",
    "                                    max_cost=7)\n",
    "    scores_clf_assistant_new_max_cost_7.append((treshhold, *metrics))\n",
    "scores_clf_assistant_new_max_cost_7 = np.array(scores_clf_assistant_new_max_cost_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.suptitle('Зависимость метрик от порога, разметка с перекрытием 10')\n",
    "plt.subplot(221)\n",
    "plt.plot(scores_clf_assistant_new_max_cost_7[:, 0], \n",
    "         scores_clf_assistant_new_max_cost_7[:, 1] * 100, \n",
    "         \".-\", \n",
    "         label=\"clf, макс. пер. 7\")\n",
    "plt.axhline(y=prod_agg_metrics[0] * 100, color=u'#1f77b4', linestyle='--', label=\"MV\")\n",
    "plt.axhline(y=rover_agg_metrics[0] * 100, color=u'#ff7f0e', linestyle=':', label=\"ROVER\")\n",
    "plt.legend()\n",
    "plt.ylabel('Агрегированая часть (%)')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(scores_clf_assistant_new_max_cost_7[:, 0], \n",
    "         scores_clf_assistant_new_max_cost_7[:, 2] * 100, \n",
    "         \".-\")\n",
    "plt.axhline(y=prod_agg_metrics[1] * 100, color=u'#1f77b4', linestyle='--')\n",
    "plt.axhline(y=rover_agg_metrics[1] * 100, color=u'#ff7f0e', linestyle=':')\n",
    "\n",
    "plt.ylabel('WER (%)')\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(scores_clf_assistant_new_max_cost_7[:, 0], \n",
    "         scores_clf_assistant_new_max_cost_7[:, 3] * 100, \n",
    "         \".-\")\n",
    "plt.axhline(y=prod_agg_metrics[2] * 100, color=u'#1f77b4', linestyle='--')\n",
    "plt.axhline(y=rover_agg_metrics[2] * 100, color=u'#ff7f0e', linestyle=':')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "plt.xlabel('Порог')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(scores_clf_assistant_new_max_cost_7[:, 0], \n",
    "         scores_clf_assistant_new_max_cost_7[:, 4], \n",
    "         \".-\")\n",
    "plt.axhline(y=prod_agg_metrics[3], color=u'#1f77b4', linestyle='--')\n",
    "plt.axhline(y=rover_agg_metrics[3], color=u'#ff7f0e', linestyle=':')\n",
    "plt.ylabel('Среднее перекрытие')\n",
    "\n",
    "plt.xlabel('Порог')\n",
    "\n",
    "plt.savefig(\"metrics_clf_new_assistant_true.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_assistant_test_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def err(value, n, conf=0.95):\n",
    "    z = norm.ppf(conf)\n",
    "    return z * np.sqrt(value * (1 - value) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.247070761033969"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9.72 - err(0.0972, 10615) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.486682835339472"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9.04 + err(0.0904, 11150) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf_assistant_new_max_cost_7[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_words_count = [\n",
    "    len(row['text'].split()) for row in data_assistant_test_valid\n",
    "]\n",
    "test_words_count = sum(test_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 0.95\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.suptitle('Зависимость метрик от порога с доверительными интервалами, уровень доверия {}'.format(conf))\n",
    "plt.subplot(311)\n",
    "plt.plot(scores_clf_assistant_new_max_cost_7[:, 0], \n",
    "         scores_clf_assistant_new_max_cost_7[:, 1] * 100, \n",
    "         \".-\", \n",
    "         label=\"clf, макс. пер. 7\",\n",
    "         color=u'#ff7f0e')\n",
    "plt.errorbar(scores_clf_assistant_new_max_cost_7[:, 0],\n",
    "            scores_clf_assistant_new_max_cost_7[:, 1] * 100,\n",
    "            [err(val, len(data_assistant_test_valid), conf=conf) * 100 for val in\n",
    "            scores_clf_assistant_new_max_cost_7[:, 1]],\n",
    "            color=u'#ff7f0e',\n",
    "            linestyle=\"None\")\n",
    "plt.axhline(y=prod_agg_metrics[0] * 100, color=u'#1f77b4', linestyle='--', label=\"MV\")\n",
    "plt.errorbar(scores_clf_assistant_new_max_cost_7[:, 0],\n",
    "            [prod_agg_metrics[0] * 100] * len(scores_clf_assistant_new_max_cost_7[:, 0]),\n",
    "            [err(val, len(data_assistant_test_valid), conf=conf) * 100 for val in \n",
    "            [prod_agg_metrics[0]] * len(scores_clf_assistant_new_max_cost_7[:, 0])],\n",
    "            color=u'#1f77b4',\n",
    "            linestyle=\"None\")\n",
    "plt.ylabel('Агрегированая часть (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(scores_clf_assistant_new_max_cost_7[:, 0], \n",
    "         scores_clf_assistant_new_max_cost_7[:, 2] * 100, \n",
    "         \".-\",\n",
    "         color=u'#ff7f0e')\n",
    "plt.errorbar(scores_clf_assistant_new_max_cost_7[:, 0],\n",
    "            scores_clf_assistant_new_max_cost_7[:, 2] * 100,\n",
    "            [err(val[2], val[5], conf=conf) * 100 for val in\n",
    "            scores_clf_assistant_new_max_cost_7[:]],\n",
    "            color=u'#ff7f0e',\n",
    "            linestyle=\"None\")\n",
    "plt.axhline(y=prod_agg_metrics[1] * 100, color=u'#1f77b4', linestyle='--')\n",
    "plt.errorbar(scores_clf_assistant_new_max_cost_7[:, 0],\n",
    "            [prod_agg_metrics[1] * 100] * len(scores_clf_assistant_new_max_cost_7[:, 0]),\n",
    "            [err(val, prod_agg_metrics[4], conf=conf) * 100 for val in \n",
    "            [prod_agg_metrics[1]] * len(scores_clf_assistant_new_max_cost_7[:, 0])],\n",
    "            color=u'#1f77b4',\n",
    "            linestyle=\"None\")\n",
    "plt.grid(True)\n",
    "plt.ylabel('WER (%)')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(scores_clf_assistant_new_max_cost_7[:, 0], \n",
    "         scores_clf_assistant_new_max_cost_7[:, 3] * 100, \n",
    "         \".-\",\n",
    "         color=u'#ff7f0e')\n",
    "plt.errorbar(scores_clf_assistant_new_max_cost_7[:, 0],\n",
    "            scores_clf_assistant_new_max_cost_7[:, 3] * 100,\n",
    "            [err(metrics[3], metrics[1] * len(data_assistant_test_valid), conf=conf) * 100 for metrics in\n",
    "            scores_clf_assistant_new_max_cost_7[:]],\n",
    "            color=u'#ff7f0e',\n",
    "            linestyle=\"None\")\n",
    "plt.axhline(y=prod_agg_metrics[2] * 100, color=u'#1f77b4', linestyle='--')\n",
    "plt.errorbar(scores_clf_assistant_new_max_cost_7[:, 0],\n",
    "            [prod_agg_metrics[2] * 100] * len(scores_clf_assistant_new_max_cost_7[:, 0]),\n",
    "            yerr=[err(val, prod_agg_metrics[0] * len(data_assistant_test_valid), conf=conf) * 100 for val in \n",
    "            [prod_agg_metrics[2]] * len(scores_clf_assistant_new_max_cost_7[:, 0])],\n",
    "            color=u'#1f77b4',\n",
    "            linestyle=\"None\")\n",
    "plt.grid(True)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "plt.xlabel('Порог')\n",
    "\n",
    "plt.savefig(\"metrics_conf_int_assistant_true.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf_assistant_new_max_cost_7 = []\n",
    "\n",
    "for treshhold in np.linspace(0.7, 0.98, 29):\n",
    "    print(treshhold)\n",
    "    metrics = evaluate_metrics_clf_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    treshhold,\n",
    "                                    max_cost=7)\n",
    "    scores_clf_assistant_new_max_cost_7.append((treshhold, *metrics))\n",
    "scores_clf_assistant_new_max_cost_7 = np.array(scores_clf_assistant_new_max_cost_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('toloka_results')\n",
    "print('Prod: ')\n",
    "prod_agg_metrics = evaluate_metrics(data_assistant_test_valid, 'toloka_results', aggregate_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_rover_with_clf_and_empty_(rows, features, field_name, clf, treshhold=0.51, min_cost=3, max_cost=10, cluster_references=None):\n",
    "    \"\"\"\n",
    "       aggregation from prod\n",
    "    \"\"\"\n",
    "    clf_results = preaggregate_with_clf(features, clf)\n",
    "    results = dict()\n",
    "    for task_id, item0 in clf_results.items():\n",
    "        for cost, item1 in sorted(item0.items()):\n",
    "            if cost < min_cost or cost > max_cost: \n",
    "                continue\n",
    "            text = \" \".join(value for value, score in item1 if value != \"\")\n",
    "            score = sum(score for value, score in item1) / len(item1)\n",
    "            if score >= treshhold or cost == max_cost: #MAX_NUM_ANSWERS:\n",
    "                if score < treshhold:\n",
    "                    conf = 0\n",
    "                    text = None\n",
    "                else:\n",
    "                    conf = score\n",
    "                results[task_id] = AggregationResult(text, conf, cost)\n",
    "                break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics_from_dict(data, aggregation_results, treshhold=0, cluster_refernces=None, print_=True):\n",
    "    errors = 0\n",
    "    total_length = 0\n",
    "    aggregated = 0\n",
    "    total_items = 0\n",
    "    correct = 0\n",
    "    cost = 0\n",
    "    words_count = 0\n",
    "    aggregated_empty = 0\n",
    "    false_empty = 0.0\n",
    "    \n",
    "    for row in data:\n",
    "        if row[\"mark\"] != \"TEST\":\n",
    "            continue\n",
    "        total_items += 1\n",
    "        hyp = aggregation_results[row[\"mds_key\"]]\n",
    "        cost += hyp.cost\n",
    "        if (hyp.text is None) or (hyp.confidence < treshhold):\n",
    "            continue\n",
    "        hyp = hyp.text\n",
    "        aggregated += 1\n",
    "        words_count += len(row[\"text\"].split())\n",
    "        _, e, l = calculate_wer(row[\"text\"], hyp, cluster_refernces)\n",
    "        errors += e\n",
    "        if e == 0:\n",
    "            correct += 1\n",
    "        total_length += l\n",
    "        if hyp == '':\n",
    "            aggregated_empty += 1\n",
    "            if row['text'] != '':\n",
    "                false_empty += 1\n",
    "\n",
    "    accuracy = correct / aggregated\n",
    "    wer = errors / total_length\n",
    "    aggregated_part = aggregated / total_items\n",
    "    cost = cost / total_items\n",
    "    if aggregated_empty:\n",
    "        false_empty /= aggregated_empty\n",
    "    if print_:\n",
    "        print(\"Aggregated: {:.4%}\\nWER: {:.4%}\\nAccuracy: {:.4%}\\nMean overlap: {:.4}\\nEmpty error part: {:.4%}\".format(\n",
    "            aggregated_part, wer, accuracy, cost, false_empty\n",
    "        ))\n",
    "    return aggregated_part, wer, accuracy, cost, words_count, false_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics_clf_and_empty_(data, \n",
    "                          features, \n",
    "                          field_name, \n",
    "                          clf, \n",
    "                          treshhold=0.51, \n",
    "                          min_cost=3, \n",
    "                          max_cost=10, \n",
    "                          cluster_refernces=None, \n",
    "                          print_=True):\n",
    "    aggregation_results = aggregate_rover_with_clf_and_empty_(data, \n",
    "                                                    features, \n",
    "                                                    field_name, \n",
    "                                                    clf, \n",
    "                                                    treshhold, \n",
    "                                                    min_cost, \n",
    "                                                    max_cost, \n",
    "                                                    cluster_refernces)\n",
    "    return evaluate_metrics_from_dict(data, aggregation_results, treshhold, cluster_refernces, print_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_and_empty_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.92,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.92,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_and_empty_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.93,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.93,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_and_empty_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.94,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.94,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.95,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics_clf_(data_assistant_test_valid, \n",
    "                                    data_assistant_new_test_features, \n",
    "                                    'toloka_results', \n",
    "                                    clf_new, \n",
    "                                    0.948,\n",
    "                                    max_cost=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_new.save_model(\"assistant_2019-09-23_prototype_overlap7.clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36",
   "language": "python",
   "name": "py_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
