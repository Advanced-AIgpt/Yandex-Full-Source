{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../rover_based_aggregation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.word_transition_network as wtn_module\n",
    "from utils.rover import RoverVotingScheme\n",
    "from utils.word_transition_network import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt.wrapper as yt\n",
    "yt.config.set_proxy(\"hahn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = yt.read_table(\n",
    "    \"//home/voice/edvls/tickets/VA-442_ideal_testsets/assistant_ideal_annotations_2019-02-16__2019-02-25\"\n",
    ")\n",
    "data_table = list(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggregationResult = collections.namedtuple('AggregationResult', 'text confidence cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_prod(raw_data):\n",
    "    \"\"\"\n",
    "       aggregation from prod\n",
    "    \"\"\"\n",
    "    cost = 2\n",
    "    while cost < 5:\n",
    "        cost += 1\n",
    "        answers = [(x[\"text\"], x[\"speech\"]) for x in raw_data[:cost]]\n",
    "        answers = Counter(answers)\n",
    "        if answers.most_common(1)[0][1] >= 3:\n",
    "            break\n",
    "\n",
    "    texts = Counter()\n",
    "    speechs = Counter()\n",
    "    for text, speech in [(x[\"text\"], x[\"speech\"]) for x in raw_data[:cost]]:\n",
    "        if speech != \"BAD\" and text:\n",
    "            text = text.lower().replace('ё', 'е')\n",
    "        else:\n",
    "            text = \"\"\n",
    "        speechs.update([speech])\n",
    "        texts.update([text])\n",
    "    \n",
    "    \n",
    "    text, text_rate = max(texts.items(), key=lambda x: (x[1], x[0] != \"\"))\n",
    "    if text != \"\" and text_rate >= 2:\n",
    "        conf = text_rate * 1.0 / sum(texts.values())\n",
    "    else:\n",
    "        text = None\n",
    "        conf = 0\n",
    "    common = speechs.most_common(2)\n",
    "    speech, speech_rate = common[0]\n",
    "    if speech == \"BAD\" and len(common) >= 2 and common[1][1] == speech_rate:\n",
    "        speech = common[1][0]\n",
    "\n",
    "    # conf = text_rate / sum(texts.values())\n",
    "    return AggregationResult(text, conf, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(data, field, algorithm, treshhold=0, cluster_refernces=None, print_=True):\n",
    "    errors = 0\n",
    "    total_length = 0\n",
    "    aggregated = 0\n",
    "    total_items = 0\n",
    "    correct = 0\n",
    "    cost = 0\n",
    "    not_aggregated_correct=0\n",
    "    aggregated_correct=[]\n",
    "    not_aggregated_incorrect=0\n",
    "    aggregated_incorrect=[]\n",
    "    agg_incorrect_hyps=[]\n",
    "    \n",
    "    for row in data:\n",
    "        if row[\"mark\"] != \"TEST\":\n",
    "            continue\n",
    "        total_items += 1\n",
    "        \n",
    "        hyp = algorithm(sorted(row[field], key=lambda x: x[\"submit_ts\"]))\n",
    "        cost += hyp.cost\n",
    "        \n",
    "        if (hyp.text is None) or (hyp.confidence < treshhold):\n",
    "            if row['text'] in [obj['text'] for obj in row[field]]:\n",
    "                not_aggregated_correct+=1\n",
    "            else:\n",
    "                not_aggregated_incorrect+=1\n",
    "            continue\n",
    "            \n",
    "        hyp = hyp.text\n",
    "        aggregated += 1\n",
    "        _, e, l = calculate_wer(row[\"text\"], hyp, cluster_refernces)\n",
    "        errors += e\n",
    "        if e == 0:\n",
    "            correct += 1\n",
    "        elif row['text'] in [obj['text'] for obj in row[field]]:\n",
    "            agg_incorrect_hyps.append(hyp)\n",
    "            aggregated_incorrect.append(row)\n",
    "        \n",
    "        total_length += l\n",
    "\n",
    "    accuracy = correct / aggregated\n",
    "    wer = errors / total_length\n",
    "    aggregated_part = aggregated / total_items\n",
    "    cost = cost / total_items\n",
    "    if print_:\n",
    "        print(\"Aggregated: {:.4%}\\nWER: {:.4%}\\nAccuracy: {:.4%}\\nMean overlap: {:.4}\".format(\n",
    "            aggregated_part, wer, accuracy, cost\n",
    "        ))\n",
    "    return aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech\n",
      "toloka_assignments_repeat_8_selected_workers_with_chorus_and_pitch\n",
      "audio\n",
      "toloka_text\n",
      "date\n",
      "number_of_speakers\n",
      "toloka_assignments\n",
      "toloka_assignments_repeat_4_with_bend\n",
      "toloka_assignments_repeat_6_with_chorus\n",
      "_other\n",
      "linguists_sugested_text\n",
      "toloka_assignments_repeat_10\n",
      "raw_text_linguists\n",
      "toloka_assignments_repeat_9_selected_workers_with_chorus_and_pitch\n",
      "check_in_yang_results\n",
      "toloka_assignments_repeat_5_with_chorus_and_pitch\n",
      "url\n",
      "toloka_assignments_repeat_3_with_pitch\n",
      "toloka_assignments_repeat_11_selected_workers_with_pitch\n",
      "yang_assignments_repeat_1\n",
      "mark\n",
      "mds_key\n",
      "toloka_number_of_speakers\n",
      "text\n",
      "toloka_assignments_repeat_2_with_pitch\n",
      "toloka_assignments_repeat_7_with_chorus_and_pitch\n",
      "toloka_speech\n",
      "linguists_worker_id\n",
      "toloka_assignments_repeat_1\n",
      "linguists_comment\n"
     ]
    }
   ],
   "source": [
    "for i in data_table[0].keys():\n",
    "    print(i)\n",
    "col_name='toloka_assignments_repeat_11_selected_workers_with_pitch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: ~production method quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 67.9862%\n",
      "WER: 7.2892%\n",
      "Accuracy: 78.9873%\n",
      "Mean overlap: 3.833\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table, col_name, aggregate_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below saves all errors (aggregated,but incorrect) of the production method in the following format: linguist answer, selected hypothesis, list of all annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "with open('errors.txt','w+',encoding='utf8') as f:\n",
    "    hyps_list=np.array(list(zip(aggregated_incorrect,agg_incorrect_hyps)))\n",
    "    np.random.shuffle(hyps_list)\n",
    "    for i,(row,hyp) in enumerate(hyps_list):\n",
    "        print(i,file=f)\n",
    "        print(row['text'],file=f)\n",
    "        print('---',file=f)\n",
    "        print(hyp,file=f)\n",
    "        print('--',file=f)\n",
    "        for annot in sorted(row[col_name],key=lambda x:x[\"submit_ts\"]):\n",
    "            print(annot['text'],file=f)\n",
    "        print('',file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel,delayed,effective_n_jobs\n",
    "from functools import partial\n",
    "from tqdm import tqdm_notebook\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def fix_misspellings(text):\n",
    "    headers = {\n",
    "        'Connection': 'close'\n",
    "    }\n",
    "    params={\n",
    "        'text':text,\n",
    "        'lang':'ru',\n",
    "        'options':'512',\n",
    "    }\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=10, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "    s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    try:\n",
    "        resp=s.get('https://speller.yandex.net/services/spellservice.json/checkText',params=params,headers=headers)\n",
    "        resp=resp.json()\n",
    "    except ValueError:\n",
    "        print(resp)\n",
    "    if not resp:\n",
    "        return text\n",
    "    first_pos=0\n",
    "    new_text=[]\n",
    "    for fix in resp:\n",
    "        new_text.append(text[first_pos:fix['pos']].strip())\n",
    "        new_text.append(fix['s'][0].strip())\n",
    "        first_pos=fix['pos']+fix['len']\n",
    "    new_text.append(text[first_pos:].strip())\n",
    "    return ' '.join(new_text).strip()\n",
    "\n",
    "def process_entry(entry):\n",
    "    new_entry={'text':entry['text'],'mark':entry['mark']}\n",
    "    new_entry[col_name]=[]\n",
    "    for elem in entry[col_name]:\n",
    "        elem['text']=fix_misspellings(elem['text'].lower().replace('ё', 'е'))\n",
    "        new_entry[col_name].append(elem)\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker skills\n",
    "Valya's idea: we can weigh answers based on worker skill, which can be estimated from the training part of the dataset by computing average value of some metric (accuracy, WER, etc.). Here I chose WER for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "def compute_worker_skills(data_table,column,metric):\n",
    "    worker_stats=defaultdict(list)\n",
    "    for entry in data_table:\n",
    "        if entry['mark']!='TEST':\n",
    "            for annot in entry[column]:\n",
    "                if 'speech'!='BAD' and annot['text'] is not None:\n",
    "                    worker_stats[annot['worker_id']].append(metric(entry['text'],annot['text']))\n",
    "    skill_for_worker={worker_id:np.mean(stats) for worker_id,stats in worker_stats.items()}\n",
    "    l=list(worker_stats.values())\n",
    "    average_skill=np.mean(np.concatenate(l))\n",
    "    return skill_for_worker,average_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_metric(ref,hyp):\n",
    "    r,e,l=calculate_wer(ref,hyp)\n",
    "    return 1-r if r is not None else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixes for the production method:\n",
    "1. Use skills (defined above), divide by max for normalization (apparently works best)\n",
    "2. Use more than 5 annotations (all of them, if we can); confidence threshold is adaptive to number of annotations\n",
    "3. Expand question marks as \"wildcard symbols\": if a sentence without question marks matches a sentence with question marks, where each \"?\" gets replaced by 0-3 words, it gains 0.5 of a vote. We also remove \"?\"'s and add 0.1 to the scores of resulting sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def aggregate_prod_fixed(raw_data,num_annot=10):\n",
    "    for overlap in range(3,num_annot+1):\n",
    "        fixed_text_speech=[elem['text'].lower().replace('ё', 'е') for elem in raw_data[:overlap]]\n",
    "        worker_ids=[elem['worker_id'] for elem in raw_data[:overlap]]\n",
    "        text_without_quest_mark=[elem for elem in fixed_text_speech if '?' not in elem]\n",
    "        text_with_quest_mark=[elem for elem in fixed_text_speech if '?' in elem]\n",
    "        texts = Counter()\n",
    "\n",
    "        for text in text_without_quest_mark:\n",
    "            texts[text]+=1\n",
    "        for text_q in text_with_quest_mark:\n",
    "            pattern=re.compile(text_q.replace('?','(\\w+){0,3}?'))\n",
    "            for text in text_without_quest_mark:\n",
    "                if re.match(pattern,text):\n",
    "                    texts[text]+=0.3\n",
    "        for text_q in text_with_quest_mark:\n",
    "            clean=' '.join(text_q.replace('?','').split())\n",
    "            texts[clean]+=0.1\n",
    "        text, text_rate = max(texts.items(), key=lambda x: x[1])\n",
    "        if text != \"\" and text_rate >= 0.5*overlap:\n",
    "            conf = text_rate * 1.0 / sum(texts.values())\n",
    "            used = overlap\n",
    "            break\n",
    "    else:\n",
    "        if text != \"\" and text_rate >= 0.375*overlap+0.1:\n",
    "            conf = text_rate * 1.0 / sum(texts.values())\n",
    "        else:\n",
    "            text = None\n",
    "            conf = 0\n",
    "        used = num_annot\n",
    "    return AggregationResult(text, conf, used)\n",
    "\n",
    "def aggregate_prod_fixed_with_skills(raw_data,skills,num_annot=10):\n",
    "    for overlap in range(3,num_annot+1):\n",
    "        fixed_text_speech=[elem['text'].lower().replace('ё', 'е') for elem in raw_data[:overlap]]\n",
    "        worker_ids=[elem['worker_id'] for elem in raw_data[:overlap]]\n",
    "\n",
    "        per_worker,average=skills\n",
    "        skills_for_workers=[per_worker.get(elem['worker_id'],average) for elem in raw_data[:overlap]]\n",
    "        \n",
    "        text_without_quest_mark=[(text,skill) for text,skill in zip(fixed_text_speech,skills_for_workers) if '?' not in text]\n",
    "        text_with_quest_mark=[(text,skill) for text,skill in zip(fixed_text_speech,skills_for_workers) if '?' in text]\n",
    "        texts = Counter()\n",
    "        \n",
    "        for text,skill in text_without_quest_mark:\n",
    "            texts[text]+=1*skill\n",
    "        for text_q,skill in text_with_quest_mark:\n",
    "            pattern=re.compile(text_q.replace('?','(\\w+){0,3}?'))\n",
    "            for text,skill_other in text_without_quest_mark:\n",
    "                if re.match(pattern,text):\n",
    "                    texts[text]+=0.3*skill\n",
    "        for text_q,skill in text_with_quest_mark:\n",
    "            clean=' '.join(text_q.replace('?','').split())\n",
    "            texts[clean]+=0.1*skill\n",
    "        text, text_rate = max(texts.items(), key=lambda x: x[1])\n",
    "        if text != \"\" and text_rate >= 0.5*overlap:\n",
    "            conf = text_rate * 1.0 / sum(texts.values())\n",
    "            used = overlap\n",
    "            break\n",
    "    else:\n",
    "        if text != \"\" and text_rate >= 0.375*overlap:\n",
    "            conf = text_rate * 1.0 / sum(texts.values())\n",
    "        else:\n",
    "            text = None\n",
    "            conf = 0\n",
    "        used = num_annot\n",
    "    return AggregationResult(text, conf, used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small ablation study: we show impact of spelling correction, using skills, accounting for question marks, using all annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 67.9862%\n",
      "WER: 7.2892%\n",
      "Accuracy: 78.9873%\n",
      "Mean overlap: 3.833\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table,\n",
    "                     col_name,\n",
    "                     aggregate_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 66.4372%\n",
      "WER: 5.5237%\n",
      "Accuracy: 82.8584%\n",
      "Mean overlap: 5.691\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table,\n",
    "                     col_name,\n",
    "                     aggregate_prod_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ This one actually yields the best results, which shows that spelling correction might be far from perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 64.7447%\n",
      "WER: 5.6927%\n",
      "Accuracy: 82.4546%\n",
      "Mean overlap: 3.855\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table,\n",
    "                     col_name,\n",
    "                     partial(aggregate_prod_fixed,num_annot=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using skills actually gives slightly worse results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skill_for_worker_wer,average_skill_wer=compute_worker_skills(data_table,col_name,wer_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 62.3924%\n",
      "WER: 4.7223%\n",
      "Accuracy: 85.4253%\n",
      "Mean overlap: 6.006\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table,\n",
    "                     col_name,\n",
    "                     partial(aggregate_prod_fixed_with_skills,skills=(skill_for_worker_wer,average_skill_wer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 58.9214%\n",
      "WER: 4.7108%\n",
      "Accuracy: 85.8325%\n",
      "Mean overlap: 3.928\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table,\n",
    "                     col_name,\n",
    "                     partial(aggregate_prod_fixed_with_skills,skills=(skill_for_worker_wer,average_skill_wer),num_annot=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction\n",
    "We try to fix misspellings and orthographic errors by feeding annotations from Toloka to Speller API (NB: it does not always work, which explains some errors). We also normalize texts before feeding them to Counter and not after (something that could be fixed in the production method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893289c363944c7698a02427fc36a073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7022), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_table_with_spell_corr=Parallel(n_jobs=effective_n_jobs(),batch_size=10)(delayed(process_entry)(entry) for entry in tqdm_notebook(data_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 67.4412%\n",
      "WER: 5.8910%\n",
      "Accuracy: 81.4547%\n",
      "Mean overlap: 5.591\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table_with_spell_corr,\n",
    "                     col_name,\n",
    "                     aggregate_prod_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the impact of transliteration, we use a very quick&dirty solution and map all words to Cyrillic characters. Nevertheless, it shows that a certain part of the metric can be explained and improved by consistency in writing (and perhaps rephrased worker/linguist task explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1daec48fe2548bba020f3e139f71aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7022), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def translit_to_cyr(entry):\n",
    "    new_entry={'text':transliterate.translit(entry['text'],'ru'),'mark':entry['mark']}\n",
    "    new_entry[col_name]=[]\n",
    "    for elem in entry[col_name]:\n",
    "        elem['text']=transliterate.translit(elem['text'],'ru')\n",
    "        new_entry[col_name].append(elem)\n",
    "    return new_entry\n",
    "data_table_transl=Parallel(effective_n_jobs())(delayed(translit_to_cyr)(entry) for entry in tqdm_notebook(data_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 65.0316%\n",
      "WER: 5.6310%\n",
      "Accuracy: 82.7525%\n",
      "Mean overlap: 3.845\n"
     ]
    }
   ],
   "source": [
    "aggregated_part, wer, accuracy, cost,aggregated_incorrect,agg_incorrect_hyps=\\\n",
    "    evaluate_metrics(data_table_transl,\n",
    "                     col_name,\n",
    "                     partial(aggregate_prod_fixed,num_annot=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "with open('errors_fixed_alg.txt','w+',encoding='utf8') as f:\n",
    "    hyps_list=np.array(list(zip(aggregated_incorrect,agg_incorrect_hyps)))\n",
    "    np.random.shuffle(hyps_list)\n",
    "    for i,(row,hyp) in enumerate(hyps_list):\n",
    "        print(i,file=f)\n",
    "        print(row['text'],file=f)\n",
    "        print('---',file=f)\n",
    "        print(hyp,file=f)\n",
    "        print('--',file=f)\n",
    "        for annot in sorted(row[col_name],key=lambda x:x[\"submit_ts\"]):\n",
    "            print(annot['text'],file=f)\n",
    "        print('',file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
