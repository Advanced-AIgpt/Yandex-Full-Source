{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../rover_based_aggregation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.word_transition_network as wtn_module\n",
    "from utils.rover import RoverVotingScheme\n",
    "from utils.word_transition_network import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt.wrapper as yt\n",
    "yt.config.set_proxy(\"hahn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = yt.read_table(\n",
    "    \"//home/voice/edvls/tickets/VA-442_ideal_testsets/assistant_ideal_annotations_2019-02-16__2019-02-25\"\n",
    ")\n",
    "data_table = list(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggregationResult = collections.namedtuple('AggregationResult', 'text confidence cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_prod(raw_data):\n",
    "    \"\"\"\n",
    "       aggregation from prod\n",
    "    \"\"\"\n",
    "    cost = 2\n",
    "    while cost < 5:\n",
    "        cost += 1\n",
    "        answers = [(x[\"text\"], x[\"speech\"]) for x in raw_data[:cost]]\n",
    "        answers = Counter(answers)\n",
    "        if answers.most_common(1)[0][1] >= 3:\n",
    "            break\n",
    "\n",
    "    texts = Counter()\n",
    "    speechs = Counter()\n",
    "    for text, speech in [(x[\"text\"], x[\"speech\"]) for x in raw_data[:cost]]:\n",
    "        if speech != \"BAD\" and text:\n",
    "            text = text.lower().replace('ё', 'е')\n",
    "        else:\n",
    "            text = \"\"\n",
    "        speechs.update([speech])\n",
    "        texts.update([text])\n",
    "    \n",
    "    \n",
    "    text, text_rate = max(texts.items(), key=lambda x: (x[1], x[0] != \"\"))\n",
    "    if text != \"\" and text_rate >= 2:\n",
    "        conf = text_rate * 1.0 / sum(texts.values())\n",
    "    else:\n",
    "        text = None\n",
    "        conf = 0\n",
    "    common = speechs.most_common(2)\n",
    "    speech, speech_rate = common[0]\n",
    "    if speech == \"BAD\" and len(common) >= 2 and common[1][1] == speech_rate:\n",
    "        speech = common[1][0]\n",
    "\n",
    "    # conf = text_rate / sum(texts.values())\n",
    "    return AggregationResult(text, conf, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(data, field, algorithm, treshhold=0, cluster_refernces=None, print_=True):\n",
    "    errors = 0\n",
    "    total_length = 0\n",
    "    aggregated = 0\n",
    "    total_items = 0\n",
    "    correct = 0\n",
    "    cost = 0\n",
    "    for row in data:\n",
    "        if row[\"mark\"] != \"TEST\":\n",
    "            continue\n",
    "        total_items += 1\n",
    "        hyp = algorithm(sorted(row[field], key=lambda x: x[\"submit_ts\"]))\n",
    "        cost += hyp.cost\n",
    "        if (hyp.text is None) or (hyp.confidence < treshhold):\n",
    "            continue\n",
    "        hyp = hyp.text\n",
    "        aggregated += 1\n",
    "        _, e, l = calculate_wer(row[\"text\"], hyp, cluster_refernces)\n",
    "        errors += e\n",
    "        if e == 0:\n",
    "            correct += 1\n",
    "        total_length += l\n",
    "\n",
    "    accuracy = correct / aggregated\n",
    "    wer = errors / total_length\n",
    "    aggregated_part = aggregated / total_items\n",
    "    cost = cost / total_items\n",
    "    if print_:\n",
    "        print(\"Aggregated: {:.4%}\\nWER: {:.4%}\\nAccuracy: {:.4%}\\nMean overlap: {:.4}\".format(\n",
    "            aggregated_part, wer, accuracy, cost\n",
    "        ))\n",
    "    return aggregated_part, wer, accuracy, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "linguists_comment\n",
      "yang_assignments_repeat_1\n",
      "audio\n",
      "toloka_assignments_repeat_10\n",
      "toloka_assignments_repeat_1\n",
      "toloka_assignments_repeat_11_selected_workers_with_pitch\n",
      "toloka_speech\n",
      "toloka_assignments_repeat_3_with_pitch\n",
      "mark\n",
      "_other\n",
      "linguists_sugested_text\n",
      "toloka_assignments_repeat_7_with_chorus_and_pitch\n",
      "linguists_worker_id\n",
      "toloka_assignments\n",
      "speech\n",
      "url\n",
      "toloka_assignments_repeat_2_with_pitch\n",
      "toloka_assignments_repeat_6_with_chorus\n",
      "toloka_assignments_repeat_4_with_bend\n",
      "date\n",
      "mds_key\n",
      "check_in_yang_results\n",
      "raw_text_linguists\n",
      "toloka_assignments_repeat_9_selected_workers_with_chorus_and_pitch\n",
      "toloka_assignments_repeat_8_selected_workers_with_chorus_and_pitch\n",
      "toloka_assignments_repeat_5_with_chorus_and_pitch\n",
      "toloka_number_of_speakers\n",
      "number_of_speakers\n",
      "toloka_text\n"
     ]
    }
   ],
   "source": [
    "for i in data_table[0].keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 67.9862%\n",
      "WER: 7.3166%\n",
      "Accuracy: 78.9030%\n",
      "Mean overlap: 3.833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6798623063683304,\n",
       " 0.07316627034936894,\n",
       " 0.7890295358649789,\n",
       " 3.8330464716006887)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_metrics(data_table, 'toloka_assignments_repeat_11_selected_workers_with_pitch', aggregate_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AggregationResult(text='алиса что такое форма три справка', confidence=1.0, cost=3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_prod(data_table[2]['toloka_assignments_repeat_11_selected_workers_with_pitch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f3e2e1590a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfasttext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_fasttext_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'toloka_logs/fasttext_output.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "fasttext_model=gensim.models.FastText.load_fasttext_format('fasttext_output.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_centroid(raw_data):\n",
    "    annot_topk=[a['text'] for a in raw_data]\n",
    "    centroid_scores=[]\n",
    "    if len(set(annot_topk))==1:\n",
    "        text=annot_topk[0]\n",
    "    else:\n",
    "        lengths=[len(set(elem for elem in annot_topk if elem!=cur)) for cur in annot_topk]\n",
    "        for annot,length in zip(annot_topk,lengths):\n",
    "            centroid_scores.append(np.sum([fasttext_model.wv.wmdistance([annot],[other_annot]) for other_annot in annot_topk])/length)\n",
    "        text=annot_topk[np.argmin(centroid_scores)]\n",
    "    conf=1\n",
    "    if text=='':\n",
    "        text=None\n",
    "        conf=0\n",
    "    return AggregationResult(text,conf,len(annot_topk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated: 75.7889%\n",
      "WER: 9.1768%\n",
      "Accuracy: 74.7918%\n",
      "Mean overlap: 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7578886976477338, 0.09176768052353684, 0.7479182437547313, 10.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_metrics(data_table, 'toloka_assignments_repeat_11_selected_workers_with_pitch', aggregate_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_ideal=[]\n",
    "with open('assistant_ideal.jsonl',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        assistant_ideal.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasar_ideal=[]\n",
    "with open('quasar_ideal.jsonl',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        quasar_ideal.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_bound(logs):\n",
    "    n=[]\n",
    "    for sample in logs:\n",
    "        true=sample['text']\n",
    "        annot=[a['text'] for a in sample['annotations']]\n",
    "        n.append(int(true in annot))\n",
    "    return np.mean(n),np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_quality(logs,topk=5):\n",
    "    n=[]\n",
    "    pre_df={'golden_ans':[],'mv_ans':[]}\n",
    "    for sample in logs:\n",
    "        true=sample['text']\n",
    "        annot=[a['text'] for a in sample['annotations']]\n",
    "        cnt=Counter(annot[:topk])#.most_common(1)[0]\n",
    "        mv=max(cnt,key=lambda k:cnt[k])\n",
    "        n.append(int(mv==true))\n",
    "        pre_df['golden_ans'].append(true)\n",
    "        pre_df['mv_ans'].append(mv)\n",
    "    return np.mean(n),np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8958986043862147, 6291)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_bound(assistant_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7022"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(assistant_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,sum_=mv_quality(assistant_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7569068641412703, 5315)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8175824175824176, 5952)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_quality(quasar_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7851039589860439, 5513)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_quality(assistant_ideal,topk=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8368131868131868, 6092)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_quality(quasar_ideal,topk=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_method_quality(logs):\n",
    "    n=[]\n",
    "    for sample in logs:\n",
    "        true=sample['text']\n",
    "        annot=[a['text'] for a in sample['annotations']]\n",
    "        mv=Counter(annot[:3]).most_common(1)[0]\n",
    "        if mv[1]>1:\n",
    "            n.append(int(mv[0]==true))\n",
    "        else:\n",
    "            mv=Counter(annot[:4]).most_common(1)[0]\n",
    "            if mv[1]>1: # (2,2) is impossible\n",
    "                n.append(int(mv[0]==true))\n",
    "            else:\n",
    "                mv=Counter(annot[:5]).most_common(1)[0]\n",
    "                if mv[1]>1:\n",
    "                    n.append(int(mv[0]==true))\n",
    "                else:\n",
    "                    n.append(int(annot[0]==true))\n",
    "    return np.mean(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7529193961834235"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_method_quality(assistant_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8108516483516484"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_method_quality(quasar_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_quality(logs,topk=5):\n",
    "    n=[]\n",
    "    for sample in logs:\n",
    "        true=sample['text']\n",
    "        annot_topk=[a['text'] for a in sample['annotations']][:topk]\n",
    "        centroid_scores=[]\n",
    "        for annot in annot_topk:\n",
    "            centroid_scores.append(np.mean([fasttext_model.wv.wmdistance([annot],[other_annot]) for other_annot in annot_topk]))\n",
    "        n.append(int(annot_topk[np.argmin(centroid_scores)]==true))\n",
    "    return np.mean(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785246368555967"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality(assistant_ideal,topk=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8368131868131868"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality(quasar_ideal,topk=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7634577043577329"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality(assistant_ideal,topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.817032967032967"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality(quasar_ideal,topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314155511250356"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality(assistant_ideal,topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7953296703296703"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality(quasar_ideal,topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_quality_median(logs):\n",
    "    n=[]\n",
    "    for sample in logs:\n",
    "        true=sample['text']\n",
    "        annot_top5=[a['text'] for a in sample['annotations']][:5]\n",
    "        centroid_scores=[]\n",
    "        for annot in annot_top5:\n",
    "            centroid_scores.append(np.median([fasttext_model.wv.wmdistance([annot],[other_annot]) for other_annot in annot_top5]))\n",
    "        n.append(int(annot_top5[np.argmin(centroid_scores)]==true))\n",
    "    return np.mean(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753631444033039"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality_median(assistant_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8083791208791209"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_quality_median(quasar_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
