{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Neuro-horoscope-ruGPT3Small.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RHDK81QqrET"
      },
      "source": [
        "# Finetune ruGPT3Small on essays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK10D3MSpYty"
      },
      "source": [
        "## Install enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9e0_a0LRm9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5692941-365d-4885-c59b-e8c4dc816a63"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ueskXMKpfqn"
      },
      "source": [
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asqMueYPeIgK",
        "outputId": "ee34aed1-a7f7-4091-92fd-41ad3a4a50b5"
      },
      "source": [
        "!pip3 install urllib3==1.25.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: urllib3==1.25.4 in /usr/local/lib/python3.6/dist-packages (1.25.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPqtVgbkeTx7",
        "outputId": "1acf62d6-399d-4aad-a898-1a15776f3316"
      },
      "source": [
        "!pip3 install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.17.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.95)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.5.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.4)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.6 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.20.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.25.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.6->boto3->transformers==2.8.0) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpkjTWefecLc",
        "outputId": "bd7b9cf0-ef4d-40e8-9352-b212c536726b"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/pretrain_transformers.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-12 08:31:32--  https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/pretrain_transformers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34150 (33K) [text/plain]\n",
            "Saving to: ‘pretrain_transformers.py.1’\n",
            "\n",
            "\r          pretrain_   0%[                    ]       0  --.-KB/s               \rpretrain_transforme 100%[===================>]  33.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-12 08:31:33 (92.9 MB/s) - ‘pretrain_transformers.py.1’ saved [34150/34150]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7zu3BqpqJQ7",
        "outputId": "2f06b3ea-21ea-44ed-8953-45467ce4c325"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-12 08:31:33--  https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10474 (10K) [text/plain]\n",
            "Saving to: ‘generate_transformers.py.1’\n",
            "\n",
            "\r          generate_   0%[                    ]       0  --.-KB/s               \rgenerate_transforme 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-12 08:31:33 (127 MB/s) - ‘generate_transformers.py.1’ saved [10474/10474]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlJB3Ln7gjO9",
        "outputId": "5fcc180a-64d0-4655-bd3c-4bc1c5b299d5"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZqpSEJglUl",
        "outputId": "277304d0-4449-4445-883a-a4b427b2cea2"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-k02dewn6\n",
            "Created temporary directory: /tmp/pip-req-tracker-3jy7du4b\n",
            "Created requirements tracker '/tmp/pip-req-tracker-3jy7du4b'\n",
            "Created temporary directory: /tmp/pip-install-85n_jb5a\n",
            "Processing ./apex\n",
            "  Created temporary directory: /tmp/pip-req-build-fkiix02o\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-3jy7du4b'\n",
            "    Running setup.py (path:/tmp/pip-req-build-fkiix02o/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-fkiix02o/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-fkiix02o/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-fkiix02o/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-fkiix02o/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-fkiix02o/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-fkiix02o/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-fkiix02o/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-fkiix02o has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-3jy7du4b'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-t8w4ihd9\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-fkiix02o/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-fkiix02o/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-t8w4ihd9/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-fkiix02o/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda-10.1/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-t8w4ihd9/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-fkiix02o\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-3jy7du4b'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP7YAlDPqknI"
      },
      "source": [
        "## Add data to colab\n",
        "Add essays file from google dirve:\n",
        "* Add [file](https://drive.google.com/file/d/10ZsjTeaoihYA80n1G40O5YZmaGw0yOXk/view?usp=sharing) to your own drive\n",
        "* And mount drive to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6yIaA1LJ1Rm",
        "outputId": "6d6aec0c-a7ed-4cc9-b98f-55418cfd1212"
      },
      "source": [
        "# общий гороскоп\n",
        "!gdown --id 1G0NHMQnB_SCK9NOmuVUoh81L2xrvx2ee"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G0NHMQnB_SCK9NOmuVUoh81L2xrvx2ee\n",
            "To: /content/Neurohoroscope_wt_sign.txt\n",
            "\r  0% 0.00/1.72M [00:00<?, ?B/s]\r100% 1.72M/1.72M [00:00<00:00, 27.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7_CX4qAVLWN",
        "outputId": "21970c0c-c422-4e77-d0ea-6b91ace4a18e"
      },
      "source": [
        "#гороскоп по знакам\n",
        "#!gdown --id 1lXdvh5_TpMsy4WJjeHAg8eYpxLFlLBeE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lXdvh5_TpMsy4WJjeHAg8eYpxLFlLBeE\n",
            "To: /content/Neurohoroscope.txt\n",
            "\r  0% 0.00/1.71M [00:00<?, ?B/s]\r100% 1.71M/1.71M [00:00<00:00, 114MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jog8C0XKCYS",
        "outputId": "9326862f-c0c3-4d03-e928-af958c1f4404"
      },
      "source": [
        "data_path = \"/content/Neurohoroscope_wt_sign.txt\"\n",
        "!ls \"$data_path\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Neurohoroscope_wt_sign.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kis9EVsSTOio",
        "outputId": "fe56f274-a220-497d-a06b-456463327cb1"
      },
      "source": [
        "#data_path = \"/content/Neurohoroscope.txt\"\n",
        "#!ls \"$data_path\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Neurohoroscope.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDYi1TVTrtkO"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXdNbrq3rgzq"
      },
      "source": [
        "with open(data_path, \"r\") as file:\n",
        "    text = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sgqpozwryu_",
        "outputId": "5da271b6-9c37-413e-ef0f-495a61081e6d"
      },
      "source": [
        "texts = text.split('\\n');len(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3258"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcroAL76Sf0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzCWaaOp7lw3"
      },
      "source": [
        "train, valid = train_test_split(texts, test_size=0.15, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXZ6GPXc7y0X",
        "outputId": "31052767-71e0-4355-f6ef-be689a4bb515"
      },
      "source": [
        "len(train), len(valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2769, 489)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPB8rrVPr-kh"
      },
      "source": [
        "with open(\"train.txt\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP5_nk_0sAB0"
      },
      "source": [
        "with open(\"valid.txt\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NitGcEKPsDQE"
      },
      "source": [
        "## Run finetuning\n",
        "The following code download our model and tokenizer from transformers and finetune model essays.\n",
        "\n",
        "This took aroung ten minutes and obtain perplexity = 13-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vL07XFvsBBU",
        "outputId": "5cdb5056-b350-47d2-9e9d-74f430526dcf"
      },
      "source": [
        "!python pretrain_transformers.py \\\n",
        "    --output_dir=neurohoroscope_model_wt_sign_0 \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=train.txt \\\n",
        "    --fp16 \\\n",
        "    --per_gpu_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --block_size 512 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=valid.txt \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-12 08:37:05.358563: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/12/2021 08:37:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "02/12/2021 08:37:07 - INFO - filelock -   Lock 139808326364240 acquired on /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758.lock\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpbk1vvrmn\n",
            "Downloading: 100% 608/608 [00:00<00:00, 736kB/s]\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json in cache at /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "02/12/2021 08:37:07 - INFO - filelock -   Lock 139808326364240 released on /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758.lock\n",
            "02/12/2021 08:37:07 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json from cache at /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "02/12/2021 08:37:07 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "02/12/2021 08:37:07 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json from cache at /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "02/12/2021 08:37:07 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "02/12/2021 08:37:07 - INFO - transformers.tokenization_utils -   Model name 'sberbank-ai/rugpt3small_based_on_gpt2' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'sberbank-ai/rugpt3small_based_on_gpt2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/12/2021 08:37:07 - INFO - filelock -   Lock 139808326364576 acquired on /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26.lock\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp7zd6_1fh\n",
            "Downloading: 100% 1.71M/1.71M [00:00<00:00, 31.1MB/s]\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/vocab.json in cache at /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26\n",
            "02/12/2021 08:37:07 - INFO - filelock -   Lock 139808326364576 released on /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26.lock\n",
            "02/12/2021 08:37:07 - INFO - filelock -   Lock 139808326327880 acquired on /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032.lock\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp9q8bq9f6\n",
            "Downloading: 100% 1.27M/1.27M [00:00<00:00, 37.6MB/s]\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/merges.txt in cache at /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032\n",
            "02/12/2021 08:37:07 - INFO - filelock -   Lock 139808326327880 released on /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032.lock\n",
            "02/12/2021 08:37:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/vocab.json from cache at /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26\n",
            "02/12/2021 08:37:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/merges.txt from cache at /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032\n",
            "02/12/2021 08:37:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/added_tokens.json from cache at None\n",
            "02/12/2021 08:37:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/special_tokens_map.json from cache at None\n",
            "02/12/2021 08:37:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/tokenizer_config.json from cache at None\n",
            "02/12/2021 08:37:07 - INFO - filelock -   Lock 139808326391008 acquired on /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d.lock\n",
            "02/12/2021 08:37:07 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpsxkcah83\n",
            "Downloading: 100% 551M/551M [00:17<00:00, 32.1MB/s]\n",
            "02/12/2021 08:37:25 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/pytorch_model.bin in cache at /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d\n",
            "02/12/2021 08:37:25 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d\n",
            "02/12/2021 08:37:25 - INFO - filelock -   Lock 139808326391008 released on /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d.lock\n",
            "02/12/2021 08:37:25 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/pytorch_model.bin from cache at /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d\n",
            "02/12/2021 08:37:30 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in GPT2LMHeadModel: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
            "02/12/2021 08:37:44 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='valid.txt', evaluate_during_training=False, fp16=True, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='sberbank-ai/rugpt3small_based_on_gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=5.0, output_dir='neurohoroscope_model_wt_sign_0', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train.txt', warmup_steps=0, weight_decay=0.01)\n",
            "02/12/2021 08:37:44 - INFO - __main__ -   Creating features from dataset file at \n",
            "02/12/2021 08:37:47 - INFO - __main__ -   Saving features into cached file gpt2_cached_lm_512_train.txt\n",
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "02/12/2021 08:37:47 - INFO - __main__ -   ***** Running training *****\n",
            "02/12/2021 08:37:47 - INFO - __main__ -     Num examples = 385\n",
            "02/12/2021 08:37:47 - INFO - __main__ -     Num Epochs = 5\n",
            "02/12/2021 08:37:47 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "02/12/2021 08:37:47 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/12/2021 08:37:47 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/12/2021 08:37:47 - INFO - __main__ -     Total optimization steps = 1925\n",
            "Epoch:   0% 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/385 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  return orig_fn(arg0, *args, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "Iteration:   0% 1/385 [00:01<07:18,  1.14s/it]\u001b[A\n",
            "Iteration:   1% 2/385 [00:01<05:26,  1.17it/s]\u001b[A\n",
            "Iteration:   1% 3/385 [00:01<04:04,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 4/385 [00:01<03:06,  2.04it/s]\u001b[A\n",
            "Iteration:   1% 5/385 [00:01<02:26,  2.59it/s]\u001b[A\n",
            "Iteration:   2% 6/385 [00:01<01:58,  3.19it/s]\u001b[A\n",
            "Iteration:   2% 7/385 [00:02<01:39,  3.81it/s]\u001b[A\n",
            "Iteration:   2% 8/385 [00:02<01:25,  4.41it/s]\u001b[A\n",
            "Iteration:   2% 9/385 [00:02<01:15,  4.96it/s]\u001b[A\n",
            "Iteration:   3% 10/385 [00:02<01:08,  5.44it/s]\u001b[A\n",
            "Iteration:   3% 11/385 [00:02<01:04,  5.83it/s]\u001b[A\n",
            "Iteration:   3% 12/385 [00:02<01:00,  6.14it/s]\u001b[A\n",
            "Iteration:   3% 13/385 [00:02<00:58,  6.37it/s]\u001b[A\n",
            "Iteration:   4% 14/385 [00:03<00:56,  6.56it/s]\u001b[A\n",
            "Iteration:   4% 15/385 [00:03<00:55,  6.69it/s]\u001b[A\n",
            "Iteration:   4% 16/385 [00:03<00:54,  6.80it/s]\u001b[A\n",
            "Iteration:   4% 17/385 [00:03<00:53,  6.85it/s]\u001b[A\n",
            "Iteration:   5% 18/385 [00:03<00:52,  6.95it/s]\u001b[A\n",
            "Iteration:   5% 19/385 [00:03<00:52,  6.95it/s]\u001b[A\n",
            "Iteration:   5% 20/385 [00:03<00:52,  6.94it/s]\u001b[A\n",
            "Iteration:   5% 21/385 [00:04<00:52,  6.97it/s]\u001b[A\n",
            "Iteration:   6% 22/385 [00:04<00:51,  6.99it/s]\u001b[A\n",
            "Iteration:   6% 23/385 [00:04<00:51,  7.01it/s]\u001b[A\n",
            "Iteration:   6% 24/385 [00:04<00:52,  6.94it/s]\u001b[A\n",
            "Iteration:   6% 25/385 [00:04<00:51,  6.97it/s]\u001b[A\n",
            "Iteration:   7% 26/385 [00:04<00:51,  6.91it/s]\u001b[A\n",
            "Iteration:   7% 27/385 [00:04<00:52,  6.86it/s]\u001b[A\n",
            "Iteration:   7% 28/385 [00:05<00:52,  6.84it/s]\u001b[A\n",
            "Iteration:   8% 29/385 [00:05<00:51,  6.86it/s]\u001b[A\n",
            "Iteration:   8% 30/385 [00:05<00:51,  6.83it/s]\u001b[A\n",
            "Iteration:   8% 31/385 [00:05<00:51,  6.84it/s]\u001b[A\n",
            "Iteration:   8% 32/385 [00:05<00:51,  6.80it/s]\u001b[A\n",
            "Iteration:   9% 33/385 [00:05<00:51,  6.86it/s]\u001b[A\n",
            "Iteration:   9% 34/385 [00:05<00:50,  6.90it/s]\u001b[A\n",
            "Iteration:   9% 35/385 [00:06<00:50,  6.94it/s]\u001b[A\n",
            "Iteration:   9% 36/385 [00:06<00:50,  6.92it/s]\u001b[A\n",
            "Iteration:  10% 37/385 [00:06<00:50,  6.95it/s]\u001b[A\n",
            "Iteration:  10% 38/385 [00:06<00:50,  6.91it/s]\u001b[A\n",
            "Iteration:  10% 39/385 [00:06<00:49,  6.95it/s]\u001b[A\n",
            "Iteration:  10% 40/385 [00:06<00:49,  6.96it/s]\u001b[A\n",
            "Iteration:  11% 41/385 [00:06<00:49,  6.99it/s]\u001b[A\n",
            "Iteration:  11% 42/385 [00:07<00:48,  7.02it/s]\u001b[A\n",
            "Iteration:  11% 43/385 [00:07<00:48,  6.98it/s]\u001b[A\n",
            "Iteration:  11% 44/385 [00:07<00:48,  7.00it/s]\u001b[A\n",
            "Iteration:  12% 45/385 [00:07<00:48,  6.95it/s]\u001b[A\n",
            "Iteration:  12% 46/385 [00:07<00:49,  6.89it/s]\u001b[A\n",
            "Iteration:  12% 47/385 [00:07<00:48,  6.91it/s]\u001b[A\n",
            "Iteration:  12% 48/385 [00:07<00:49,  6.84it/s]\u001b[A\n",
            "Iteration:  13% 49/385 [00:08<00:48,  6.92it/s]\u001b[A\n",
            "Iteration:  13% 50/385 [00:08<00:48,  6.90it/s]\u001b[A\n",
            "Iteration:  13% 51/385 [00:08<00:48,  6.91it/s]\u001b[A\n",
            "Iteration:  14% 52/385 [00:08<00:49,  6.78it/s]\u001b[A\n",
            "Iteration:  14% 53/385 [00:08<00:48,  6.88it/s]\u001b[A\n",
            "Iteration:  14% 54/385 [00:08<00:48,  6.89it/s]\u001b[A\n",
            "Iteration:  14% 55/385 [00:08<00:47,  6.93it/s]\u001b[A\n",
            "Iteration:  15% 56/385 [00:09<00:47,  6.96it/s]\u001b[A\n",
            "Iteration:  15% 57/385 [00:09<00:47,  6.97it/s]\u001b[A\n",
            "Iteration:  15% 58/385 [00:09<00:46,  6.98it/s]\u001b[A\n",
            "Iteration:  15% 59/385 [00:09<00:47,  6.86it/s]\u001b[A\n",
            "Iteration:  16% 60/385 [00:09<00:47,  6.90it/s]\u001b[A\n",
            "Iteration:  16% 61/385 [00:09<00:46,  6.90it/s]\u001b[A\n",
            "Iteration:  16% 62/385 [00:09<00:46,  6.90it/s]\u001b[A\n",
            "Iteration:  16% 63/385 [00:10<00:46,  6.91it/s]\u001b[A\n",
            "Iteration:  17% 64/385 [00:10<00:46,  6.93it/s]\u001b[A\n",
            "Iteration:  17% 65/385 [00:10<00:46,  6.95it/s]\u001b[A\n",
            "Iteration:  17% 66/385 [00:10<00:46,  6.86it/s]\u001b[A\n",
            "Iteration:  17% 67/385 [00:10<00:45,  6.91it/s]\u001b[A\n",
            "Iteration:  18% 68/385 [00:10<00:45,  6.92it/s]\u001b[A\n",
            "Iteration:  18% 69/385 [00:10<00:45,  6.98it/s]\u001b[A\n",
            "Iteration:  18% 70/385 [00:11<00:45,  7.00it/s]\u001b[A\n",
            "Iteration:  18% 71/385 [00:11<00:45,  6.97it/s]\u001b[A\n",
            "Iteration:  19% 72/385 [00:11<00:44,  6.96it/s]\u001b[A\n",
            "Iteration:  19% 73/385 [00:11<00:44,  6.95it/s]\u001b[A\n",
            "Iteration:  19% 74/385 [00:11<00:44,  6.95it/s]\u001b[A\n",
            "Iteration:  19% 75/385 [00:11<00:44,  6.94it/s]\u001b[A\n",
            "Iteration:  20% 76/385 [00:11<00:44,  6.93it/s]\u001b[A\n",
            "Iteration:  20% 77/385 [00:12<00:44,  6.94it/s]\u001b[A\n",
            "Iteration:  20% 78/385 [00:12<00:44,  6.96it/s]\u001b[A\n",
            "Iteration:  21% 79/385 [00:12<00:43,  6.97it/s]\u001b[A\n",
            "Iteration:  21% 80/385 [00:12<00:43,  6.95it/s]\u001b[A\n",
            "Iteration:  21% 81/385 [00:12<00:43,  7.01it/s]\u001b[A\n",
            "Iteration:  21% 82/385 [00:12<00:43,  7.01it/s]\u001b[A\n",
            "Iteration:  22% 83/385 [00:12<00:43,  6.99it/s]\u001b[A\n",
            "Iteration:  22% 84/385 [00:13<00:43,  6.99it/s]\u001b[A\n",
            "Iteration:  22% 85/385 [00:13<00:43,  6.98it/s]\u001b[A\n",
            "Iteration:  22% 86/385 [00:13<00:42,  7.02it/s]\u001b[A\n",
            "Iteration:  23% 87/385 [00:13<00:42,  7.02it/s]\u001b[A\n",
            "Iteration:  23% 88/385 [00:13<00:42,  7.00it/s]\u001b[A\n",
            "Iteration:  23% 89/385 [00:13<00:42,  7.02it/s]\u001b[A\n",
            "Iteration:  23% 90/385 [00:13<00:42,  6.97it/s]\u001b[A\n",
            "Iteration:  24% 91/385 [00:14<00:42,  6.96it/s]\u001b[A\n",
            "Iteration:  24% 92/385 [00:14<00:42,  6.96it/s]\u001b[A\n",
            "Iteration:  24% 93/385 [00:14<00:42,  6.94it/s]\u001b[A\n",
            "Iteration:  24% 94/385 [00:14<00:41,  6.95it/s]\u001b[A\n",
            "Iteration:  25% 95/385 [00:14<00:41,  6.95it/s]\u001b[A\n",
            "Iteration:  25% 96/385 [00:14<00:41,  6.94it/s]\u001b[A\n",
            "Iteration:  25% 97/385 [00:15<00:42,  6.81it/s]\u001b[A\n",
            "Iteration:  25% 98/385 [00:15<00:42,  6.81it/s]\u001b[A\n",
            "Iteration:  26% 99/385 [00:15<00:42,  6.77it/s]\u001b[A\n",
            "Iteration:  26% 100/385 [00:15<00:41,  6.80it/s]\u001b[A\n",
            "Iteration:  26% 101/385 [00:15<00:42,  6.74it/s]\u001b[A\n",
            "Iteration:  26% 102/385 [00:15<00:40,  6.90it/s]\u001b[A\n",
            "Iteration:  27% 103/385 [00:15<00:40,  6.89it/s]\u001b[A\n",
            "Iteration:  27% 104/385 [00:16<00:40,  6.95it/s]\u001b[A\n",
            "Iteration:  27% 105/385 [00:16<00:40,  6.98it/s]\u001b[A\n",
            "Iteration:  28% 106/385 [00:16<00:40,  6.97it/s]\u001b[A\n",
            "Iteration:  28% 107/385 [00:16<00:39,  6.99it/s]\u001b[A\n",
            "Iteration:  28% 108/385 [00:16<00:39,  6.99it/s]\u001b[A\n",
            "Iteration:  28% 109/385 [00:16<00:39,  6.99it/s]\u001b[A\n",
            "Iteration:  29% 110/385 [00:16<00:39,  6.99it/s]\u001b[A\n",
            "Iteration:  29% 111/385 [00:17<00:39,  6.93it/s]\u001b[A\n",
            "Iteration:  29% 112/385 [00:17<00:39,  6.95it/s]\u001b[A\n",
            "Iteration:  29% 113/385 [00:17<00:39,  6.96it/s]\u001b[A\n",
            "Iteration:  30% 114/385 [00:17<00:38,  7.00it/s]\u001b[A\n",
            "Iteration:  30% 115/385 [00:17<00:38,  7.00it/s]\u001b[A\n",
            "Iteration:  30% 116/385 [00:17<00:38,  6.92it/s]\u001b[A\n",
            "Iteration:  30% 117/385 [00:17<00:38,  6.92it/s]\u001b[A\n",
            "Iteration:  31% 118/385 [00:18<00:38,  6.91it/s]\u001b[A\n",
            "Iteration:  31% 119/385 [00:18<00:38,  6.92it/s]\u001b[A\n",
            "Iteration:  31% 120/385 [00:18<00:38,  6.90it/s]\u001b[A\n",
            "Iteration:  31% 121/385 [00:18<00:38,  6.94it/s]\u001b[A\n",
            "Iteration:  32% 122/385 [00:18<00:38,  6.91it/s]\u001b[A\n",
            "Iteration:  32% 123/385 [00:18<00:37,  6.95it/s]\u001b[A\n",
            "Iteration:  32% 124/385 [00:18<00:37,  6.94it/s]\u001b[A\n",
            "Iteration:  32% 125/385 [00:19<00:37,  6.93it/s]\u001b[A\n",
            "Iteration:  33% 126/385 [00:19<00:37,  6.90it/s]\u001b[A\n",
            "Iteration:  33% 127/385 [00:19<00:37,  6.89it/s]\u001b[A\n",
            "Iteration:  33% 128/385 [00:19<00:37,  6.90it/s]\u001b[A\n",
            "Iteration:  34% 129/385 [00:19<00:36,  6.93it/s]\u001b[A\n",
            "Iteration:  34% 130/385 [00:19<00:36,  6.94it/s]\u001b[A\n",
            "Iteration:  34% 131/385 [00:19<00:36,  6.96it/s]\u001b[A\n",
            "Iteration:  34% 132/385 [00:20<00:36,  6.97it/s]\u001b[A\n",
            "Iteration:  35% 133/385 [00:20<00:36,  6.97it/s]\u001b[A\n",
            "Iteration:  35% 134/385 [00:20<00:36,  6.95it/s]\u001b[A\n",
            "Iteration:  35% 135/385 [00:20<00:35,  6.97it/s]\u001b[A\n",
            "Iteration:  35% 136/385 [00:20<00:35,  6.94it/s]\u001b[A\n",
            "Iteration:  36% 137/385 [00:20<00:35,  6.97it/s]\u001b[A\n",
            "Iteration:  36% 138/385 [00:20<00:35,  6.98it/s]\u001b[A\n",
            "Iteration:  36% 139/385 [00:21<00:35,  6.94it/s]\u001b[A\n",
            "Iteration:  36% 140/385 [00:21<00:35,  6.89it/s]\u001b[A\n",
            "Iteration:  37% 141/385 [00:21<00:35,  6.89it/s]\u001b[A\n",
            "Iteration:  37% 142/385 [00:21<00:35,  6.90it/s]\u001b[A\n",
            "Iteration:  37% 143/385 [00:21<00:34,  6.93it/s]\u001b[A\n",
            "Iteration:  37% 144/385 [00:21<00:34,  6.94it/s]\u001b[A\n",
            "Iteration:  38% 145/385 [00:21<00:34,  6.97it/s]\u001b[A\n",
            "Iteration:  38% 146/385 [00:22<00:34,  6.99it/s]\u001b[A\n",
            "Iteration:  38% 147/385 [00:22<00:34,  6.98it/s]\u001b[A\n",
            "Iteration:  38% 148/385 [00:22<00:34,  6.91it/s]\u001b[A\n",
            "Iteration:  39% 149/385 [00:22<00:33,  6.95it/s]\u001b[A\n",
            "Iteration:  39% 150/385 [00:22<00:34,  6.88it/s]\u001b[A\n",
            "Iteration:  39% 151/385 [00:22<00:34,  6.86it/s]\u001b[A\n",
            "Iteration:  39% 152/385 [00:22<00:34,  6.82it/s]\u001b[A\n",
            "Iteration:  40% 153/385 [00:23<00:33,  6.90it/s]\u001b[A\n",
            "Iteration:  40% 154/385 [00:23<00:33,  6.90it/s]\u001b[A\n",
            "Iteration:  40% 155/385 [00:23<00:33,  6.91it/s]\u001b[A\n",
            "Iteration:  41% 156/385 [00:23<00:33,  6.94it/s]\u001b[A\n",
            "Iteration:  41% 157/385 [00:23<00:32,  6.96it/s]\u001b[A\n",
            "Iteration:  41% 158/385 [00:23<00:32,  6.95it/s]\u001b[A\n",
            "Iteration:  41% 159/385 [00:23<00:32,  6.97it/s]\u001b[A\n",
            "Iteration:  42% 160/385 [00:24<00:32,  6.99it/s]\u001b[A\n",
            "Iteration:  42% 161/385 [00:24<00:32,  6.99it/s]\u001b[A\n",
            "Iteration:  42% 162/385 [00:24<00:31,  7.03it/s]\u001b[A\n",
            "Iteration:  42% 163/385 [00:24<00:31,  6.98it/s]\u001b[A\n",
            "Iteration:  43% 164/385 [00:24<00:32,  6.90it/s]\u001b[A\n",
            "Iteration:  43% 165/385 [00:24<00:32,  6.82it/s]\u001b[A\n",
            "Iteration:  43% 166/385 [00:24<00:32,  6.84it/s]\u001b[A\n",
            "Iteration:  43% 167/385 [00:25<00:31,  6.90it/s]\u001b[A\n",
            "Iteration:  44% 168/385 [00:25<00:31,  6.93it/s]\u001b[A\n",
            "Iteration:  44% 169/385 [00:25<00:31,  6.93it/s]\u001b[A\n",
            "Iteration:  44% 170/385 [00:25<00:30,  6.97it/s]\u001b[A\n",
            "Iteration:  44% 171/385 [00:25<00:30,  6.96it/s]\u001b[A\n",
            "Iteration:  45% 172/385 [00:25<00:30,  6.89it/s]\u001b[A\n",
            "Iteration:  45% 173/385 [00:25<00:31,  6.78it/s]\u001b[A\n",
            "Iteration:  45% 174/385 [00:26<00:30,  6.83it/s]\u001b[A\n",
            "Iteration:  45% 175/385 [00:26<00:30,  6.81it/s]\u001b[A\n",
            "Iteration:  46% 176/385 [00:26<00:30,  6.92it/s]\u001b[A\n",
            "Iteration:  46% 177/385 [00:26<00:30,  6.87it/s]\u001b[A\n",
            "Iteration:  46% 178/385 [00:26<00:29,  6.92it/s]\u001b[A\n",
            "Iteration:  46% 179/385 [00:26<00:29,  6.94it/s]\u001b[A\n",
            "Iteration:  47% 180/385 [00:26<00:29,  6.94it/s]\u001b[A\n",
            "Iteration:  47% 181/385 [00:27<00:29,  6.96it/s]\u001b[A\n",
            "Iteration:  47% 182/385 [00:27<00:29,  6.93it/s]\u001b[A\n",
            "Iteration:  48% 183/385 [00:27<00:29,  6.92it/s]\u001b[A\n",
            "Iteration:  48% 184/385 [00:27<00:29,  6.91it/s]\u001b[A\n",
            "Iteration:  48% 185/385 [00:27<00:28,  6.90it/s]\u001b[A\n",
            "Iteration:  48% 186/385 [00:27<00:28,  6.89it/s]\u001b[A\n",
            "Iteration:  49% 187/385 [00:27<00:28,  6.91it/s]\u001b[A\n",
            "Iteration:  49% 188/385 [00:28<00:28,  6.93it/s]\u001b[A\n",
            "Iteration:  49% 189/385 [00:28<00:28,  6.93it/s]\u001b[A\n",
            "Iteration:  49% 190/385 [00:28<00:27,  6.97it/s]\u001b[A\n",
            "Iteration:  50% 191/385 [00:28<00:27,  6.95it/s]\u001b[A\n",
            "Iteration:  50% 192/385 [00:28<00:27,  6.98it/s]\u001b[A\n",
            "Iteration:  50% 193/385 [00:28<00:28,  6.76it/s]\u001b[A\n",
            "Iteration:  50% 194/385 [00:29<00:27,  6.87it/s]\u001b[A\n",
            "Iteration:  51% 195/385 [00:29<00:27,  6.86it/s]\u001b[A\n",
            "Iteration:  51% 196/385 [00:29<00:27,  6.89it/s]\u001b[A\n",
            "Iteration:  51% 197/385 [00:29<00:27,  6.93it/s]\u001b[A\n",
            "Iteration:  51% 198/385 [00:29<00:26,  6.94it/s]\u001b[A\n",
            "Iteration:  52% 199/385 [00:29<00:26,  6.96it/s]\u001b[A\n",
            "Iteration:  52% 200/385 [00:29<00:26,  6.95it/s]\u001b[A\n",
            "Iteration:  52% 201/385 [00:30<00:26,  6.97it/s]\u001b[A\n",
            "Iteration:  52% 202/385 [00:30<00:26,  6.98it/s]\u001b[A\n",
            "Iteration:  53% 203/385 [00:30<00:26,  6.92it/s]\u001b[A\n",
            "Iteration:  53% 204/385 [00:30<00:26,  6.91it/s]\u001b[A\n",
            "Iteration:  53% 205/385 [00:30<00:26,  6.86it/s]\u001b[A\n",
            "Iteration:  54% 206/385 [00:30<00:26,  6.87it/s]\u001b[A\n",
            "Iteration:  54% 207/385 [00:30<00:26,  6.84it/s]\u001b[A\n",
            "Iteration:  54% 208/385 [00:31<00:25,  6.94it/s]\u001b[A\n",
            "Iteration:  54% 209/385 [00:31<00:25,  6.96it/s]\u001b[A\n",
            "Iteration:  55% 210/385 [00:31<00:25,  6.95it/s]\u001b[A\n",
            "Iteration:  55% 211/385 [00:31<00:25,  6.95it/s]\u001b[A\n",
            "Iteration:  55% 212/385 [00:31<00:24,  6.96it/s]\u001b[A\n",
            "Iteration:  55% 213/385 [00:31<00:24,  6.98it/s]\u001b[A\n",
            "Iteration:  56% 214/385 [00:31<00:24,  6.95it/s]\u001b[A\n",
            "Iteration:  56% 215/385 [00:32<00:24,  6.96it/s]\u001b[A\n",
            "Iteration:  56% 216/385 [00:32<00:24,  6.97it/s]\u001b[A\n",
            "Iteration:  56% 217/385 [00:32<00:24,  6.97it/s]\u001b[A\n",
            "Iteration:  57% 218/385 [00:32<00:24,  6.95it/s]\u001b[A\n",
            "Iteration:  57% 219/385 [00:32<00:24,  6.86it/s]\u001b[A\n",
            "Iteration:  57% 220/385 [00:32<00:23,  6.89it/s]\u001b[A\n",
            "Iteration:  57% 221/385 [00:32<00:23,  6.87it/s]\u001b[A\n",
            "Iteration:  58% 222/385 [00:33<00:23,  6.93it/s]\u001b[A\n",
            "Iteration:  58% 223/385 [00:33<00:23,  6.94it/s]\u001b[A\n",
            "Iteration:  58% 224/385 [00:33<00:23,  6.96it/s]\u001b[A\n",
            "Iteration:  58% 225/385 [00:33<00:22,  6.97it/s]\u001b[A\n",
            "Iteration:  59% 226/385 [00:33<00:22,  6.97it/s]\u001b[A\n",
            "Iteration:  59% 227/385 [00:33<00:22,  6.94it/s]\u001b[A\n",
            "Iteration:  59% 228/385 [00:33<00:23,  6.81it/s]\u001b[A\n",
            "Iteration:  59% 229/385 [00:34<00:22,  6.84it/s]\u001b[A\n",
            "Iteration:  60% 230/385 [00:34<00:22,  6.88it/s]\u001b[A\n",
            "Iteration:  60% 231/385 [00:34<00:22,  6.92it/s]\u001b[A\n",
            "Iteration:  60% 232/385 [00:34<00:22,  6.94it/s]\u001b[A\n",
            "Iteration:  61% 233/385 [00:34<00:21,  6.91it/s]\u001b[A\n",
            "Iteration:  61% 234/385 [00:34<00:21,  6.93it/s]\u001b[A\n",
            "Iteration:  61% 235/385 [00:34<00:22,  6.81it/s]\u001b[A\n",
            "Iteration:  61% 236/385 [00:35<00:21,  6.80it/s]\u001b[A\n",
            "Iteration:  62% 237/385 [00:35<00:21,  6.82it/s]\u001b[A\n",
            "Iteration:  62% 238/385 [00:35<00:21,  6.88it/s]\u001b[A\n",
            "Iteration:  62% 239/385 [00:35<00:21,  6.93it/s]\u001b[A\n",
            "Iteration:  62% 240/385 [00:35<00:20,  6.95it/s]\u001b[A\n",
            "Iteration:  63% 241/385 [00:35<00:20,  6.96it/s]\u001b[A\n",
            "Iteration:  63% 242/385 [00:35<00:21,  6.79it/s]\u001b[A\n",
            "Iteration:  63% 243/385 [00:36<00:20,  6.81it/s]\u001b[A\n",
            "Iteration:  63% 244/385 [00:36<00:20,  6.85it/s]\u001b[A\n",
            "Iteration:  64% 245/385 [00:36<00:20,  6.90it/s]\u001b[A\n",
            "Iteration:  64% 246/385 [00:36<00:20,  6.93it/s]\u001b[A\n",
            "Iteration:  64% 247/385 [00:36<00:19,  6.95it/s]\u001b[A\n",
            "Iteration:  64% 248/385 [00:36<00:19,  6.94it/s]\u001b[A\n",
            "Iteration:  65% 249/385 [00:36<00:19,  6.86it/s]\u001b[A\n",
            "Iteration:  65% 250/385 [00:37<00:19,  6.89it/s]\u001b[A\n",
            "Iteration:  65% 251/385 [00:37<00:19,  6.84it/s]\u001b[A\n",
            "Iteration:  65% 252/385 [00:37<00:19,  6.85it/s]\u001b[A\n",
            "Iteration:  66% 253/385 [00:37<00:19,  6.88it/s]\u001b[A\n",
            "Iteration:  66% 254/385 [00:37<00:18,  6.91it/s]\u001b[A\n",
            "Iteration:  66% 255/385 [00:37<00:18,  6.94it/s]\u001b[A\n",
            "Iteration:  66% 256/385 [00:37<00:18,  6.90it/s]\u001b[A\n",
            "Iteration:  67% 257/385 [00:38<00:18,  6.87it/s]\u001b[A\n",
            "Iteration:  67% 258/385 [00:38<00:18,  6.85it/s]\u001b[A\n",
            "Iteration:  67% 259/385 [00:38<00:18,  6.85it/s]\u001b[A\n",
            "Iteration:  68% 260/385 [00:38<00:18,  6.79it/s]\u001b[A\n",
            "Iteration:  68% 261/385 [00:38<00:17,  6.90it/s]\u001b[A\n",
            "Iteration:  68% 262/385 [00:38<00:17,  6.92it/s]\u001b[A\n",
            "Iteration:  68% 263/385 [00:39<00:17,  6.95it/s]\u001b[A\n",
            "Iteration:  69% 264/385 [00:39<00:17,  6.96it/s]\u001b[A\n",
            "Iteration:  69% 265/385 [00:39<00:17,  6.96it/s]\u001b[A\n",
            "Iteration:  69% 266/385 [00:39<00:17,  6.93it/s]\u001b[A\n",
            "Iteration:  69% 267/385 [00:39<00:16,  6.98it/s]\u001b[A\n",
            "Iteration:  70% 268/385 [00:39<00:16,  6.98it/s]\u001b[A\n",
            "Iteration:  70% 269/385 [00:39<00:16,  6.97it/s]\u001b[A\n",
            "Iteration:  70% 270/385 [00:40<00:16,  6.92it/s]\u001b[A\n",
            "Iteration:  70% 271/385 [00:40<00:16,  6.92it/s]\u001b[A\n",
            "Iteration:  71% 272/385 [00:40<00:16,  6.83it/s]\u001b[A\n",
            "Iteration:  71% 273/385 [00:40<00:16,  6.89it/s]\u001b[A\n",
            "Iteration:  71% 274/385 [00:40<00:16,  6.91it/s]\u001b[A\n",
            "Iteration:  71% 275/385 [00:40<00:15,  6.96it/s]\u001b[A\n",
            "Iteration:  72% 276/385 [00:40<00:15,  6.97it/s]\u001b[A\n",
            "Iteration:  72% 277/385 [00:41<00:15,  6.92it/s]\u001b[A\n",
            "Iteration:  72% 278/385 [00:41<00:15,  6.94it/s]\u001b[A\n",
            "Iteration:  72% 279/385 [00:41<00:15,  6.92it/s]\u001b[A\n",
            "Iteration:  73% 280/385 [00:41<00:15,  6.90it/s]\u001b[A\n",
            "Iteration:  73% 281/385 [00:41<00:15,  6.89it/s]\u001b[A\n",
            "Iteration:  73% 282/385 [00:41<00:14,  6.91it/s]\u001b[A\n",
            "Iteration:  74% 283/385 [00:41<00:14,  6.94it/s]\u001b[A\n",
            "Iteration:  74% 284/385 [00:42<00:14,  6.95it/s]\u001b[A\n",
            "Iteration:  74% 285/385 [00:42<00:14,  6.97it/s]\u001b[A\n",
            "Iteration:  74% 286/385 [00:42<00:14,  6.91it/s]\u001b[A\n",
            "Iteration:  75% 287/385 [00:42<00:14,  6.92it/s]\u001b[A\n",
            "Iteration:  75% 288/385 [00:42<00:14,  6.85it/s]\u001b[A\n",
            "Iteration:  75% 289/385 [00:42<00:13,  6.86it/s]\u001b[A\n",
            "Iteration:  75% 290/385 [00:42<00:14,  6.75it/s]\u001b[A\n",
            "Iteration:  76% 291/385 [00:43<00:13,  6.85it/s]\u001b[A\n",
            "Iteration:  76% 292/385 [00:43<00:13,  6.84it/s]\u001b[A\n",
            "Iteration:  76% 293/385 [00:43<00:13,  6.82it/s]\u001b[A\n",
            "Iteration:  76% 294/385 [00:43<00:13,  6.86it/s]\u001b[A\n",
            "Iteration:  77% 295/385 [00:43<00:13,  6.89it/s]\u001b[A\n",
            "Iteration:  77% 296/385 [00:43<00:12,  6.92it/s]\u001b[A\n",
            "Iteration:  77% 297/385 [00:43<00:12,  6.93it/s]\u001b[A\n",
            "Iteration:  77% 298/385 [00:44<00:12,  6.94it/s]\u001b[A\n",
            "Iteration:  78% 299/385 [00:44<00:12,  6.94it/s]\u001b[A\n",
            "Iteration:  78% 300/385 [00:44<00:12,  6.93it/s]\u001b[A\n",
            "Iteration:  78% 301/385 [00:44<00:12,  6.91it/s]\u001b[A\n",
            "Iteration:  78% 302/385 [00:44<00:12,  6.80it/s]\u001b[A\n",
            "Iteration:  79% 303/385 [00:44<00:12,  6.83it/s]\u001b[A\n",
            "Iteration:  79% 304/385 [00:44<00:11,  6.78it/s]\u001b[A\n",
            "Iteration:  79% 305/385 [00:45<00:11,  6.89it/s]\u001b[A\n",
            "Iteration:  79% 306/385 [00:45<00:11,  6.79it/s]\u001b[A\n",
            "Iteration:  80% 307/385 [00:45<00:11,  6.86it/s]\u001b[A\n",
            "Iteration:  80% 308/385 [00:45<00:11,  6.89it/s]\u001b[A\n",
            "Iteration:  80% 309/385 [00:45<00:11,  6.91it/s]\u001b[A\n",
            "Iteration:  81% 310/385 [00:45<00:10,  6.93it/s]\u001b[A\n",
            "Iteration:  81% 311/385 [00:45<00:10,  6.93it/s]\u001b[A\n",
            "Iteration:  81% 312/385 [00:46<00:10,  6.95it/s]\u001b[A\n",
            "Iteration:  81% 313/385 [00:46<00:10,  6.95it/s]\u001b[A\n",
            "Iteration:  82% 314/385 [00:46<00:10,  6.94it/s]\u001b[A\n",
            "Iteration:  82% 315/385 [00:46<00:10,  6.92it/s]\u001b[A\n",
            "Iteration:  82% 316/385 [00:46<00:10,  6.88it/s]\u001b[A\n",
            "Iteration:  82% 317/385 [00:46<00:09,  6.89it/s]\u001b[A\n",
            "Iteration:  83% 318/385 [00:46<00:09,  6.87it/s]\u001b[A\n",
            "Iteration:  83% 319/385 [00:47<00:09,  6.93it/s]\u001b[A\n",
            "Iteration:  83% 320/385 [00:47<00:09,  6.91it/s]\u001b[A\n",
            "Iteration:  83% 321/385 [00:47<00:09,  6.96it/s]\u001b[A\n",
            "Iteration:  84% 322/385 [00:47<00:09,  6.91it/s]\u001b[A\n",
            "Iteration:  84% 323/385 [00:47<00:08,  6.97it/s]\u001b[A\n",
            "Iteration:  84% 324/385 [00:47<00:08,  6.96it/s]\u001b[A\n",
            "Iteration:  84% 325/385 [00:47<00:08,  6.91it/s]\u001b[A\n",
            "Iteration:  85% 326/385 [00:48<00:08,  6.87it/s]\u001b[A\n",
            "Iteration:  85% 327/385 [00:48<00:08,  6.84it/s]\u001b[A\n",
            "Iteration:  85% 328/385 [00:48<00:08,  6.88it/s]\u001b[A\n",
            "Iteration:  85% 329/385 [00:48<00:08,  6.88it/s]\u001b[A\n",
            "Iteration:  86% 330/385 [00:48<00:07,  6.93it/s]\u001b[A\n",
            "Iteration:  86% 331/385 [00:48<00:07,  6.88it/s]\u001b[A\n",
            "Iteration:  86% 332/385 [00:49<00:07,  6.83it/s]\u001b[A\n",
            "Iteration:  86% 333/385 [00:49<00:07,  6.88it/s]\u001b[A\n",
            "Iteration:  87% 334/385 [00:49<00:07,  6.83it/s]\u001b[A\n",
            "Iteration:  87% 335/385 [00:49<00:07,  6.86it/s]\u001b[A\n",
            "Iteration:  87% 336/385 [00:49<00:07,  6.82it/s]\u001b[A\n",
            "Iteration:  88% 337/385 [00:49<00:07,  6.83it/s]\u001b[A\n",
            "Iteration:  88% 338/385 [00:49<00:06,  6.84it/s]\u001b[A\n",
            "Iteration:  88% 339/385 [00:50<00:06,  6.85it/s]\u001b[A\n",
            "Iteration:  88% 340/385 [00:50<00:06,  6.89it/s]\u001b[A\n",
            "Iteration:  89% 341/385 [00:50<00:06,  6.93it/s]\u001b[A\n",
            "Iteration:  89% 342/385 [00:50<00:06,  6.93it/s]\u001b[A\n",
            "Iteration:  89% 343/385 [00:50<00:06,  6.90it/s]\u001b[A\n",
            "Iteration:  89% 344/385 [00:50<00:05,  6.87it/s]\u001b[A\n",
            "Iteration:  90% 345/385 [00:50<00:05,  6.86it/s]\u001b[A\n",
            "Iteration:  90% 346/385 [00:51<00:05,  6.83it/s]\u001b[A\n",
            "Iteration:  90% 347/385 [00:51<00:05,  6.84it/s]\u001b[A\n",
            "Iteration:  90% 348/385 [00:51<00:05,  6.86it/s]\u001b[A\n",
            "Iteration:  91% 349/385 [00:51<00:05,  6.84it/s]\u001b[A\n",
            "Iteration:  91% 350/385 [00:51<00:05,  6.82it/s]\u001b[A\n",
            "Iteration:  91% 351/385 [00:51<00:04,  6.84it/s]\u001b[A\n",
            "Iteration:  91% 352/385 [00:51<00:04,  6.84it/s]\u001b[A\n",
            "Iteration:  92% 353/385 [00:52<00:04,  6.82it/s]\u001b[A\n",
            "Iteration:  92% 354/385 [00:52<00:04,  6.77it/s]\u001b[A\n",
            "Iteration:  92% 355/385 [00:52<00:04,  6.82it/s]\u001b[A\n",
            "Iteration:  92% 356/385 [00:52<00:04,  6.83it/s]\u001b[A\n",
            "Iteration:  93% 357/385 [00:52<00:04,  6.92it/s]\u001b[A\n",
            "Iteration:  93% 358/385 [00:52<00:03,  6.85it/s]\u001b[A\n",
            "Iteration:  93% 359/385 [00:52<00:03,  6.79it/s]\u001b[A\n",
            "Iteration:  94% 360/385 [00:53<00:03,  6.77it/s]\u001b[A\n",
            "Iteration:  94% 361/385 [00:53<00:03,  6.82it/s]\u001b[A\n",
            "Iteration:  94% 362/385 [00:53<00:03,  6.86it/s]\u001b[A\n",
            "Iteration:  94% 363/385 [00:53<00:03,  6.89it/s]\u001b[A\n",
            "Iteration:  95% 364/385 [00:53<00:03,  6.92it/s]\u001b[A\n",
            "Iteration:  95% 365/385 [00:53<00:02,  6.93it/s]\u001b[A\n",
            "Iteration:  95% 366/385 [00:53<00:02,  6.94it/s]\u001b[A\n",
            "Iteration:  95% 367/385 [00:54<00:02,  6.90it/s]\u001b[A\n",
            "Iteration:  96% 368/385 [00:54<00:02,  6.87it/s]\u001b[A\n",
            "Iteration:  96% 369/385 [00:54<00:02,  6.86it/s]\u001b[A\n",
            "Iteration:  96% 370/385 [00:54<00:02,  6.81it/s]\u001b[A\n",
            "Iteration:  96% 371/385 [00:54<00:02,  6.85it/s]\u001b[A\n",
            "Iteration:  97% 372/385 [00:54<00:01,  6.84it/s]\u001b[A\n",
            "Iteration:  97% 373/385 [00:54<00:01,  6.87it/s]\u001b[A\n",
            "Iteration:  97% 374/385 [00:55<00:01,  6.81it/s]\u001b[A\n",
            "Iteration:  97% 375/385 [00:55<00:01,  6.85it/s]\u001b[A\n",
            "Iteration:  98% 376/385 [00:55<00:01,  6.85it/s]\u001b[A\n",
            "Iteration:  98% 377/385 [00:55<00:01,  6.88it/s]\u001b[A\n",
            "Iteration:  98% 378/385 [00:55<00:01,  6.91it/s]\u001b[A\n",
            "Iteration:  98% 379/385 [00:55<00:00,  6.91it/s]\u001b[A\n",
            "Iteration:  99% 380/385 [00:55<00:00,  6.90it/s]\u001b[A\n",
            "Iteration:  99% 381/385 [00:56<00:00,  6.79it/s]\u001b[A\n",
            "Iteration:  99% 382/385 [00:56<00:00,  6.81it/s]\u001b[A\n",
            "Iteration:  99% 383/385 [00:56<00:00,  6.76it/s]\u001b[A\n",
            "Iteration: 100% 384/385 [00:56<00:00,  6.75it/s]\u001b[A\n",
            "Iteration: 100% 385/385 [00:56<00:00,  6.78it/s]\n",
            "Epoch:  20% 1/5 [00:56<03:46, 56.74s/it]\n",
            "Iteration:   0% 0/385 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/385 [00:00<00:55,  6.95it/s]\u001b[A\n",
            "Iteration:   1% 2/385 [00:00<00:55,  6.89it/s]\u001b[A\n",
            "Iteration:   1% 3/385 [00:00<00:55,  6.94it/s]\u001b[A\n",
            "Iteration:   1% 4/385 [00:00<00:55,  6.91it/s]\u001b[A\n",
            "Iteration:   1% 5/385 [00:00<00:55,  6.87it/s]\u001b[A\n",
            "Iteration:   2% 6/385 [00:00<00:55,  6.86it/s]\u001b[A\n",
            "Iteration:   2% 7/385 [00:01<00:54,  6.88it/s]\u001b[A\n",
            "Iteration:   2% 8/385 [00:01<00:54,  6.90it/s]\u001b[A\n",
            "Iteration:   2% 9/385 [00:01<00:55,  6.75it/s]\u001b[A\n",
            "Iteration:   3% 10/385 [00:01<00:55,  6.70it/s]\u001b[A\n",
            "Iteration:   3% 11/385 [00:01<00:56,  6.66it/s]\u001b[A\n",
            "Iteration:   3% 12/385 [00:01<00:55,  6.72it/s]\u001b[A\n",
            "Iteration:   3% 13/385 [00:01<00:55,  6.74it/s]\u001b[A\n",
            "Iteration:   4% 14/385 [00:02<00:54,  6.81it/s]\u001b[A\n",
            "Iteration:   4% 15/385 [00:02<00:54,  6.78it/s]\u001b[A\n",
            "Iteration:   4% 16/385 [00:02<00:54,  6.80it/s]\u001b[A\n",
            "Iteration:   4% 17/385 [00:02<00:54,  6.81it/s]\u001b[A\n",
            "Iteration:   5% 18/385 [00:02<00:53,  6.85it/s]\u001b[A\n",
            "Iteration:   5% 19/385 [00:02<00:53,  6.87it/s]\u001b[A\n",
            "Iteration:   5% 20/385 [00:02<00:53,  6.83it/s]\u001b[A\n",
            "Iteration:   5% 21/385 [00:03<00:53,  6.82it/s]\u001b[A\n",
            "Iteration:   6% 22/385 [00:03<00:53,  6.81it/s]\u001b[A\n",
            "Iteration:   6% 23/385 [00:03<00:53,  6.83it/s]\u001b[A\n",
            "Iteration:   6% 24/385 [00:03<00:52,  6.82it/s]\u001b[A\n",
            "Iteration:   6% 25/385 [00:03<00:52,  6.85it/s]\u001b[A\n",
            "Iteration:   7% 26/385 [00:03<00:52,  6.83it/s]\u001b[A\n",
            "Iteration:   7% 27/385 [00:03<00:52,  6.83it/s]\u001b[A\n",
            "Iteration:   7% 28/385 [00:04<00:51,  6.87it/s]\u001b[A\n",
            "Iteration:   8% 29/385 [00:04<00:51,  6.89it/s]\u001b[A\n",
            "Iteration:   8% 30/385 [00:04<00:51,  6.89it/s]\u001b[A\n",
            "Iteration:   8% 31/385 [00:04<00:52,  6.79it/s]\u001b[A\n",
            "Iteration:   8% 32/385 [00:04<00:51,  6.80it/s]\u001b[A\n",
            "Iteration:   9% 33/385 [00:04<00:51,  6.81it/s]\u001b[A\n",
            "Iteration:   9% 34/385 [00:04<00:51,  6.85it/s]\u001b[A\n",
            "Iteration:   9% 35/385 [00:05<00:51,  6.75it/s]\u001b[A\n",
            "Iteration:   9% 36/385 [00:05<00:51,  6.78it/s]\u001b[A\n",
            "Iteration:  10% 37/385 [00:05<00:51,  6.70it/s]\u001b[A\n",
            "Iteration:  10% 38/385 [00:05<00:51,  6.79it/s]\u001b[A\n",
            "Iteration:  10% 39/385 [00:05<00:51,  6.75it/s]\u001b[A\n",
            "Iteration:  10% 40/385 [00:05<00:50,  6.80it/s]\u001b[A\n",
            "Iteration:  11% 41/385 [00:06<00:50,  6.83it/s]\u001b[A\n",
            "Iteration:  11% 42/385 [00:06<00:50,  6.83it/s]\u001b[A\n",
            "Iteration:  11% 43/385 [00:06<00:49,  6.84it/s]\u001b[A\n",
            "Iteration:  11% 44/385 [00:06<00:50,  6.82it/s]\u001b[A\n",
            "Iteration:  12% 45/385 [00:06<00:49,  6.81it/s]\u001b[A\n",
            "Iteration:  12% 46/385 [00:06<00:49,  6.84it/s]\u001b[A\n",
            "Iteration:  12% 47/385 [00:06<00:49,  6.85it/s]\u001b[A\n",
            "Iteration:  12% 48/385 [00:07<00:49,  6.85it/s]\u001b[A\n",
            "Iteration:  13% 49/385 [00:07<00:49,  6.79it/s]\u001b[A\n",
            "Iteration:  13% 50/385 [00:07<00:49,  6.83it/s]\u001b[A\n",
            "Iteration:  13% 51/385 [00:07<00:48,  6.86it/s]\u001b[A\n",
            "Iteration:  14% 52/385 [00:07<00:48,  6.88it/s]\u001b[A\n",
            "Iteration:  14% 53/385 [00:07<00:48,  6.86it/s]\u001b[A\n",
            "Iteration:  14% 54/385 [00:07<00:48,  6.87it/s]\u001b[A\n",
            "Iteration:  14% 55/385 [00:08<00:48,  6.83it/s]\u001b[A\n",
            "Iteration:  15% 56/385 [00:08<00:48,  6.80it/s]\u001b[A\n",
            "Iteration:  15% 57/385 [00:08<00:48,  6.81it/s]\u001b[A\n",
            "Iteration:  15% 58/385 [00:08<00:47,  6.85it/s]\u001b[A\n",
            "Iteration:  15% 59/385 [00:08<00:47,  6.86it/s]\u001b[A\n",
            "Iteration:  16% 60/385 [00:08<00:47,  6.86it/s]\u001b[A\n",
            "Iteration:  16% 61/385 [00:08<00:47,  6.82it/s]\u001b[A\n",
            "Iteration:  16% 62/385 [00:09<00:47,  6.83it/s]\u001b[A\n",
            "Iteration:  16% 63/385 [00:09<00:47,  6.84it/s]\u001b[A\n",
            "Iteration:  17% 64/385 [00:09<00:47,  6.76it/s]\u001b[A\n",
            "Iteration:  17% 65/385 [00:09<00:47,  6.76it/s]\u001b[A\n",
            "Iteration:  17% 66/385 [00:09<00:47,  6.76it/s]\u001b[A\n",
            "Iteration:  17% 67/385 [00:09<00:46,  6.82it/s]\u001b[A\n",
            "Iteration:  18% 68/385 [00:09<00:46,  6.80it/s]\u001b[A\n",
            "Iteration:  18% 69/385 [00:10<00:46,  6.79it/s]\u001b[A\n",
            "Iteration:  18% 70/385 [00:10<00:46,  6.72it/s]\u001b[A\n",
            "Iteration:  18% 71/385 [00:10<00:46,  6.81it/s]\u001b[A\n",
            "Iteration:  19% 72/385 [00:10<00:46,  6.80it/s]\u001b[A\n",
            "Iteration:  19% 73/385 [00:10<00:45,  6.80it/s]\u001b[A\n",
            "Iteration:  19% 74/385 [00:10<00:45,  6.82it/s]\u001b[A\n",
            "Iteration:  19% 75/385 [00:11<00:45,  6.83it/s]\u001b[A\n",
            "Iteration:  20% 76/385 [00:11<00:45,  6.84it/s]\u001b[A\n",
            "Iteration:  20% 77/385 [00:11<00:45,  6.81it/s]\u001b[A\n",
            "Iteration:  20% 78/385 [00:11<00:45,  6.79it/s]\u001b[A\n",
            "Iteration:  21% 79/385 [00:11<00:45,  6.76it/s]\u001b[A\n",
            "Iteration:  21% 80/385 [00:11<00:44,  6.79it/s]\u001b[A\n",
            "Iteration:  21% 81/385 [00:11<00:44,  6.78it/s]\u001b[A\n",
            "Iteration:  21% 82/385 [00:12<00:44,  6.78it/s]\u001b[A\n",
            "Iteration:  22% 83/385 [00:12<00:44,  6.80it/s]\u001b[A\n",
            "Iteration:  22% 84/385 [00:12<00:44,  6.80it/s]\u001b[A\n",
            "Iteration:  22% 85/385 [00:12<00:43,  6.82it/s]\u001b[A\n",
            "Iteration:  22% 86/385 [00:12<00:44,  6.71it/s]\u001b[A\n",
            "Iteration:  23% 87/385 [00:12<00:43,  6.81it/s]\u001b[A\n",
            "Iteration:  23% 88/385 [00:12<00:43,  6.82it/s]\u001b[A\n",
            "Iteration:  23% 89/385 [00:13<00:43,  6.79it/s]\u001b[A\n",
            "Iteration:  23% 90/385 [00:13<00:43,  6.77it/s]\u001b[A\n",
            "Iteration:  24% 91/385 [00:13<00:43,  6.80it/s]\u001b[A\n",
            "Iteration:  24% 92/385 [00:13<00:42,  6.83it/s]\u001b[A\n",
            "Iteration:  24% 93/385 [00:13<00:42,  6.84it/s]\u001b[A\n",
            "Iteration:  24% 94/385 [00:13<00:42,  6.83it/s]\u001b[A\n",
            "Iteration:  25% 95/385 [00:13<00:42,  6.80it/s]\u001b[A\n",
            "Iteration:  25% 96/385 [00:14<00:42,  6.84it/s]\u001b[A\n",
            "Iteration:  25% 97/385 [00:14<00:42,  6.80it/s]\u001b[A\n",
            "Iteration:  25% 98/385 [00:14<00:42,  6.82it/s]\u001b[A\n",
            "Iteration:  26% 99/385 [00:14<00:42,  6.74it/s]\u001b[A\n",
            "Iteration:  26% 100/385 [00:14<00:41,  6.82it/s]\u001b[A\n",
            "Iteration:  26% 101/385 [00:14<00:41,  6.89it/s]\u001b[A\n",
            "Iteration:  26% 102/385 [00:14<00:41,  6.86it/s]\u001b[A\n",
            "Iteration:  27% 103/385 [00:15<00:41,  6.86it/s]\u001b[A\n",
            "Iteration:  27% 104/385 [00:15<00:41,  6.82it/s]\u001b[A\n",
            "Iteration:  27% 105/385 [00:15<00:41,  6.79it/s]\u001b[A\n",
            "Iteration:  28% 106/385 [00:15<00:41,  6.71it/s]\u001b[A\n",
            "Iteration:  28% 107/385 [00:15<00:41,  6.72it/s]\u001b[A\n",
            "Iteration:  28% 108/385 [00:15<00:41,  6.69it/s]\u001b[A\n",
            "Iteration:  28% 109/385 [00:16<00:40,  6.74it/s]\u001b[A\n",
            "Iteration:  29% 110/385 [00:16<00:41,  6.66it/s]\u001b[A\n",
            "Iteration:  29% 111/385 [00:16<00:40,  6.74it/s]\u001b[A\n",
            "Iteration:  29% 112/385 [00:16<00:40,  6.78it/s]\u001b[A\n",
            "Iteration:  29% 113/385 [00:16<00:40,  6.79it/s]\u001b[A\n",
            "Iteration:  30% 114/385 [00:16<00:40,  6.75it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "02/12/2021 08:39:01 - INFO - transformers.configuration_utils -   Configuration saved in neurohoroscope_model_wt_sign_0/checkpoint-500/config.json\n",
            "02/12/2021 08:39:03 - INFO - transformers.modeling_utils -   Model weights saved in neurohoroscope_model_wt_sign_0/checkpoint-500/pytorch_model.bin\n",
            "02/12/2021 08:39:03 - INFO - __main__ -   Saving model checkpoint to neurohoroscope_model_wt_sign_0/checkpoint-500\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "02/12/2021 08:39:10 - INFO - __main__ -   Saving optimizer and scheduler states to neurohoroscope_model_wt_sign_0/checkpoint-500\n",
            "\n",
            "Iteration:  30% 115/385 [00:26<13:19,  2.96s/it]\u001b[A\n",
            "Iteration:  30% 116/385 [00:26<09:31,  2.12s/it]\u001b[A\n",
            "Iteration:  30% 117/385 [00:26<06:50,  1.53s/it]\u001b[A\n",
            "Iteration:  31% 118/385 [00:26<04:58,  1.12s/it]\u001b[A\n",
            "Iteration:  31% 119/385 [00:26<03:39,  1.21it/s]\u001b[A\n",
            "Iteration:  31% 120/385 [00:27<02:44,  1.61it/s]\u001b[A\n",
            "Iteration:  31% 121/385 [00:27<02:06,  2.09it/s]\u001b[A\n",
            "Iteration:  32% 122/385 [00:27<01:39,  2.64it/s]\u001b[A\n",
            "Iteration:  32% 123/385 [00:27<01:21,  3.23it/s]\u001b[A\n",
            "Iteration:  32% 124/385 [00:27<01:07,  3.85it/s]\u001b[A\n",
            "Iteration:  32% 125/385 [00:27<00:58,  4.43it/s]\u001b[A\n",
            "Iteration:  33% 126/385 [00:27<00:52,  4.95it/s]\u001b[A\n",
            "Iteration:  33% 127/385 [00:28<00:48,  5.34it/s]\u001b[A\n",
            "Iteration:  33% 128/385 [00:28<00:44,  5.73it/s]\u001b[A\n",
            "Iteration:  34% 129/385 [00:28<00:42,  6.04it/s]\u001b[A\n",
            "Iteration:  34% 130/385 [00:28<00:40,  6.26it/s]\u001b[A\n",
            "Iteration:  34% 131/385 [00:28<00:39,  6.42it/s]\u001b[A\n",
            "Iteration:  34% 132/385 [00:28<00:38,  6.50it/s]\u001b[A\n",
            "Iteration:  35% 133/385 [00:28<00:38,  6.62it/s]\u001b[A\n",
            "Iteration:  35% 134/385 [00:29<00:37,  6.65it/s]\u001b[A\n",
            "Iteration:  35% 135/385 [00:29<00:37,  6.69it/s]\u001b[A\n",
            "Iteration:  35% 136/385 [00:29<00:37,  6.67it/s]\u001b[A\n",
            "Iteration:  36% 137/385 [00:29<00:36,  6.71it/s]\u001b[A\n",
            "Iteration:  36% 138/385 [00:29<00:36,  6.76it/s]\u001b[A\n",
            "Iteration:  36% 139/385 [00:29<00:36,  6.75it/s]\u001b[A\n",
            "Iteration:  36% 140/385 [00:29<00:36,  6.75it/s]\u001b[A\n",
            "Iteration:  37% 141/385 [00:30<00:35,  6.78it/s]\u001b[A\n",
            "Iteration:  37% 142/385 [00:30<00:35,  6.82it/s]\u001b[A\n",
            "Iteration:  37% 143/385 [00:30<00:35,  6.84it/s]\u001b[A\n",
            "Iteration:  37% 144/385 [00:30<00:35,  6.82it/s]\u001b[A\n",
            "Iteration:  38% 145/385 [00:30<00:35,  6.78it/s]\u001b[A\n",
            "Iteration:  38% 146/385 [00:30<00:35,  6.82it/s]\u001b[A\n",
            "Iteration:  38% 147/385 [00:31<00:34,  6.80it/s]\u001b[A\n",
            "Iteration:  38% 148/385 [00:31<00:34,  6.85it/s]\u001b[A\n",
            "Iteration:  39% 149/385 [00:31<00:34,  6.85it/s]\u001b[A\n",
            "Iteration:  39% 150/385 [00:31<00:34,  6.83it/s]\u001b[A\n",
            "Iteration:  39% 151/385 [00:31<00:35,  6.59it/s]\u001b[A\n",
            "Iteration:  39% 152/385 [00:31<00:35,  6.64it/s]\u001b[A\n",
            "Iteration:  40% 153/385 [00:31<00:34,  6.72it/s]\u001b[A\n",
            "Iteration:  40% 154/385 [00:32<00:34,  6.72it/s]\u001b[A\n",
            "Iteration:  40% 155/385 [00:32<00:34,  6.70it/s]\u001b[A\n",
            "Iteration:  41% 156/385 [00:32<00:33,  6.76it/s]\u001b[A\n",
            "Iteration:  41% 157/385 [00:32<00:33,  6.78it/s]\u001b[A\n",
            "Iteration:  41% 158/385 [00:32<00:33,  6.80it/s]\u001b[A\n",
            "Iteration:  41% 159/385 [00:32<00:33,  6.77it/s]\u001b[A\n",
            "Iteration:  42% 160/385 [00:32<00:34,  6.48it/s]\u001b[A\n",
            "Iteration:  42% 161/385 [00:33<00:33,  6.62it/s]\u001b[A\n",
            "Iteration:  42% 162/385 [00:33<00:33,  6.66it/s]\u001b[A\n",
            "Iteration:  42% 163/385 [00:33<00:33,  6.71it/s]\u001b[A\n",
            "Iteration:  43% 164/385 [00:33<00:33,  6.70it/s]\u001b[A\n",
            "Iteration:  43% 165/385 [00:33<00:32,  6.75it/s]\u001b[A\n",
            "Iteration:  43% 166/385 [00:33<00:32,  6.80it/s]\u001b[A\n",
            "Iteration:  43% 167/385 [00:33<00:32,  6.70it/s]\u001b[A\n",
            "Iteration:  44% 168/385 [00:34<00:32,  6.60it/s]\u001b[A\n",
            "Iteration:  44% 169/385 [00:34<00:32,  6.69it/s]\u001b[A\n",
            "Iteration:  44% 170/385 [00:34<00:32,  6.69it/s]\u001b[A\n",
            "Iteration:  44% 171/385 [00:34<00:31,  6.73it/s]\u001b[A\n",
            "Iteration:  45% 172/385 [00:34<00:31,  6.77it/s]\u001b[A\n",
            "Iteration:  45% 173/385 [00:34<00:31,  6.71it/s]\u001b[A\n",
            "Iteration:  45% 174/385 [00:35<00:31,  6.71it/s]\u001b[A\n",
            "Iteration:  45% 175/385 [00:35<00:31,  6.75it/s]\u001b[A\n",
            "Iteration:  46% 176/385 [00:35<00:30,  6.76it/s]\u001b[A\n",
            "Iteration:  46% 177/385 [00:35<00:31,  6.68it/s]\u001b[A\n",
            "Iteration:  46% 178/385 [00:35<00:30,  6.75it/s]\u001b[A\n",
            "Iteration:  46% 179/385 [00:35<00:30,  6.70it/s]\u001b[A\n",
            "Iteration:  47% 180/385 [00:35<00:30,  6.74it/s]\u001b[A\n",
            "Iteration:  47% 181/385 [00:36<00:30,  6.63it/s]\u001b[A\n",
            "Iteration:  47% 182/385 [00:36<00:29,  6.80it/s]\u001b[A\n",
            "Iteration:  48% 183/385 [00:36<00:30,  6.65it/s]\u001b[A\n",
            "Iteration:  48% 184/385 [00:36<00:29,  6.77it/s]\u001b[A\n",
            "Iteration:  48% 185/385 [00:36<00:29,  6.74it/s]\u001b[A\n",
            "Iteration:  48% 186/385 [00:36<00:29,  6.83it/s]\u001b[A\n",
            "Iteration:  49% 187/385 [00:36<00:28,  6.83it/s]\u001b[A\n",
            "Iteration:  49% 188/385 [00:37<00:29,  6.75it/s]\u001b[A\n",
            "Iteration:  49% 189/385 [00:37<00:29,  6.68it/s]\u001b[A\n",
            "Iteration:  49% 190/385 [00:37<00:28,  6.74it/s]\u001b[A\n",
            "Iteration:  50% 191/385 [00:37<00:28,  6.70it/s]\u001b[A\n",
            "Iteration:  50% 192/385 [00:37<00:28,  6.75it/s]\u001b[A\n",
            "Iteration:  50% 193/385 [00:37<00:28,  6.80it/s]\u001b[A\n",
            "Iteration:  50% 194/385 [00:37<00:28,  6.78it/s]\u001b[A\n",
            "Iteration:  51% 195/385 [00:38<00:28,  6.78it/s]\u001b[A\n",
            "Iteration:  51% 196/385 [00:38<00:27,  6.75it/s]\u001b[A\n",
            "Iteration:  51% 197/385 [00:38<00:27,  6.78it/s]\u001b[A\n",
            "Iteration:  51% 198/385 [00:38<00:27,  6.81it/s]\u001b[A\n",
            "Iteration:  52% 199/385 [00:38<00:27,  6.79it/s]\u001b[A\n",
            "Iteration:  52% 200/385 [00:38<00:27,  6.72it/s]\u001b[A\n",
            "Iteration:  52% 201/385 [00:39<00:27,  6.75it/s]\u001b[A\n",
            "Iteration:  52% 202/385 [00:39<00:27,  6.74it/s]\u001b[A\n",
            "Iteration:  53% 203/385 [00:39<00:27,  6.71it/s]\u001b[A\n",
            "Iteration:  53% 204/385 [00:39<00:26,  6.75it/s]\u001b[A\n",
            "Iteration:  53% 205/385 [00:39<00:26,  6.79it/s]\u001b[A\n",
            "Iteration:  54% 206/385 [00:39<00:26,  6.82it/s]\u001b[A\n",
            "Iteration:  54% 207/385 [00:39<00:26,  6.79it/s]\u001b[A\n",
            "Iteration:  54% 208/385 [00:40<00:26,  6.76it/s]\u001b[A\n",
            "Iteration:  54% 209/385 [00:40<00:26,  6.71it/s]\u001b[A\n",
            "Iteration:  55% 210/385 [00:40<00:25,  6.75it/s]\u001b[A\n",
            "Iteration:  55% 211/385 [00:40<00:26,  6.67it/s]\u001b[A\n",
            "Iteration:  55% 212/385 [00:40<00:25,  6.74it/s]\u001b[A\n",
            "Iteration:  55% 213/385 [00:40<00:25,  6.70it/s]\u001b[A\n",
            "Iteration:  56% 214/385 [00:40<00:25,  6.75it/s]\u001b[A\n",
            "Iteration:  56% 215/385 [00:41<00:25,  6.73it/s]\u001b[A\n",
            "Iteration:  56% 216/385 [00:41<00:24,  6.77it/s]\u001b[A\n",
            "Iteration:  56% 217/385 [00:41<00:24,  6.73it/s]\u001b[A\n",
            "Iteration:  57% 218/385 [00:41<00:24,  6.75it/s]\u001b[A\n",
            "Iteration:  57% 219/385 [00:41<00:24,  6.73it/s]\u001b[A\n",
            "Iteration:  57% 220/385 [00:41<00:24,  6.80it/s]\u001b[A\n",
            "Iteration:  57% 221/385 [00:41<00:24,  6.82it/s]\u001b[A\n",
            "Iteration:  58% 222/385 [00:42<00:24,  6.79it/s]\u001b[A\n",
            "Iteration:  58% 223/385 [00:42<00:23,  6.75it/s]\u001b[A\n",
            "Iteration:  58% 224/385 [00:42<00:23,  6.76it/s]\u001b[A\n",
            "Iteration:  58% 225/385 [00:42<00:23,  6.78it/s]\u001b[A\n",
            "Iteration:  59% 226/385 [00:42<00:23,  6.76it/s]\u001b[A\n",
            "Iteration:  59% 227/385 [00:42<00:23,  6.75it/s]\u001b[A\n",
            "Iteration:  59% 228/385 [00:43<00:23,  6.73it/s]\u001b[A\n",
            "Iteration:  59% 229/385 [00:43<00:23,  6.67it/s]\u001b[A\n",
            "Iteration:  60% 230/385 [00:43<00:23,  6.66it/s]\u001b[A\n",
            "Iteration:  60% 231/385 [00:43<00:22,  6.75it/s]\u001b[A\n",
            "Iteration:  60% 232/385 [00:43<00:22,  6.67it/s]\u001b[A\n",
            "Iteration:  61% 233/385 [00:43<00:22,  6.72it/s]\u001b[A\n",
            "Iteration:  61% 234/385 [00:43<00:22,  6.72it/s]\u001b[A\n",
            "Iteration:  61% 235/385 [00:44<00:22,  6.78it/s]\u001b[A\n",
            "Iteration:  61% 236/385 [00:44<00:22,  6.76it/s]\u001b[A\n",
            "Iteration:  62% 237/385 [00:44<00:21,  6.74it/s]\u001b[A\n",
            "Iteration:  62% 238/385 [00:44<00:21,  6.75it/s]\u001b[A\n",
            "Iteration:  62% 239/385 [00:44<00:21,  6.77it/s]\u001b[A\n",
            "Iteration:  62% 240/385 [00:44<00:21,  6.74it/s]\u001b[A\n",
            "Iteration:  63% 241/385 [00:44<00:21,  6.75it/s]\u001b[A\n",
            "Iteration:  63% 242/385 [00:45<00:21,  6.76it/s]\u001b[A\n",
            "Iteration:  63% 243/385 [00:45<00:20,  6.77it/s]\u001b[A\n",
            "Iteration:  63% 244/385 [00:45<00:20,  6.74it/s]\u001b[A\n",
            "Iteration:  64% 245/385 [00:45<00:20,  6.70it/s]\u001b[A\n",
            "Iteration:  64% 246/385 [00:45<00:20,  6.71it/s]\u001b[A\n",
            "Iteration:  64% 247/385 [00:45<00:20,  6.66it/s]\u001b[A\n",
            "Iteration:  64% 248/385 [00:46<00:20,  6.72it/s]\u001b[A\n",
            "Iteration:  65% 249/385 [00:46<00:20,  6.70it/s]\u001b[A\n",
            "Iteration:  65% 250/385 [00:46<00:20,  6.69it/s]\u001b[A\n",
            "Iteration:  65% 251/385 [00:46<00:20,  6.69it/s]\u001b[A\n",
            "Iteration:  65% 252/385 [00:46<00:19,  6.78it/s]\u001b[A\n",
            "Iteration:  66% 253/385 [00:46<00:19,  6.72it/s]\u001b[A\n",
            "Iteration:  66% 254/385 [00:46<00:19,  6.75it/s]\u001b[A\n",
            "Iteration:  66% 255/385 [00:47<00:19,  6.76it/s]\u001b[A\n",
            "Iteration:  66% 256/385 [00:47<00:19,  6.78it/s]\u001b[A\n",
            "Iteration:  67% 257/385 [00:47<00:18,  6.75it/s]\u001b[A\n",
            "Iteration:  67% 258/385 [00:47<00:18,  6.79it/s]\u001b[A\n",
            "Iteration:  67% 259/385 [00:47<00:18,  6.76it/s]\u001b[A\n",
            "Iteration:  68% 260/385 [00:47<00:18,  6.75it/s]\u001b[A\n",
            "Iteration:  68% 261/385 [00:47<00:18,  6.70it/s]\u001b[A\n",
            "Iteration:  68% 262/385 [00:48<00:18,  6.69it/s]\u001b[A\n",
            "Iteration:  68% 263/385 [00:48<00:18,  6.66it/s]\u001b[A\n",
            "Iteration:  69% 264/385 [00:48<00:18,  6.63it/s]\u001b[A\n",
            "Iteration:  69% 265/385 [00:48<00:17,  6.69it/s]\u001b[A\n",
            "Iteration:  69% 266/385 [00:48<00:17,  6.65it/s]\u001b[A\n",
            "Iteration:  69% 267/385 [00:48<00:17,  6.67it/s]\u001b[A\n",
            "Iteration:  70% 268/385 [00:48<00:17,  6.65it/s]\u001b[A\n",
            "Iteration:  70% 269/385 [00:49<00:17,  6.72it/s]\u001b[A\n",
            "Iteration:  70% 270/385 [00:49<00:17,  6.63it/s]\u001b[A\n",
            "Iteration:  70% 271/385 [00:49<00:17,  6.68it/s]\u001b[A\n",
            "Iteration:  71% 272/385 [00:49<00:17,  6.61it/s]\u001b[A\n",
            "Iteration:  71% 273/385 [00:49<00:16,  6.68it/s]\u001b[A\n",
            "Iteration:  71% 274/385 [00:49<00:16,  6.64it/s]\u001b[A\n",
            "Iteration:  71% 275/385 [00:50<00:16,  6.68it/s]\u001b[A\n",
            "Iteration:  72% 276/385 [00:50<00:16,  6.68it/s]\u001b[A\n",
            "Iteration:  72% 277/385 [00:50<00:16,  6.65it/s]\u001b[A\n",
            "Iteration:  72% 278/385 [00:50<00:16,  6.62it/s]\u001b[A\n",
            "Iteration:  72% 279/385 [00:50<00:15,  6.65it/s]\u001b[A\n",
            "Iteration:  73% 280/385 [00:50<00:15,  6.60it/s]\u001b[A\n",
            "Iteration:  73% 281/385 [00:50<00:15,  6.67it/s]\u001b[A\n",
            "Iteration:  73% 282/385 [00:51<00:15,  6.58it/s]\u001b[A\n",
            "Iteration:  74% 283/385 [00:51<00:15,  6.66it/s]\u001b[A\n",
            "Iteration:  74% 284/385 [00:51<00:15,  6.62it/s]\u001b[A\n",
            "Iteration:  74% 285/385 [00:51<00:15,  6.67it/s]\u001b[A\n",
            "Iteration:  74% 286/385 [00:51<00:14,  6.66it/s]\u001b[A\n",
            "Iteration:  75% 287/385 [00:51<00:14,  6.65it/s]\u001b[A\n",
            "Iteration:  75% 288/385 [00:51<00:14,  6.62it/s]\u001b[A\n",
            "Iteration:  75% 289/385 [00:52<00:14,  6.62it/s]\u001b[A\n",
            "Iteration:  75% 290/385 [00:52<00:14,  6.58it/s]\u001b[A\n",
            "Iteration:  76% 291/385 [00:52<00:14,  6.61it/s]\u001b[A\n",
            "Iteration:  76% 292/385 [00:52<00:14,  6.56it/s]\u001b[A\n",
            "Iteration:  76% 293/385 [00:52<00:13,  6.59it/s]\u001b[A\n",
            "Iteration:  76% 294/385 [00:52<00:13,  6.59it/s]\u001b[A\n",
            "Iteration:  77% 295/385 [00:53<00:13,  6.65it/s]\u001b[A\n",
            "Iteration:  77% 296/385 [00:53<00:13,  6.61it/s]\u001b[A\n",
            "Iteration:  77% 297/385 [00:53<00:13,  6.68it/s]\u001b[A\n",
            "Iteration:  77% 298/385 [00:53<00:13,  6.60it/s]\u001b[A\n",
            "Iteration:  78% 299/385 [00:53<00:12,  6.66it/s]\u001b[A\n",
            "Iteration:  78% 300/385 [00:53<00:12,  6.59it/s]\u001b[A\n",
            "Iteration:  78% 301/385 [00:53<00:12,  6.65it/s]\u001b[A\n",
            "Iteration:  78% 302/385 [00:54<00:12,  6.63it/s]\u001b[A\n",
            "Iteration:  79% 303/385 [00:54<00:12,  6.65it/s]\u001b[A\n",
            "Iteration:  79% 304/385 [00:54<00:12,  6.59it/s]\u001b[A\n",
            "Iteration:  79% 305/385 [00:54<00:12,  6.62it/s]\u001b[A\n",
            "Iteration:  79% 306/385 [00:54<00:11,  6.61it/s]\u001b[A\n",
            "Iteration:  80% 307/385 [00:54<00:11,  6.67it/s]\u001b[A\n",
            "Iteration:  80% 308/385 [00:55<00:11,  6.57it/s]\u001b[A\n",
            "Iteration:  80% 309/385 [00:55<00:11,  6.64it/s]\u001b[A\n",
            "Iteration:  81% 310/385 [00:55<00:11,  6.57it/s]\u001b[A\n",
            "Iteration:  81% 311/385 [00:55<00:11,  6.66it/s]\u001b[A\n",
            "Iteration:  81% 312/385 [00:55<00:11,  6.62it/s]\u001b[A\n",
            "Iteration:  81% 313/385 [00:55<00:10,  6.66it/s]\u001b[A\n",
            "Iteration:  82% 314/385 [00:55<00:10,  6.64it/s]\u001b[A\n",
            "Iteration:  82% 315/385 [00:56<00:10,  6.62it/s]\u001b[A\n",
            "Iteration:  82% 316/385 [00:56<00:10,  6.61it/s]\u001b[A\n",
            "Iteration:  82% 317/385 [00:56<00:10,  6.63it/s]\u001b[A\n",
            "Iteration:  83% 318/385 [00:56<00:10,  6.61it/s]\u001b[A\n",
            "Iteration:  83% 319/385 [00:56<00:09,  6.65it/s]\u001b[A\n",
            "Iteration:  83% 320/385 [00:56<00:09,  6.57it/s]\u001b[A\n",
            "Iteration:  83% 321/385 [00:56<00:09,  6.61it/s]\u001b[A\n",
            "Iteration:  84% 322/385 [00:57<00:09,  6.58it/s]\u001b[A\n",
            "Iteration:  84% 323/385 [00:57<00:09,  6.60it/s]\u001b[A\n",
            "Iteration:  84% 324/385 [00:57<00:09,  6.59it/s]\u001b[A\n",
            "Iteration:  84% 325/385 [00:57<00:09,  6.65it/s]\u001b[A\n",
            "Iteration:  85% 326/385 [00:57<00:08,  6.58it/s]\u001b[A\n",
            "Iteration:  85% 327/385 [00:57<00:08,  6.65it/s]\u001b[A\n",
            "Iteration:  85% 328/385 [00:58<00:08,  6.60it/s]\u001b[A\n",
            "Iteration:  85% 329/385 [00:58<00:08,  6.65it/s]\u001b[A\n",
            "Iteration:  86% 330/385 [00:58<00:08,  6.59it/s]\u001b[A\n",
            "Iteration:  86% 331/385 [00:58<00:08,  6.64it/s]\u001b[A\n",
            "Iteration:  86% 332/385 [00:58<00:07,  6.66it/s]\u001b[A\n",
            "Iteration:  86% 333/385 [00:58<00:07,  6.68it/s]\u001b[A\n",
            "Iteration:  87% 334/385 [00:58<00:07,  6.64it/s]\u001b[A\n",
            "Iteration:  87% 335/385 [00:59<00:07,  6.69it/s]\u001b[A\n",
            "Iteration:  87% 336/385 [00:59<00:07,  6.64it/s]\u001b[A\n",
            "Iteration:  88% 337/385 [00:59<00:07,  6.68it/s]\u001b[A\n",
            "Iteration:  88% 338/385 [00:59<00:07,  6.61it/s]\u001b[A\n",
            "Iteration:  88% 339/385 [00:59<00:06,  6.67it/s]\u001b[A\n",
            "Iteration:  88% 340/385 [00:59<00:06,  6.66it/s]\u001b[A\n",
            "Iteration:  89% 341/385 [00:59<00:06,  6.65it/s]\u001b[A\n",
            "Iteration:  89% 342/385 [01:00<00:06,  6.59it/s]\u001b[A\n",
            "Iteration:  89% 343/385 [01:00<00:06,  6.60it/s]\u001b[A\n",
            "Iteration:  89% 344/385 [01:00<00:06,  6.60it/s]\u001b[A\n",
            "Iteration:  90% 345/385 [01:00<00:06,  6.63it/s]\u001b[A\n",
            "Iteration:  90% 346/385 [01:00<00:05,  6.56it/s]\u001b[A\n",
            "Iteration:  90% 347/385 [01:00<00:05,  6.56it/s]\u001b[A\n",
            "Iteration:  90% 348/385 [01:01<00:05,  6.60it/s]\u001b[A\n",
            "Iteration:  91% 349/385 [01:01<00:05,  6.64it/s]\u001b[A\n",
            "Iteration:  91% 350/385 [01:01<00:05,  6.62it/s]\u001b[A\n",
            "Iteration:  91% 351/385 [01:01<00:05,  6.65it/s]\u001b[A\n",
            "Iteration:  91% 352/385 [01:01<00:04,  6.63it/s]\u001b[A\n",
            "Iteration:  92% 353/385 [01:01<00:04,  6.67it/s]\u001b[A\n",
            "Iteration:  92% 354/385 [01:01<00:04,  6.60it/s]\u001b[A\n",
            "Iteration:  92% 355/385 [01:02<00:04,  6.63it/s]\u001b[A\n",
            "Iteration:  92% 356/385 [01:02<00:04,  6.61it/s]\u001b[A\n",
            "Iteration:  93% 357/385 [01:02<00:04,  6.54it/s]\u001b[A\n",
            "Iteration:  93% 358/385 [01:02<00:04,  6.61it/s]\u001b[A\n",
            "Iteration:  93% 359/385 [01:02<00:03,  6.61it/s]\u001b[A\n",
            "Iteration:  94% 360/385 [01:02<00:03,  6.63it/s]\u001b[A\n",
            "Iteration:  94% 361/385 [01:03<00:03,  6.64it/s]\u001b[A\n",
            "Iteration:  94% 362/385 [01:03<00:03,  6.67it/s]\u001b[A\n",
            "Iteration:  94% 363/385 [01:03<00:03,  6.71it/s]\u001b[A\n",
            "Iteration:  95% 364/385 [01:03<00:03,  6.64it/s]\u001b[A\n",
            "Iteration:  95% 365/385 [01:03<00:02,  6.69it/s]\u001b[A\n",
            "Iteration:  95% 366/385 [01:03<00:02,  6.61it/s]\u001b[A\n",
            "Iteration:  95% 367/385 [01:03<00:02,  6.67it/s]\u001b[A\n",
            "Iteration:  96% 368/385 [01:04<00:02,  6.65it/s]\u001b[A\n",
            "Iteration:  96% 369/385 [01:04<00:02,  6.67it/s]\u001b[A\n",
            "Iteration:  96% 370/385 [01:04<00:02,  6.63it/s]\u001b[A\n",
            "Iteration:  96% 371/385 [01:04<00:02,  6.57it/s]\u001b[A\n",
            "Iteration:  97% 372/385 [01:04<00:01,  6.60it/s]\u001b[A\n",
            "Iteration:  97% 373/385 [01:04<00:01,  6.61it/s]\u001b[A\n",
            "Iteration:  97% 374/385 [01:04<00:01,  6.59it/s]\u001b[A\n",
            "Iteration:  97% 375/385 [01:05<00:01,  6.61it/s]\u001b[A\n",
            "Iteration:  98% 376/385 [01:05<00:01,  6.62it/s]\u001b[A\n",
            "Iteration:  98% 377/385 [01:05<00:01,  6.65it/s]\u001b[A\n",
            "Iteration:  98% 378/385 [01:05<00:01,  6.61it/s]\u001b[A\n",
            "Iteration:  98% 379/385 [01:05<00:00,  6.66it/s]\u001b[A\n",
            "Iteration:  99% 380/385 [01:05<00:00,  6.61it/s]\u001b[A\n",
            "Iteration:  99% 381/385 [01:06<00:00,  6.63it/s]\u001b[A\n",
            "Iteration:  99% 382/385 [01:06<00:00,  6.62it/s]\u001b[A\n",
            "Iteration:  99% 383/385 [01:06<00:00,  6.62it/s]\u001b[A\n",
            "Iteration: 100% 384/385 [01:06<00:00,  6.62it/s]\u001b[A\n",
            "Iteration: 100% 385/385 [01:06<00:00,  5.78it/s]\n",
            "Epoch:  40% 2/5 [02:03<02:59, 59.71s/it]\n",
            "Iteration:   0% 0/385 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/385 [00:00<00:58,  6.56it/s]\u001b[A\n",
            "Iteration:   1% 2/385 [00:00<00:58,  6.59it/s]\u001b[A\n",
            "Iteration:   1% 3/385 [00:00<00:58,  6.58it/s]\u001b[A\n",
            "Iteration:   1% 4/385 [00:00<00:57,  6.61it/s]\u001b[A\n",
            "Iteration:   1% 5/385 [00:00<00:57,  6.55it/s]\u001b[A\n",
            "Iteration:   2% 6/385 [00:00<00:57,  6.61it/s]\u001b[A\n",
            "Iteration:   2% 7/385 [00:01<00:57,  6.57it/s]\u001b[A\n",
            "Iteration:   2% 8/385 [00:01<00:57,  6.60it/s]\u001b[A\n",
            "Iteration:   2% 9/385 [00:01<00:57,  6.58it/s]\u001b[A\n",
            "Iteration:   3% 10/385 [00:01<00:56,  6.64it/s]\u001b[A\n",
            "Iteration:   3% 11/385 [00:01<00:56,  6.61it/s]\u001b[A\n",
            "Iteration:   3% 12/385 [00:01<00:56,  6.64it/s]\u001b[A\n",
            "Iteration:   3% 13/385 [00:01<00:56,  6.57it/s]\u001b[A\n",
            "Iteration:   4% 14/385 [00:02<00:56,  6.61it/s]\u001b[A\n",
            "Iteration:   4% 15/385 [00:02<00:55,  6.61it/s]\u001b[A\n",
            "Iteration:   4% 16/385 [00:02<00:55,  6.61it/s]\u001b[A\n",
            "Iteration:   4% 17/385 [00:02<00:55,  6.59it/s]\u001b[A\n",
            "Iteration:   5% 18/385 [00:02<00:55,  6.58it/s]\u001b[A\n",
            "Iteration:   5% 19/385 [00:02<00:55,  6.58it/s]\u001b[A\n",
            "Iteration:   5% 20/385 [00:03<00:55,  6.59it/s]\u001b[A\n",
            "Iteration:   5% 21/385 [00:03<00:55,  6.60it/s]\u001b[A\n",
            "Iteration:   6% 22/385 [00:03<00:54,  6.60it/s]\u001b[A\n",
            "Iteration:   6% 23/385 [00:03<00:54,  6.60it/s]\u001b[A\n",
            "Iteration:   6% 24/385 [00:03<00:54,  6.64it/s]\u001b[A\n",
            "Iteration:   6% 25/385 [00:03<00:54,  6.60it/s]\u001b[A\n",
            "Iteration:   7% 26/385 [00:03<00:54,  6.59it/s]\u001b[A\n",
            "Iteration:   7% 27/385 [00:04<00:54,  6.51it/s]\u001b[A\n",
            "Iteration:   7% 28/385 [00:04<00:54,  6.59it/s]\u001b[A\n",
            "Iteration:   8% 29/385 [00:04<00:54,  6.59it/s]\u001b[A\n",
            "Iteration:   8% 30/385 [00:04<00:53,  6.64it/s]\u001b[A\n",
            "Iteration:   8% 31/385 [00:04<00:54,  6.55it/s]\u001b[A\n",
            "Iteration:   8% 32/385 [00:04<00:53,  6.60it/s]\u001b[A\n",
            "Iteration:   9% 33/385 [00:05<00:53,  6.55it/s]\u001b[A\n",
            "Iteration:   9% 34/385 [00:05<00:53,  6.57it/s]\u001b[A\n",
            "Iteration:   9% 35/385 [00:05<00:53,  6.55it/s]\u001b[A\n",
            "Iteration:   9% 36/385 [00:05<00:52,  6.60it/s]\u001b[A\n",
            "Iteration:  10% 37/385 [00:05<00:52,  6.62it/s]\u001b[A\n",
            "Iteration:  10% 38/385 [00:05<00:52,  6.63it/s]\u001b[A\n",
            "Iteration:  10% 39/385 [00:05<00:52,  6.55it/s]\u001b[A\n",
            "Iteration:  10% 40/385 [00:06<00:52,  6.63it/s]\u001b[A\n",
            "Iteration:  11% 41/385 [00:06<00:51,  6.62it/s]\u001b[A\n",
            "Iteration:  11% 42/385 [00:06<00:51,  6.61it/s]\u001b[A\n",
            "Iteration:  11% 43/385 [00:06<00:51,  6.63it/s]\u001b[A\n",
            "Iteration:  11% 44/385 [00:06<00:51,  6.67it/s]\u001b[A\n",
            "Iteration:  12% 45/385 [00:06<00:51,  6.64it/s]\u001b[A\n",
            "Iteration:  12% 46/385 [00:06<00:50,  6.69it/s]\u001b[A\n",
            "Iteration:  12% 47/385 [00:07<00:50,  6.64it/s]\u001b[A\n",
            "Iteration:  12% 48/385 [00:07<00:50,  6.68it/s]\u001b[A\n",
            "Iteration:  13% 49/385 [00:07<00:50,  6.65it/s]\u001b[A\n",
            "Iteration:  13% 50/385 [00:07<00:50,  6.64it/s]\u001b[A\n",
            "Iteration:  13% 51/385 [00:07<00:50,  6.62it/s]\u001b[A\n",
            "Iteration:  14% 52/385 [00:07<00:50,  6.60it/s]\u001b[A\n",
            "Iteration:  14% 53/385 [00:08<00:50,  6.60it/s]\u001b[A\n",
            "Iteration:  14% 54/385 [00:08<00:49,  6.62it/s]\u001b[A\n",
            "Iteration:  14% 55/385 [00:08<00:49,  6.62it/s]\u001b[A\n",
            "Iteration:  15% 56/385 [00:08<00:49,  6.68it/s]\u001b[A\n",
            "Iteration:  15% 57/385 [00:08<00:49,  6.64it/s]\u001b[A\n",
            "Iteration:  15% 58/385 [00:08<00:48,  6.68it/s]\u001b[A\n",
            "Iteration:  15% 59/385 [00:08<00:49,  6.59it/s]\u001b[A\n",
            "Iteration:  16% 60/385 [00:09<00:48,  6.66it/s]\u001b[A\n",
            "Iteration:  16% 61/385 [00:09<00:49,  6.60it/s]\u001b[A\n",
            "Iteration:  16% 62/385 [00:09<00:48,  6.62it/s]\u001b[A\n",
            "Iteration:  16% 63/385 [00:09<00:48,  6.62it/s]\u001b[A\n",
            "Iteration:  17% 64/385 [00:09<00:48,  6.61it/s]\u001b[A\n",
            "Iteration:  17% 65/385 [00:09<00:48,  6.56it/s]\u001b[A\n",
            "Iteration:  17% 66/385 [00:09<00:48,  6.58it/s]\u001b[A\n",
            "Iteration:  17% 67/385 [00:10<00:48,  6.58it/s]\u001b[A\n",
            "Iteration:  18% 68/385 [00:10<00:48,  6.59it/s]\u001b[A\n",
            "Iteration:  18% 69/385 [00:10<00:47,  6.62it/s]\u001b[A\n",
            "Iteration:  18% 70/385 [00:10<00:47,  6.68it/s]\u001b[A\n",
            "Iteration:  18% 71/385 [00:10<00:47,  6.54it/s]\u001b[A\n",
            "Iteration:  19% 72/385 [00:10<00:46,  6.70it/s]\u001b[A\n",
            "Iteration:  19% 73/385 [00:11<00:46,  6.67it/s]\u001b[A\n",
            "Iteration:  19% 74/385 [00:11<00:46,  6.68it/s]\u001b[A\n",
            "Iteration:  19% 75/385 [00:11<00:46,  6.67it/s]\u001b[A\n",
            "Iteration:  20% 76/385 [00:11<00:46,  6.71it/s]\u001b[A\n",
            "Iteration:  20% 77/385 [00:11<00:46,  6.65it/s]\u001b[A\n",
            "Iteration:  20% 78/385 [00:11<00:45,  6.71it/s]\u001b[A\n",
            "Iteration:  21% 79/385 [00:11<00:46,  6.65it/s]\u001b[A\n",
            "Iteration:  21% 80/385 [00:12<00:45,  6.71it/s]\u001b[A\n",
            "Iteration:  21% 81/385 [00:12<00:45,  6.67it/s]\u001b[A\n",
            "Iteration:  21% 82/385 [00:12<00:44,  6.74it/s]\u001b[A\n",
            "Iteration:  22% 83/385 [00:12<00:45,  6.65it/s]\u001b[A\n",
            "Iteration:  22% 84/385 [00:12<00:44,  6.75it/s]\u001b[A\n",
            "Iteration:  22% 85/385 [00:12<00:44,  6.67it/s]\u001b[A\n",
            "Iteration:  22% 86/385 [00:12<00:44,  6.69it/s]\u001b[A\n",
            "Iteration:  23% 87/385 [00:13<00:44,  6.73it/s]\u001b[A\n",
            "Iteration:  23% 88/385 [00:13<00:43,  6.77it/s]\u001b[A\n",
            "Iteration:  23% 89/385 [00:13<00:44,  6.72it/s]\u001b[A\n",
            "Iteration:  23% 90/385 [00:13<00:43,  6.72it/s]\u001b[A\n",
            "Iteration:  24% 91/385 [00:13<00:43,  6.72it/s]\u001b[A\n",
            "Iteration:  24% 92/385 [00:13<00:43,  6.70it/s]\u001b[A\n",
            "Iteration:  24% 93/385 [00:14<00:43,  6.73it/s]\u001b[A\n",
            "Iteration:  24% 94/385 [00:14<00:42,  6.77it/s]\u001b[A\n",
            "Iteration:  25% 95/385 [00:14<00:43,  6.74it/s]\u001b[A\n",
            "Iteration:  25% 96/385 [00:14<00:42,  6.73it/s]\u001b[A\n",
            "Iteration:  25% 97/385 [00:14<00:42,  6.76it/s]\u001b[A\n",
            "Iteration:  25% 98/385 [00:14<00:42,  6.76it/s]\u001b[A\n",
            "Iteration:  26% 99/385 [00:14<00:42,  6.73it/s]\u001b[A\n",
            "Iteration:  26% 100/385 [00:15<00:42,  6.76it/s]\u001b[A\n",
            "Iteration:  26% 101/385 [00:15<00:42,  6.71it/s]\u001b[A\n",
            "Iteration:  26% 102/385 [00:15<00:42,  6.70it/s]\u001b[A\n",
            "Iteration:  27% 103/385 [00:15<00:41,  6.73it/s]\u001b[A\n",
            "Iteration:  27% 104/385 [00:15<00:41,  6.71it/s]\u001b[A\n",
            "Iteration:  27% 105/385 [00:15<00:41,  6.68it/s]\u001b[A\n",
            "Iteration:  28% 106/385 [00:15<00:41,  6.73it/s]\u001b[A\n",
            "Iteration:  28% 107/385 [00:16<00:41,  6.75it/s]\u001b[A\n",
            "Iteration:  28% 108/385 [00:16<00:41,  6.68it/s]\u001b[A\n",
            "Iteration:  28% 109/385 [00:16<00:40,  6.75it/s]\u001b[A\n",
            "Iteration:  29% 110/385 [00:16<00:40,  6.78it/s]\u001b[A\n",
            "Iteration:  29% 111/385 [00:16<00:40,  6.79it/s]\u001b[A\n",
            "Iteration:  29% 112/385 [00:16<00:40,  6.78it/s]\u001b[A\n",
            "Iteration:  29% 113/385 [00:16<00:40,  6.77it/s]\u001b[A\n",
            "Iteration:  30% 114/385 [00:17<00:40,  6.69it/s]\u001b[A\n",
            "Iteration:  30% 115/385 [00:17<00:40,  6.67it/s]\u001b[A\n",
            "Iteration:  30% 116/385 [00:17<00:40,  6.72it/s]\u001b[A\n",
            "Iteration:  30% 117/385 [00:17<00:39,  6.73it/s]\u001b[A\n",
            "Iteration:  31% 118/385 [00:17<00:39,  6.71it/s]\u001b[A\n",
            "Iteration:  31% 119/385 [00:17<00:39,  6.73it/s]\u001b[A\n",
            "Iteration:  31% 120/385 [00:18<00:39,  6.71it/s]\u001b[A\n",
            "Iteration:  31% 121/385 [00:18<00:39,  6.71it/s]\u001b[A\n",
            "Iteration:  32% 122/385 [00:18<00:39,  6.72it/s]\u001b[A\n",
            "Iteration:  32% 123/385 [00:18<00:38,  6.76it/s]\u001b[A\n",
            "Iteration:  32% 124/385 [00:18<00:38,  6.78it/s]\u001b[A\n",
            "Iteration:  32% 125/385 [00:18<00:38,  6.76it/s]\u001b[A\n",
            "Iteration:  33% 126/385 [00:18<00:38,  6.76it/s]\u001b[A\n",
            "Iteration:  33% 127/385 [00:19<00:38,  6.75it/s]\u001b[A\n",
            "Iteration:  33% 128/385 [00:19<00:38,  6.71it/s]\u001b[A\n",
            "Iteration:  34% 129/385 [00:19<00:38,  6.71it/s]\u001b[A\n",
            "Iteration:  34% 130/385 [00:19<00:38,  6.70it/s]\u001b[A\n",
            "Iteration:  34% 131/385 [00:19<00:38,  6.68it/s]\u001b[A\n",
            "Iteration:  34% 132/385 [00:19<00:37,  6.72it/s]\u001b[A\n",
            "Iteration:  35% 133/385 [00:19<00:37,  6.74it/s]\u001b[A\n",
            "Iteration:  35% 134/385 [00:20<00:37,  6.63it/s]\u001b[A\n",
            "Iteration:  35% 135/385 [00:20<00:37,  6.68it/s]\u001b[A\n",
            "Iteration:  35% 136/385 [00:20<00:37,  6.73it/s]\u001b[A\n",
            "Iteration:  36% 137/385 [00:20<00:37,  6.70it/s]\u001b[A\n",
            "Iteration:  36% 138/385 [00:20<00:36,  6.74it/s]\u001b[A\n",
            "Iteration:  36% 139/385 [00:20<00:36,  6.72it/s]\u001b[A\n",
            "Iteration:  36% 140/385 [00:21<00:36,  6.68it/s]\u001b[A\n",
            "Iteration:  37% 141/385 [00:21<00:36,  6.73it/s]\u001b[A\n",
            "Iteration:  37% 142/385 [00:21<00:35,  6.76it/s]\u001b[A\n",
            "Iteration:  37% 143/385 [00:21<00:35,  6.77it/s]\u001b[A\n",
            "Iteration:  37% 144/385 [00:21<00:35,  6.74it/s]\u001b[A\n",
            "Iteration:  38% 145/385 [00:21<00:35,  6.77it/s]\u001b[A\n",
            "Iteration:  38% 146/385 [00:21<00:35,  6.79it/s]\u001b[A\n",
            "Iteration:  38% 147/385 [00:22<00:34,  6.80it/s]\u001b[A\n",
            "Iteration:  38% 148/385 [00:22<00:35,  6.72it/s]\u001b[A\n",
            "Iteration:  39% 149/385 [00:22<00:35,  6.74it/s]\u001b[A\n",
            "Iteration:  39% 150/385 [00:22<00:34,  6.77it/s]\u001b[A\n",
            "Iteration:  39% 151/385 [00:22<00:34,  6.73it/s]\u001b[A\n",
            "Iteration:  39% 152/385 [00:22<00:34,  6.74it/s]\u001b[A\n",
            "Iteration:  40% 153/385 [00:22<00:34,  6.70it/s]\u001b[A\n",
            "Iteration:  40% 154/385 [00:23<00:34,  6.73it/s]\u001b[A\n",
            "Iteration:  40% 155/385 [00:23<00:34,  6.70it/s]\u001b[A\n",
            "Iteration:  41% 156/385 [00:23<00:33,  6.74it/s]\u001b[A\n",
            "Iteration:  41% 157/385 [00:23<00:34,  6.70it/s]\u001b[A\n",
            "Iteration:  41% 158/385 [00:23<00:33,  6.73it/s]\u001b[A\n",
            "Iteration:  41% 159/385 [00:23<00:33,  6.72it/s]\u001b[A\n",
            "Iteration:  42% 160/385 [00:23<00:33,  6.78it/s]\u001b[A\n",
            "Iteration:  42% 161/385 [00:24<00:33,  6.65it/s]\u001b[A\n",
            "Iteration:  42% 162/385 [00:24<00:32,  6.80it/s]\u001b[A\n",
            "Iteration:  42% 163/385 [00:24<00:32,  6.82it/s]\u001b[A\n",
            "Iteration:  43% 164/385 [00:24<00:32,  6.83it/s]\u001b[A\n",
            "Iteration:  43% 165/385 [00:24<00:32,  6.77it/s]\u001b[A\n",
            "Iteration:  43% 166/385 [00:24<00:32,  6.78it/s]\u001b[A\n",
            "Iteration:  43% 167/385 [00:25<00:32,  6.80it/s]\u001b[A\n",
            "Iteration:  44% 168/385 [00:25<00:32,  6.74it/s]\u001b[A\n",
            "Iteration:  44% 169/385 [00:25<00:32,  6.73it/s]\u001b[A\n",
            "Iteration:  44% 170/385 [00:25<00:32,  6.72it/s]\u001b[A\n",
            "Iteration:  44% 171/385 [00:25<00:31,  6.78it/s]\u001b[A\n",
            "Iteration:  45% 172/385 [00:25<00:31,  6.70it/s]\u001b[A\n",
            "Iteration:  45% 173/385 [00:25<00:31,  6.78it/s]\u001b[A\n",
            "Iteration:  45% 174/385 [00:26<00:31,  6.80it/s]\u001b[A\n",
            "Iteration:  45% 175/385 [00:26<00:30,  6.82it/s]\u001b[A\n",
            "Iteration:  46% 176/385 [00:26<00:30,  6.81it/s]\u001b[A\n",
            "Iteration:  46% 177/385 [00:26<00:30,  6.76it/s]\u001b[A\n",
            "Iteration:  46% 178/385 [00:26<00:30,  6.79it/s]\u001b[A\n",
            "Iteration:  46% 179/385 [00:26<00:30,  6.74it/s]\u001b[A\n",
            "Iteration:  47% 180/385 [00:26<00:30,  6.70it/s]\u001b[A\n",
            "Iteration:  47% 181/385 [00:27<00:30,  6.75it/s]\u001b[A\n",
            "Iteration:  47% 182/385 [00:27<00:30,  6.76it/s]\u001b[A\n",
            "Iteration:  48% 183/385 [00:27<00:30,  6.68it/s]\u001b[A\n",
            "Iteration:  48% 184/385 [00:27<00:30,  6.69it/s]\u001b[A\n",
            "Iteration:  48% 185/385 [00:27<00:29,  6.67it/s]\u001b[A\n",
            "Iteration:  48% 186/385 [00:27<00:29,  6.70it/s]\u001b[A\n",
            "Iteration:  49% 187/385 [00:27<00:29,  6.74it/s]\u001b[A\n",
            "Iteration:  49% 188/385 [00:28<00:29,  6.74it/s]\u001b[A\n",
            "Iteration:  49% 189/385 [00:28<00:29,  6.70it/s]\u001b[A\n",
            "Iteration:  49% 190/385 [00:28<00:28,  6.75it/s]\u001b[A\n",
            "Iteration:  50% 191/385 [00:28<00:28,  6.76it/s]\u001b[A\n",
            "Iteration:  50% 192/385 [00:28<00:28,  6.71it/s]\u001b[A\n",
            "Iteration:  50% 193/385 [00:28<00:28,  6.73it/s]\u001b[A\n",
            "Iteration:  50% 194/385 [00:29<00:28,  6.70it/s]\u001b[A\n",
            "Iteration:  51% 195/385 [00:29<00:28,  6.65it/s]\u001b[A\n",
            "Iteration:  51% 196/385 [00:29<00:28,  6.72it/s]\u001b[A\n",
            "Iteration:  51% 197/385 [00:29<00:27,  6.76it/s]\u001b[A\n",
            "Iteration:  51% 198/385 [00:29<00:27,  6.73it/s]\u001b[A\n",
            "Iteration:  52% 199/385 [00:29<00:27,  6.72it/s]\u001b[A\n",
            "Iteration:  52% 200/385 [00:29<00:27,  6.69it/s]\u001b[A\n",
            "Iteration:  52% 201/385 [00:30<00:27,  6.69it/s]\u001b[A\n",
            "Iteration:  52% 202/385 [00:30<00:27,  6.59it/s]\u001b[A\n",
            "Iteration:  53% 203/385 [00:30<00:27,  6.66it/s]\u001b[A\n",
            "Iteration:  53% 204/385 [00:30<00:27,  6.63it/s]\u001b[A\n",
            "Iteration:  53% 205/385 [00:30<00:26,  6.71it/s]\u001b[A\n",
            "Iteration:  54% 206/385 [00:30<00:26,  6.69it/s]\u001b[A\n",
            "Iteration:  54% 207/385 [00:30<00:26,  6.70it/s]\u001b[A\n",
            "Iteration:  54% 208/385 [00:31<00:26,  6.64it/s]\u001b[A\n",
            "Iteration:  54% 209/385 [00:31<00:26,  6.66it/s]\u001b[A\n",
            "Iteration:  55% 210/385 [00:31<00:26,  6.65it/s]\u001b[A\n",
            "Iteration:  55% 211/385 [00:31<00:26,  6.65it/s]\u001b[A\n",
            "Iteration:  55% 212/385 [00:31<00:26,  6.61it/s]\u001b[A\n",
            "Iteration:  55% 213/385 [00:31<00:25,  6.64it/s]\u001b[A\n",
            "Iteration:  56% 214/385 [00:32<00:25,  6.60it/s]\u001b[A\n",
            "Iteration:  56% 215/385 [00:32<00:25,  6.63it/s]\u001b[A\n",
            "Iteration:  56% 216/385 [00:32<00:25,  6.60it/s]\u001b[A\n",
            "Iteration:  56% 217/385 [00:32<00:25,  6.66it/s]\u001b[A\n",
            "Iteration:  57% 218/385 [00:32<00:25,  6.60it/s]\u001b[A\n",
            "Iteration:  57% 219/385 [00:32<00:25,  6.64it/s]\u001b[A\n",
            "Iteration:  57% 220/385 [00:32<00:24,  6.61it/s]\u001b[A\n",
            "Iteration:  57% 221/385 [00:33<00:24,  6.66it/s]\u001b[A\n",
            "Iteration:  58% 222/385 [00:33<00:24,  6.57it/s]\u001b[A\n",
            "Iteration:  58% 223/385 [00:33<00:24,  6.66it/s]\u001b[A\n",
            "Iteration:  58% 224/385 [00:33<00:24,  6.58it/s]\u001b[A\n",
            "Iteration:  58% 225/385 [00:33<00:24,  6.64it/s]\u001b[A\n",
            "Iteration:  59% 226/385 [00:33<00:24,  6.61it/s]\u001b[A\n",
            "Iteration:  59% 227/385 [00:33<00:23,  6.69it/s]\u001b[A\n",
            "Iteration:  59% 228/385 [00:34<00:23,  6.71it/s]\u001b[A\n",
            "Iteration:  59% 229/385 [00:34<00:23,  6.71it/s]\u001b[A02/12/2021 08:40:25 - INFO - transformers.configuration_utils -   Configuration saved in neurohoroscope_model_wt_sign_0/checkpoint-1000/config.json\n",
            "02/12/2021 08:40:27 - INFO - transformers.modeling_utils -   Model weights saved in neurohoroscope_model_wt_sign_0/checkpoint-1000/pytorch_model.bin\n",
            "02/12/2021 08:40:27 - INFO - __main__ -   Saving model checkpoint to neurohoroscope_model_wt_sign_0/checkpoint-1000\n",
            "02/12/2021 08:40:34 - INFO - __main__ -   Saving optimizer and scheduler states to neurohoroscope_model_wt_sign_0/checkpoint-1000\n",
            "\n",
            "Iteration:  60% 230/385 [00:43<07:44,  3.00s/it]\u001b[A\n",
            "Iteration:  60% 231/385 [00:44<05:30,  2.15s/it]\u001b[A\n",
            "Iteration:  60% 232/385 [00:44<03:56,  1.55s/it]\u001b[A\n",
            "Iteration:  61% 233/385 [00:44<02:51,  1.13s/it]\u001b[A\n",
            "Iteration:  61% 234/385 [00:44<02:06,  1.20it/s]\u001b[A\n",
            "Iteration:  61% 235/385 [00:44<01:34,  1.59it/s]\u001b[A\n",
            "Iteration:  61% 236/385 [00:44<01:12,  2.07it/s]\u001b[A\n",
            "Iteration:  62% 237/385 [00:44<00:56,  2.62it/s]\u001b[A\n",
            "Iteration:  62% 238/385 [00:45<00:46,  3.19it/s]\u001b[A\n",
            "Iteration:  62% 239/385 [00:45<00:38,  3.78it/s]\u001b[A\n",
            "Iteration:  62% 240/385 [00:45<00:33,  4.35it/s]\u001b[A\n",
            "Iteration:  63% 241/385 [00:45<00:31,  4.63it/s]\u001b[A\n",
            "Iteration:  63% 242/385 [00:45<00:27,  5.11it/s]\u001b[A\n",
            "Iteration:  63% 243/385 [00:45<00:25,  5.54it/s]\u001b[A\n",
            "Iteration:  63% 244/385 [00:46<00:24,  5.85it/s]\u001b[A\n",
            "Iteration:  64% 245/385 [00:46<00:23,  6.07it/s]\u001b[A\n",
            "Iteration:  64% 246/385 [00:46<00:22,  6.27it/s]\u001b[A\n",
            "Iteration:  64% 247/385 [00:46<00:21,  6.43it/s]\u001b[A\n",
            "Iteration:  64% 248/385 [00:46<00:21,  6.48it/s]\u001b[A\n",
            "Iteration:  65% 249/385 [00:46<00:20,  6.54it/s]\u001b[A\n",
            "Iteration:  65% 250/385 [00:46<00:20,  6.57it/s]\u001b[A\n",
            "Iteration:  65% 251/385 [00:47<00:20,  6.63it/s]\u001b[A\n",
            "Iteration:  65% 252/385 [00:47<00:20,  6.60it/s]\u001b[A\n",
            "Iteration:  66% 253/385 [00:47<00:20,  6.44it/s]\u001b[A\n",
            "Iteration:  66% 254/385 [00:47<00:20,  6.50it/s]\u001b[A\n",
            "Iteration:  66% 255/385 [00:47<00:19,  6.52it/s]\u001b[A\n",
            "Iteration:  66% 256/385 [00:47<00:19,  6.65it/s]\u001b[A\n",
            "Iteration:  67% 257/385 [00:47<00:19,  6.71it/s]\u001b[A\n",
            "Iteration:  67% 258/385 [00:48<00:18,  6.72it/s]\u001b[A\n",
            "Iteration:  67% 259/385 [00:48<00:18,  6.74it/s]\u001b[A\n",
            "Iteration:  68% 260/385 [00:48<00:18,  6.72it/s]\u001b[A\n",
            "Iteration:  68% 261/385 [00:48<00:18,  6.78it/s]\u001b[A\n",
            "Iteration:  68% 262/385 [00:48<00:18,  6.76it/s]\u001b[A\n",
            "Iteration:  68% 263/385 [00:48<00:17,  6.79it/s]\u001b[A\n",
            "Iteration:  69% 264/385 [00:49<00:17,  6.74it/s]\u001b[A\n",
            "Iteration:  69% 265/385 [00:49<00:17,  6.79it/s]\u001b[A\n",
            "Iteration:  69% 266/385 [00:49<00:17,  6.79it/s]\u001b[A\n",
            "Iteration:  69% 267/385 [00:49<00:17,  6.80it/s]\u001b[A\n",
            "Iteration:  70% 268/385 [00:49<00:17,  6.75it/s]\u001b[A\n",
            "Iteration:  70% 269/385 [00:49<00:17,  6.75it/s]\u001b[A\n",
            "Iteration:  70% 270/385 [00:49<00:17,  6.69it/s]\u001b[A\n",
            "Iteration:  70% 271/385 [00:50<00:16,  6.72it/s]\u001b[A\n",
            "Iteration:  71% 272/385 [00:50<00:16,  6.76it/s]\u001b[A\n",
            "Iteration:  71% 273/385 [00:50<00:16,  6.75it/s]\u001b[A\n",
            "Iteration:  71% 274/385 [00:50<00:16,  6.69it/s]\u001b[A\n",
            "Iteration:  71% 275/385 [00:50<00:16,  6.65it/s]\u001b[A\n",
            "Iteration:  72% 276/385 [00:50<00:16,  6.68it/s]\u001b[A\n",
            "Iteration:  72% 277/385 [00:50<00:16,  6.72it/s]\u001b[A\n",
            "Iteration:  72% 278/385 [00:51<00:15,  6.78it/s]\u001b[A\n",
            "Iteration:  72% 279/385 [00:51<00:15,  6.80it/s]\u001b[A\n",
            "Iteration:  73% 280/385 [00:51<00:15,  6.79it/s]\u001b[A\n",
            "Iteration:  73% 281/385 [00:51<00:15,  6.76it/s]\u001b[A\n",
            "Iteration:  73% 282/385 [00:51<00:15,  6.78it/s]\u001b[A\n",
            "Iteration:  74% 283/385 [00:51<00:14,  6.81it/s]\u001b[A\n",
            "Iteration:  74% 284/385 [00:51<00:14,  6.75it/s]\u001b[A\n",
            "Iteration:  74% 285/385 [00:52<00:14,  6.75it/s]\u001b[A\n",
            "Iteration:  74% 286/385 [00:52<00:14,  6.76it/s]\u001b[A\n",
            "Iteration:  75% 287/385 [00:52<00:14,  6.78it/s]\u001b[A\n",
            "Iteration:  75% 288/385 [00:52<00:14,  6.65it/s]\u001b[A\n",
            "Iteration:  75% 289/385 [00:52<00:14,  6.72it/s]\u001b[A\n",
            "Iteration:  75% 290/385 [00:52<00:14,  6.67it/s]\u001b[A\n",
            "Iteration:  76% 291/385 [00:53<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  76% 292/385 [00:53<00:13,  6.68it/s]\u001b[A\n",
            "Iteration:  76% 293/385 [00:53<00:13,  6.81it/s]\u001b[A\n",
            "Iteration:  76% 294/385 [00:53<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  77% 295/385 [00:53<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  77% 296/385 [00:53<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  77% 297/385 [00:53<00:13,  6.77it/s]\u001b[A\n",
            "Iteration:  77% 298/385 [00:54<00:12,  6.73it/s]\u001b[A\n",
            "Iteration:  78% 299/385 [00:54<00:12,  6.79it/s]\u001b[A\n",
            "Iteration:  78% 300/385 [00:54<00:12,  6.79it/s]\u001b[A\n",
            "Iteration:  78% 301/385 [00:54<00:12,  6.73it/s]\u001b[A\n",
            "Iteration:  78% 302/385 [00:54<00:12,  6.76it/s]\u001b[A\n",
            "Iteration:  79% 303/385 [00:54<00:12,  6.75it/s]\u001b[A\n",
            "Iteration:  79% 304/385 [00:54<00:12,  6.74it/s]\u001b[A\n",
            "Iteration:  79% 305/385 [00:55<00:11,  6.73it/s]\u001b[A\n",
            "Iteration:  79% 306/385 [00:55<00:11,  6.76it/s]\u001b[A\n",
            "Iteration:  80% 307/385 [00:55<00:11,  6.71it/s]\u001b[A\n",
            "Iteration:  80% 308/385 [00:55<00:11,  6.78it/s]\u001b[A\n",
            "Iteration:  80% 309/385 [00:55<00:11,  6.70it/s]\u001b[A\n",
            "Iteration:  81% 310/385 [00:55<00:11,  6.76it/s]\u001b[A\n",
            "Iteration:  81% 311/385 [00:56<00:11,  6.63it/s]\u001b[A\n",
            "Iteration:  81% 312/385 [00:56<00:10,  6.78it/s]\u001b[A\n",
            "Iteration:  81% 313/385 [00:56<00:10,  6.79it/s]\u001b[A\n",
            "Iteration:  82% 314/385 [00:56<00:10,  6.81it/s]\u001b[A\n",
            "Iteration:  82% 315/385 [00:56<00:10,  6.77it/s]\u001b[A\n",
            "Iteration:  82% 316/385 [00:56<00:10,  6.71it/s]\u001b[A\n",
            "Iteration:  82% 317/385 [00:56<00:10,  6.74it/s]\u001b[A\n",
            "Iteration:  83% 318/385 [00:57<00:09,  6.75it/s]\u001b[A\n",
            "Iteration:  83% 319/385 [00:57<00:09,  6.74it/s]\u001b[A\n",
            "Iteration:  83% 320/385 [00:57<00:09,  6.71it/s]\u001b[A\n",
            "Iteration:  83% 321/385 [00:57<00:09,  6.72it/s]\u001b[A\n",
            "Iteration:  84% 322/385 [00:57<00:09,  6.71it/s]\u001b[A\n",
            "Iteration:  84% 323/385 [00:57<00:09,  6.75it/s]\u001b[A\n",
            "Iteration:  84% 324/385 [00:57<00:09,  6.68it/s]\u001b[A\n",
            "Iteration:  84% 325/385 [00:58<00:08,  6.71it/s]\u001b[A\n",
            "Iteration:  85% 326/385 [00:58<00:08,  6.69it/s]\u001b[A\n",
            "Iteration:  85% 327/385 [00:58<00:08,  6.69it/s]\u001b[A\n",
            "Iteration:  85% 328/385 [00:58<00:08,  6.65it/s]\u001b[A\n",
            "Iteration:  85% 329/385 [00:58<00:08,  6.65it/s]\u001b[A\n",
            "Iteration:  86% 330/385 [00:58<00:08,  6.59it/s]\u001b[A\n",
            "Iteration:  86% 331/385 [00:58<00:08,  6.64it/s]\u001b[A\n",
            "Iteration:  86% 332/385 [00:59<00:08,  6.57it/s]\u001b[A\n",
            "Iteration:  86% 333/385 [00:59<00:07,  6.63it/s]\u001b[A\n",
            "Iteration:  87% 334/385 [00:59<00:07,  6.56it/s]\u001b[A\n",
            "Iteration:  87% 335/385 [00:59<00:07,  6.64it/s]\u001b[A\n",
            "Iteration:  87% 336/385 [00:59<00:07,  6.61it/s]\u001b[A\n",
            "Iteration:  88% 337/385 [00:59<00:07,  6.64it/s]\u001b[A\n",
            "Iteration:  88% 338/385 [01:00<00:07,  6.58it/s]\u001b[A\n",
            "Iteration:  88% 339/385 [01:00<00:06,  6.65it/s]\u001b[A\n",
            "Iteration:  88% 340/385 [01:00<00:06,  6.56it/s]\u001b[A\n",
            "Iteration:  89% 341/385 [01:00<00:06,  6.63it/s]\u001b[A\n",
            "Iteration:  89% 342/385 [01:00<00:06,  6.60it/s]\u001b[A\n",
            "Iteration:  89% 343/385 [01:00<00:06,  6.53it/s]\u001b[A\n",
            "Iteration:  89% 344/385 [01:00<00:06,  6.57it/s]\u001b[A\n",
            "Iteration:  90% 345/385 [01:01<00:06,  6.61it/s]\u001b[A\n",
            "Iteration:  90% 346/385 [01:01<00:05,  6.65it/s]\u001b[A\n",
            "Iteration:  90% 347/385 [01:01<00:05,  6.70it/s]\u001b[A\n",
            "Iteration:  90% 348/385 [01:01<00:05,  6.64it/s]\u001b[A\n",
            "Iteration:  91% 349/385 [01:01<00:05,  6.66it/s]\u001b[A\n",
            "Iteration:  91% 350/385 [01:01<00:05,  6.68it/s]\u001b[A\n",
            "Iteration:  91% 351/385 [01:02<00:05,  6.70it/s]\u001b[A\n",
            "Iteration:  91% 352/385 [01:02<00:04,  6.72it/s]\u001b[A\n",
            "Iteration:  92% 353/385 [01:02<00:04,  6.76it/s]\u001b[A\n",
            "Iteration:  92% 354/385 [01:02<00:04,  6.62it/s]\u001b[A\n",
            "Iteration:  92% 355/385 [01:02<00:04,  6.72it/s]\u001b[A\n",
            "Iteration:  92% 356/385 [01:02<00:04,  6.64it/s]\u001b[A\n",
            "Iteration:  93% 357/385 [01:02<00:04,  6.74it/s]\u001b[A\n",
            "Iteration:  93% 358/385 [01:03<00:04,  6.69it/s]\u001b[A\n",
            "Iteration:  93% 359/385 [01:03<00:03,  6.75it/s]\u001b[A\n",
            "Iteration:  94% 360/385 [01:03<00:03,  6.67it/s]\u001b[A\n",
            "Iteration:  94% 361/385 [01:03<00:03,  6.75it/s]\u001b[A\n",
            "Iteration:  94% 362/385 [01:03<00:03,  6.75it/s]\u001b[A\n",
            "Iteration:  94% 363/385 [01:03<00:03,  6.77it/s]\u001b[A\n",
            "Iteration:  95% 364/385 [01:03<00:03,  6.72it/s]\u001b[A\n",
            "Iteration:  95% 365/385 [01:04<00:02,  6.77it/s]\u001b[A\n",
            "Iteration:  95% 366/385 [01:04<00:02,  6.78it/s]\u001b[A\n",
            "Iteration:  95% 367/385 [01:04<00:02,  6.73it/s]\u001b[A\n",
            "Iteration:  96% 368/385 [01:04<00:02,  6.72it/s]\u001b[A\n",
            "Iteration:  96% 369/385 [01:04<00:02,  6.75it/s]\u001b[A\n",
            "Iteration:  96% 370/385 [01:04<00:02,  6.73it/s]\u001b[A\n",
            "Iteration:  96% 371/385 [01:04<00:02,  6.71it/s]\u001b[A\n",
            "Iteration:  97% 372/385 [01:05<00:01,  6.76it/s]\u001b[A\n",
            "Iteration:  97% 373/385 [01:05<00:01,  6.70it/s]\u001b[A\n",
            "Iteration:  97% 374/385 [01:05<00:01,  6.70it/s]\u001b[A\n",
            "Iteration:  97% 375/385 [01:05<00:01,  6.63it/s]\u001b[A\n",
            "Iteration:  98% 376/385 [01:05<00:01,  6.66it/s]\u001b[A\n",
            "Iteration:  98% 377/385 [01:05<00:01,  6.64it/s]\u001b[A\n",
            "Iteration:  98% 378/385 [01:06<00:01,  6.68it/s]\u001b[A\n",
            "Iteration:  98% 379/385 [01:06<00:00,  6.58it/s]\u001b[A\n",
            "Iteration:  99% 380/385 [01:06<00:00,  6.67it/s]\u001b[A\n",
            "Iteration:  99% 381/385 [01:06<00:00,  6.62it/s]\u001b[A\n",
            "Iteration:  99% 382/385 [01:06<00:00,  6.70it/s]\u001b[A\n",
            "Iteration:  99% 383/385 [01:06<00:00,  6.59it/s]\u001b[A\n",
            "Iteration: 100% 384/385 [01:06<00:00,  6.68it/s]\u001b[A\n",
            "Iteration: 100% 385/385 [01:07<00:00,  5.74it/s]\n",
            "Epoch:  60% 3/5 [03:10<02:03, 61.93s/it]\n",
            "Iteration:   0% 0/385 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/385 [00:00<00:55,  6.87it/s]\u001b[A\n",
            "Iteration:   1% 2/385 [00:00<00:56,  6.79it/s]\u001b[A\n",
            "Iteration:   1% 3/385 [00:00<00:56,  6.77it/s]\u001b[A\n",
            "Iteration:   1% 4/385 [00:00<00:56,  6.70it/s]\u001b[A\n",
            "Iteration:   1% 5/385 [00:00<00:56,  6.72it/s]\u001b[A\n",
            "Iteration:   2% 6/385 [00:00<00:57,  6.63it/s]\u001b[A\n",
            "Iteration:   2% 7/385 [00:01<00:56,  6.72it/s]\u001b[A\n",
            "Iteration:   2% 8/385 [00:01<00:56,  6.70it/s]\u001b[A\n",
            "Iteration:   2% 9/385 [00:01<00:56,  6.70it/s]\u001b[A\n",
            "Iteration:   3% 10/385 [00:01<00:56,  6.64it/s]\u001b[A\n",
            "Iteration:   3% 11/385 [00:01<00:55,  6.74it/s]\u001b[A\n",
            "Iteration:   3% 12/385 [00:01<00:55,  6.71it/s]\u001b[A\n",
            "Iteration:   3% 13/385 [00:01<00:55,  6.76it/s]\u001b[A\n",
            "Iteration:   4% 14/385 [00:02<00:54,  6.79it/s]\u001b[A\n",
            "Iteration:   4% 15/385 [00:02<00:55,  6.72it/s]\u001b[A\n",
            "Iteration:   4% 16/385 [00:02<00:55,  6.70it/s]\u001b[A\n",
            "Iteration:   4% 17/385 [00:02<00:54,  6.71it/s]\u001b[A\n",
            "Iteration:   5% 18/385 [00:02<00:54,  6.71it/s]\u001b[A\n",
            "Iteration:   5% 19/385 [00:02<00:55,  6.62it/s]\u001b[A\n",
            "Iteration:   5% 20/385 [00:02<00:55,  6.59it/s]\u001b[A\n",
            "Iteration:   5% 21/385 [00:03<00:55,  6.62it/s]\u001b[A\n",
            "Iteration:   6% 22/385 [00:03<00:54,  6.65it/s]\u001b[A\n",
            "Iteration:   6% 23/385 [00:03<00:54,  6.59it/s]\u001b[A\n",
            "Iteration:   6% 24/385 [00:03<00:54,  6.64it/s]\u001b[A\n",
            "Iteration:   6% 25/385 [00:03<00:54,  6.55it/s]\u001b[A\n",
            "Iteration:   7% 26/385 [00:03<00:54,  6.64it/s]\u001b[A\n",
            "Iteration:   7% 27/385 [00:04<00:54,  6.62it/s]\u001b[A\n",
            "Iteration:   7% 28/385 [00:04<00:53,  6.64it/s]\u001b[A\n",
            "Iteration:   8% 29/385 [00:04<00:53,  6.65it/s]\u001b[A\n",
            "Iteration:   8% 30/385 [00:04<00:53,  6.67it/s]\u001b[A\n",
            "Iteration:   8% 31/385 [00:04<00:53,  6.64it/s]\u001b[A\n",
            "Iteration:   8% 32/385 [00:04<00:53,  6.63it/s]\u001b[A\n",
            "Iteration:   9% 33/385 [00:04<00:53,  6.63it/s]\u001b[A\n",
            "Iteration:   9% 34/385 [00:05<00:52,  6.67it/s]\u001b[A\n",
            "Iteration:   9% 35/385 [00:05<00:52,  6.61it/s]\u001b[A\n",
            "Iteration:   9% 36/385 [00:05<00:52,  6.66it/s]\u001b[A\n",
            "Iteration:  10% 37/385 [00:05<00:52,  6.63it/s]\u001b[A\n",
            "Iteration:  10% 38/385 [00:05<00:51,  6.69it/s]\u001b[A\n",
            "Iteration:  10% 39/385 [00:05<00:51,  6.67it/s]\u001b[A\n",
            "Iteration:  10% 40/385 [00:06<00:51,  6.65it/s]\u001b[A\n",
            "Iteration:  11% 41/385 [00:06<00:51,  6.62it/s]\u001b[A\n",
            "Iteration:  11% 42/385 [00:06<00:51,  6.63it/s]\u001b[A\n",
            "Iteration:  11% 43/385 [00:06<00:51,  6.61it/s]\u001b[A\n",
            "Iteration:  11% 44/385 [00:06<00:51,  6.64it/s]\u001b[A\n",
            "Iteration:  12% 45/385 [00:06<00:51,  6.61it/s]\u001b[A\n",
            "Iteration:  12% 46/385 [00:06<00:51,  6.63it/s]\u001b[A\n",
            "Iteration:  12% 47/385 [00:07<00:51,  6.61it/s]\u001b[A\n",
            "Iteration:  12% 48/385 [00:07<00:50,  6.66it/s]\u001b[A\n",
            "Iteration:  13% 49/385 [00:07<00:50,  6.61it/s]\u001b[A\n",
            "Iteration:  13% 50/385 [00:07<00:50,  6.68it/s]\u001b[A\n",
            "Iteration:  13% 51/385 [00:07<00:50,  6.62it/s]\u001b[A\n",
            "Iteration:  14% 52/385 [00:07<00:50,  6.62it/s]\u001b[A\n",
            "Iteration:  14% 53/385 [00:07<00:50,  6.61it/s]\u001b[A\n",
            "Iteration:  14% 54/385 [00:08<00:50,  6.61it/s]\u001b[A\n",
            "Iteration:  14% 55/385 [00:08<00:50,  6.59it/s]\u001b[A\n",
            "Iteration:  15% 56/385 [00:08<00:49,  6.59it/s]\u001b[A\n",
            "Iteration:  15% 57/385 [00:08<00:49,  6.57it/s]\u001b[A\n",
            "Iteration:  15% 58/385 [00:08<00:49,  6.62it/s]\u001b[A\n",
            "Iteration:  15% 59/385 [00:08<00:49,  6.60it/s]\u001b[A\n",
            "Iteration:  16% 60/385 [00:09<00:48,  6.65it/s]\u001b[A\n",
            "Iteration:  16% 61/385 [00:09<00:49,  6.57it/s]\u001b[A\n",
            "Iteration:  16% 62/385 [00:09<00:48,  6.61it/s]\u001b[A\n",
            "Iteration:  16% 63/385 [00:09<00:49,  6.55it/s]\u001b[A\n",
            "Iteration:  17% 64/385 [00:09<00:48,  6.61it/s]\u001b[A\n",
            "Iteration:  17% 65/385 [00:09<00:48,  6.56it/s]\u001b[A\n",
            "Iteration:  17% 66/385 [00:09<00:48,  6.61it/s]\u001b[A\n",
            "Iteration:  17% 67/385 [00:10<00:48,  6.57it/s]\u001b[A\n",
            "Iteration:  18% 68/385 [00:10<00:47,  6.64it/s]\u001b[A\n",
            "Iteration:  18% 69/385 [00:10<00:47,  6.62it/s]\u001b[A\n",
            "Iteration:  18% 70/385 [00:10<00:47,  6.65it/s]\u001b[A\n",
            "Iteration:  18% 71/385 [00:10<00:47,  6.62it/s]\u001b[A\n",
            "Iteration:  19% 72/385 [00:10<00:47,  6.56it/s]\u001b[A\n",
            "Iteration:  19% 73/385 [00:11<00:47,  6.55it/s]\u001b[A\n",
            "Iteration:  19% 74/385 [00:11<00:47,  6.59it/s]\u001b[A\n",
            "Iteration:  19% 75/385 [00:11<00:47,  6.56it/s]\u001b[A\n",
            "Iteration:  20% 76/385 [00:11<00:46,  6.60it/s]\u001b[A\n",
            "Iteration:  20% 77/385 [00:11<00:46,  6.57it/s]\u001b[A\n",
            "Iteration:  20% 78/385 [00:11<00:46,  6.59it/s]\u001b[A\n",
            "Iteration:  21% 79/385 [00:11<00:46,  6.59it/s]\u001b[A\n",
            "Iteration:  21% 80/385 [00:12<00:46,  6.59it/s]\u001b[A\n",
            "Iteration:  21% 81/385 [00:12<00:46,  6.60it/s]\u001b[A\n",
            "Iteration:  21% 82/385 [00:12<00:45,  6.61it/s]\u001b[A\n",
            "Iteration:  22% 83/385 [00:12<00:46,  6.53it/s]\u001b[A\n",
            "Iteration:  22% 84/385 [00:12<00:45,  6.55it/s]\u001b[A\n",
            "Iteration:  22% 85/385 [00:12<00:45,  6.55it/s]\u001b[A\n",
            "Iteration:  22% 86/385 [00:12<00:45,  6.55it/s]\u001b[A\n",
            "Iteration:  23% 87/385 [00:13<00:45,  6.60it/s]\u001b[A\n",
            "Iteration:  23% 88/385 [00:13<00:44,  6.63it/s]\u001b[A\n",
            "Iteration:  23% 89/385 [00:13<00:44,  6.62it/s]\u001b[A\n",
            "Iteration:  23% 90/385 [00:13<00:44,  6.65it/s]\u001b[A\n",
            "Iteration:  24% 91/385 [00:13<00:44,  6.65it/s]\u001b[A\n",
            "Iteration:  24% 92/385 [00:13<00:44,  6.59it/s]\u001b[A\n",
            "Iteration:  24% 93/385 [00:14<00:44,  6.59it/s]\u001b[A\n",
            "Iteration:  24% 94/385 [00:14<00:43,  6.65it/s]\u001b[A\n",
            "Iteration:  25% 95/385 [00:14<00:43,  6.63it/s]\u001b[A\n",
            "Iteration:  25% 96/385 [00:14<00:43,  6.62it/s]\u001b[A\n",
            "Iteration:  25% 97/385 [00:14<00:43,  6.67it/s]\u001b[A\n",
            "Iteration:  25% 98/385 [00:14<00:42,  6.69it/s]\u001b[A\n",
            "Iteration:  26% 99/385 [00:14<00:42,  6.66it/s]\u001b[A\n",
            "Iteration:  26% 100/385 [00:15<00:42,  6.70it/s]\u001b[A\n",
            "Iteration:  26% 101/385 [00:15<00:42,  6.67it/s]\u001b[A\n",
            "Iteration:  26% 102/385 [00:15<00:42,  6.67it/s]\u001b[A\n",
            "Iteration:  27% 103/385 [00:15<00:42,  6.64it/s]\u001b[A\n",
            "Iteration:  27% 104/385 [00:15<00:42,  6.59it/s]\u001b[A\n",
            "Iteration:  27% 105/385 [00:15<00:42,  6.63it/s]\u001b[A\n",
            "Iteration:  28% 106/385 [00:15<00:42,  6.59it/s]\u001b[A\n",
            "Iteration:  28% 107/385 [00:16<00:41,  6.63it/s]\u001b[A\n",
            "Iteration:  28% 108/385 [00:16<00:41,  6.65it/s]\u001b[A\n",
            "Iteration:  28% 109/385 [00:16<00:41,  6.57it/s]\u001b[A\n",
            "Iteration:  29% 110/385 [00:16<00:42,  6.54it/s]\u001b[A\n",
            "Iteration:  29% 111/385 [00:16<00:41,  6.61it/s]\u001b[A\n",
            "Iteration:  29% 112/385 [00:16<00:41,  6.58it/s]\u001b[A\n",
            "Iteration:  29% 113/385 [00:17<00:41,  6.63it/s]\u001b[A\n",
            "Iteration:  30% 114/385 [00:17<00:40,  6.64it/s]\u001b[A\n",
            "Iteration:  30% 115/385 [00:17<00:40,  6.61it/s]\u001b[A\n",
            "Iteration:  30% 116/385 [00:17<00:40,  6.66it/s]\u001b[A\n",
            "Iteration:  30% 117/385 [00:17<00:40,  6.68it/s]\u001b[A\n",
            "Iteration:  31% 118/385 [00:17<00:40,  6.65it/s]\u001b[A\n",
            "Iteration:  31% 119/385 [00:17<00:40,  6.65it/s]\u001b[A\n",
            "Iteration:  31% 120/385 [00:18<00:39,  6.66it/s]\u001b[A\n",
            "Iteration:  31% 121/385 [00:18<00:39,  6.61it/s]\u001b[A\n",
            "Iteration:  32% 122/385 [00:18<00:40,  6.55it/s]\u001b[A\n",
            "Iteration:  32% 123/385 [00:18<00:39,  6.56it/s]\u001b[A\n",
            "Iteration:  32% 124/385 [00:18<00:39,  6.58it/s]\u001b[A\n",
            "Iteration:  32% 125/385 [00:18<00:39,  6.58it/s]\u001b[A\n",
            "Iteration:  33% 126/385 [00:19<00:39,  6.61it/s]\u001b[A\n",
            "Iteration:  33% 127/385 [00:19<00:38,  6.63it/s]\u001b[A\n",
            "Iteration:  33% 128/385 [00:19<00:38,  6.67it/s]\u001b[A\n",
            "Iteration:  34% 129/385 [00:19<00:38,  6.64it/s]\u001b[A\n",
            "Iteration:  34% 130/385 [00:19<00:38,  6.69it/s]\u001b[A\n",
            "Iteration:  34% 131/385 [00:19<00:38,  6.66it/s]\u001b[A\n",
            "Iteration:  34% 132/385 [00:19<00:37,  6.66it/s]\u001b[A\n",
            "Iteration:  35% 133/385 [00:20<00:38,  6.59it/s]\u001b[A\n",
            "Iteration:  35% 134/385 [00:20<00:37,  6.62it/s]\u001b[A\n",
            "Iteration:  35% 135/385 [00:20<00:37,  6.62it/s]\u001b[A\n",
            "Iteration:  35% 136/385 [00:20<00:37,  6.66it/s]\u001b[A\n",
            "Iteration:  36% 137/385 [00:20<00:37,  6.53it/s]\u001b[A\n",
            "Iteration:  36% 138/385 [00:20<00:37,  6.61it/s]\u001b[A\n",
            "Iteration:  36% 139/385 [00:20<00:37,  6.54it/s]\u001b[A\n",
            "Iteration:  36% 140/385 [00:21<00:37,  6.60it/s]\u001b[A\n",
            "Iteration:  37% 141/385 [00:21<00:37,  6.58it/s]\u001b[A\n",
            "Iteration:  37% 142/385 [00:21<00:36,  6.66it/s]\u001b[A\n",
            "Iteration:  37% 143/385 [00:21<00:36,  6.61it/s]\u001b[A\n",
            "Iteration:  37% 144/385 [00:21<00:36,  6.66it/s]\u001b[A\n",
            "Iteration:  38% 145/385 [00:21<00:36,  6.63it/s]\u001b[A\n",
            "Iteration:  38% 146/385 [00:22<00:35,  6.67it/s]\u001b[A\n",
            "Iteration:  38% 147/385 [00:22<00:36,  6.58it/s]\u001b[A\n",
            "Iteration:  38% 148/385 [00:22<00:35,  6.65it/s]\u001b[A\n",
            "Iteration:  39% 149/385 [00:22<00:35,  6.62it/s]\u001b[A\n",
            "Iteration:  39% 150/385 [00:22<00:35,  6.70it/s]\u001b[A\n",
            "Iteration:  39% 151/385 [00:22<00:35,  6.51it/s]\u001b[A\n",
            "Iteration:  39% 152/385 [00:22<00:35,  6.57it/s]\u001b[A\n",
            "Iteration:  40% 153/385 [00:23<00:35,  6.48it/s]\u001b[A\n",
            "Iteration:  40% 154/385 [00:23<00:35,  6.55it/s]\u001b[A\n",
            "Iteration:  40% 155/385 [00:23<00:35,  6.51it/s]\u001b[A\n",
            "Iteration:  41% 156/385 [00:23<00:34,  6.60it/s]\u001b[A\n",
            "Iteration:  41% 157/385 [00:23<00:34,  6.56it/s]\u001b[A\n",
            "Iteration:  41% 158/385 [00:23<00:34,  6.60it/s]\u001b[A\n",
            "Iteration:  41% 159/385 [00:24<00:34,  6.53it/s]\u001b[A\n",
            "Iteration:  42% 160/385 [00:24<00:34,  6.59it/s]\u001b[A\n",
            "Iteration:  42% 161/385 [00:24<00:33,  6.63it/s]\u001b[A\n",
            "Iteration:  42% 162/385 [00:24<00:33,  6.65it/s]\u001b[A\n",
            "Iteration:  42% 163/385 [00:24<00:33,  6.65it/s]\u001b[A\n",
            "Iteration:  43% 164/385 [00:24<00:33,  6.70it/s]\u001b[A\n",
            "Iteration:  43% 165/385 [00:24<00:33,  6.61it/s]\u001b[A\n",
            "Iteration:  43% 166/385 [00:25<00:32,  6.68it/s]\u001b[A\n",
            "Iteration:  43% 167/385 [00:25<00:32,  6.65it/s]\u001b[A\n",
            "Iteration:  44% 168/385 [00:25<00:32,  6.62it/s]\u001b[A\n",
            "Iteration:  44% 169/385 [00:25<00:32,  6.57it/s]\u001b[A\n",
            "Iteration:  44% 170/385 [00:25<00:32,  6.60it/s]\u001b[A\n",
            "Iteration:  44% 171/385 [00:25<00:32,  6.55it/s]\u001b[A\n",
            "Iteration:  45% 172/385 [00:25<00:32,  6.61it/s]\u001b[A\n",
            "Iteration:  45% 173/385 [00:26<00:32,  6.56it/s]\u001b[A\n",
            "Iteration:  45% 174/385 [00:26<00:32,  6.56it/s]\u001b[A\n",
            "Iteration:  45% 175/385 [00:26<00:32,  6.53it/s]\u001b[A\n",
            "Iteration:  46% 176/385 [00:26<00:31,  6.58it/s]\u001b[A\n",
            "Iteration:  46% 177/385 [00:26<00:31,  6.53it/s]\u001b[A\n",
            "Iteration:  46% 178/385 [00:26<00:31,  6.59it/s]\u001b[A\n",
            "Iteration:  46% 179/385 [00:27<00:31,  6.55it/s]\u001b[A\n",
            "Iteration:  47% 180/385 [00:27<00:30,  6.63it/s]\u001b[A\n",
            "Iteration:  47% 181/385 [00:27<00:31,  6.53it/s]\u001b[A\n",
            "Iteration:  47% 182/385 [00:27<00:30,  6.60it/s]\u001b[A\n",
            "Iteration:  48% 183/385 [00:27<00:30,  6.61it/s]\u001b[A\n",
            "Iteration:  48% 184/385 [00:27<00:30,  6.60it/s]\u001b[A\n",
            "Iteration:  48% 185/385 [00:27<00:30,  6.50it/s]\u001b[A\n",
            "Iteration:  48% 186/385 [00:28<00:30,  6.55it/s]\u001b[A\n",
            "Iteration:  49% 187/385 [00:28<00:30,  6.54it/s]\u001b[A\n",
            "Iteration:  49% 188/385 [00:28<00:30,  6.55it/s]\u001b[A\n",
            "Iteration:  49% 189/385 [00:28<00:29,  6.60it/s]\u001b[A\n",
            "Iteration:  49% 190/385 [00:28<00:29,  6.65it/s]\u001b[A\n",
            "Iteration:  50% 191/385 [00:28<00:29,  6.60it/s]\u001b[A\n",
            "Iteration:  50% 192/385 [00:29<00:28,  6.67it/s]\u001b[A\n",
            "Iteration:  50% 193/385 [00:29<00:29,  6.54it/s]\u001b[A\n",
            "Iteration:  50% 194/385 [00:29<00:28,  6.63it/s]\u001b[A\n",
            "Iteration:  51% 195/385 [00:29<00:28,  6.60it/s]\u001b[A\n",
            "Iteration:  51% 196/385 [00:29<00:28,  6.67it/s]\u001b[A\n",
            "Iteration:  51% 197/385 [00:29<00:28,  6.64it/s]\u001b[A\n",
            "Iteration:  51% 198/385 [00:29<00:28,  6.68it/s]\u001b[A\n",
            "Iteration:  52% 199/385 [00:30<00:28,  6.60it/s]\u001b[A\n",
            "Iteration:  52% 200/385 [00:30<00:27,  6.69it/s]\u001b[A\n",
            "Iteration:  52% 201/385 [00:30<00:27,  6.61it/s]\u001b[A\n",
            "Iteration:  52% 202/385 [00:30<00:27,  6.64it/s]\u001b[A\n",
            "Iteration:  53% 203/385 [00:30<00:27,  6.56it/s]\u001b[A\n",
            "Iteration:  53% 204/385 [00:30<00:27,  6.63it/s]\u001b[A\n",
            "Iteration:  53% 205/385 [00:30<00:27,  6.59it/s]\u001b[A\n",
            "Iteration:  54% 206/385 [00:31<00:26,  6.65it/s]\u001b[A\n",
            "Iteration:  54% 207/385 [00:31<00:26,  6.63it/s]\u001b[A\n",
            "Iteration:  54% 208/385 [00:31<00:26,  6.67it/s]\u001b[A\n",
            "Iteration:  54% 209/385 [00:31<00:26,  6.66it/s]\u001b[A\n",
            "Iteration:  55% 210/385 [00:31<00:26,  6.72it/s]\u001b[A\n",
            "Iteration:  55% 211/385 [00:31<00:26,  6.63it/s]\u001b[A\n",
            "Iteration:  55% 212/385 [00:32<00:25,  6.76it/s]\u001b[A\n",
            "Iteration:  55% 213/385 [00:32<00:25,  6.70it/s]\u001b[A\n",
            "Iteration:  56% 214/385 [00:32<00:25,  6.69it/s]\u001b[A\n",
            "Iteration:  56% 215/385 [00:32<00:25,  6.66it/s]\u001b[A\n",
            "Iteration:  56% 216/385 [00:32<00:25,  6.70it/s]\u001b[A\n",
            "Iteration:  56% 217/385 [00:32<00:25,  6.63it/s]\u001b[A\n",
            "Iteration:  57% 218/385 [00:32<00:24,  6.73it/s]\u001b[A\n",
            "Iteration:  57% 219/385 [00:33<00:24,  6.68it/s]\u001b[A\n",
            "Iteration:  57% 220/385 [00:33<00:24,  6.71it/s]\u001b[A\n",
            "Iteration:  57% 221/385 [00:33<00:24,  6.70it/s]\u001b[A\n",
            "Iteration:  58% 222/385 [00:33<00:24,  6.75it/s]\u001b[A\n",
            "Iteration:  58% 223/385 [00:33<00:24,  6.71it/s]\u001b[A\n",
            "Iteration:  58% 224/385 [00:33<00:23,  6.76it/s]\u001b[A\n",
            "Iteration:  58% 225/385 [00:33<00:23,  6.79it/s]\u001b[A\n",
            "Iteration:  59% 226/385 [00:34<00:23,  6.77it/s]\u001b[A\n",
            "Iteration:  59% 227/385 [00:34<00:23,  6.71it/s]\u001b[A\n",
            "Iteration:  59% 228/385 [00:34<00:23,  6.74it/s]\u001b[A\n",
            "Iteration:  59% 229/385 [00:34<00:23,  6.76it/s]\u001b[A\n",
            "Iteration:  60% 230/385 [00:34<00:23,  6.71it/s]\u001b[A\n",
            "Iteration:  60% 231/385 [00:34<00:22,  6.74it/s]\u001b[A\n",
            "Iteration:  60% 232/385 [00:34<00:22,  6.76it/s]\u001b[A\n",
            "Iteration:  61% 233/385 [00:35<00:22,  6.75it/s]\u001b[A\n",
            "Iteration:  61% 234/385 [00:35<00:22,  6.78it/s]\u001b[A\n",
            "Iteration:  61% 235/385 [00:35<00:22,  6.75it/s]\u001b[A\n",
            "Iteration:  61% 236/385 [00:35<00:22,  6.77it/s]\u001b[A\n",
            "Iteration:  62% 237/385 [00:35<00:21,  6.77it/s]\u001b[A\n",
            "Iteration:  62% 238/385 [00:35<00:21,  6.73it/s]\u001b[A\n",
            "Iteration:  62% 239/385 [00:36<00:21,  6.74it/s]\u001b[A\n",
            "Iteration:  62% 240/385 [00:36<00:21,  6.74it/s]\u001b[A\n",
            "Iteration:  63% 241/385 [00:36<00:21,  6.66it/s]\u001b[A\n",
            "Iteration:  63% 242/385 [00:36<00:21,  6.70it/s]\u001b[A\n",
            "Iteration:  63% 243/385 [00:36<00:21,  6.66it/s]\u001b[A\n",
            "Iteration:  63% 244/385 [00:36<00:21,  6.69it/s]\u001b[A\n",
            "Iteration:  64% 245/385 [00:36<00:20,  6.69it/s]\u001b[A\n",
            "Iteration:  64% 246/385 [00:37<00:20,  6.70it/s]\u001b[A\n",
            "Iteration:  64% 247/385 [00:37<00:20,  6.63it/s]\u001b[A\n",
            "Iteration:  64% 248/385 [00:37<00:20,  6.70it/s]\u001b[A\n",
            "Iteration:  65% 249/385 [00:37<00:20,  6.66it/s]\u001b[A\n",
            "Iteration:  65% 250/385 [00:37<00:20,  6.68it/s]\u001b[A\n",
            "Iteration:  65% 251/385 [00:37<00:20,  6.66it/s]\u001b[A\n",
            "Iteration:  65% 252/385 [00:37<00:19,  6.66it/s]\u001b[A\n",
            "Iteration:  66% 253/385 [00:38<00:19,  6.68it/s]\u001b[A\n",
            "Iteration:  66% 254/385 [00:38<00:19,  6.66it/s]\u001b[A\n",
            "Iteration:  66% 255/385 [00:38<00:19,  6.58it/s]\u001b[A\n",
            "Iteration:  66% 256/385 [00:38<00:19,  6.66it/s]\u001b[A\n",
            "Iteration:  67% 257/385 [00:38<00:19,  6.62it/s]\u001b[A\n",
            "Iteration:  67% 258/385 [00:38<00:18,  6.71it/s]\u001b[A\n",
            "Iteration:  67% 259/385 [00:39<00:18,  6.72it/s]\u001b[A\n",
            "Iteration:  68% 260/385 [00:39<00:18,  6.79it/s]\u001b[A\n",
            "Iteration:  68% 261/385 [00:39<00:18,  6.71it/s]\u001b[A\n",
            "Iteration:  68% 262/385 [00:39<00:18,  6.77it/s]\u001b[A\n",
            "Iteration:  68% 263/385 [00:39<00:17,  6.79it/s]\u001b[A\n",
            "Iteration:  69% 264/385 [00:39<00:17,  6.78it/s]\u001b[A\n",
            "Iteration:  69% 265/385 [00:39<00:17,  6.77it/s]\u001b[A\n",
            "Iteration:  69% 266/385 [00:40<00:17,  6.75it/s]\u001b[A\n",
            "Iteration:  69% 267/385 [00:40<00:17,  6.74it/s]\u001b[A\n",
            "Iteration:  70% 268/385 [00:40<00:17,  6.81it/s]\u001b[A\n",
            "Iteration:  70% 269/385 [00:40<00:16,  6.82it/s]\u001b[A\n",
            "Iteration:  70% 270/385 [00:40<00:17,  6.76it/s]\u001b[A\n",
            "Iteration:  70% 271/385 [00:40<00:16,  6.76it/s]\u001b[A\n",
            "Iteration:  71% 272/385 [00:40<00:16,  6.67it/s]\u001b[A\n",
            "Iteration:  71% 273/385 [00:41<00:16,  6.68it/s]\u001b[A\n",
            "Iteration:  71% 274/385 [00:41<00:16,  6.66it/s]\u001b[A\n",
            "Iteration:  71% 275/385 [00:41<00:16,  6.62it/s]\u001b[A\n",
            "Iteration:  72% 276/385 [00:41<00:16,  6.59it/s]\u001b[A\n",
            "Iteration:  72% 277/385 [00:41<00:16,  6.68it/s]\u001b[A\n",
            "Iteration:  72% 278/385 [00:41<00:16,  6.62it/s]\u001b[A\n",
            "Iteration:  72% 279/385 [00:42<00:15,  6.69it/s]\u001b[A\n",
            "Iteration:  73% 280/385 [00:42<00:15,  6.68it/s]\u001b[A\n",
            "Iteration:  73% 281/385 [00:42<00:15,  6.69it/s]\u001b[A\n",
            "Iteration:  73% 282/385 [00:42<00:15,  6.67it/s]\u001b[A\n",
            "Iteration:  74% 283/385 [00:42<00:15,  6.73it/s]\u001b[A\n",
            "Iteration:  74% 284/385 [00:42<00:15,  6.64it/s]\u001b[A\n",
            "Iteration:  74% 285/385 [00:42<00:14,  6.70it/s]\u001b[A\n",
            "Iteration:  74% 286/385 [00:43<00:14,  6.69it/s]\u001b[A\n",
            "Iteration:  75% 287/385 [00:43<00:14,  6.73it/s]\u001b[A\n",
            "Iteration:  75% 288/385 [00:43<00:14,  6.68it/s]\u001b[A\n",
            "Iteration:  75% 289/385 [00:43<00:14,  6.77it/s]\u001b[A\n",
            "Iteration:  75% 290/385 [00:43<00:14,  6.74it/s]\u001b[A\n",
            "Iteration:  76% 291/385 [00:43<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  76% 292/385 [00:43<00:13,  6.78it/s]\u001b[A\n",
            "Iteration:  76% 293/385 [00:44<00:13,  6.78it/s]\u001b[A\n",
            "Iteration:  76% 294/385 [00:44<00:13,  6.79it/s]\u001b[A\n",
            "Iteration:  77% 295/385 [00:44<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  77% 296/385 [00:44<00:13,  6.79it/s]\u001b[A\n",
            "Iteration:  77% 297/385 [00:44<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  77% 298/385 [00:44<00:12,  6.73it/s]\u001b[A\n",
            "Iteration:  78% 299/385 [00:44<00:12,  6.71it/s]\u001b[A\n",
            "Iteration:  78% 300/385 [00:45<00:12,  6.79it/s]\u001b[A\n",
            "Iteration:  78% 301/385 [00:45<00:12,  6.67it/s]\u001b[A\n",
            "Iteration:  78% 302/385 [00:45<00:12,  6.75it/s]\u001b[A\n",
            "Iteration:  79% 303/385 [00:45<00:12,  6.73it/s]\u001b[A\n",
            "Iteration:  79% 304/385 [00:45<00:11,  6.80it/s]\u001b[A\n",
            "Iteration:  79% 305/385 [00:45<00:11,  6.79it/s]\u001b[A\n",
            "Iteration:  79% 306/385 [00:46<00:11,  6.76it/s]\u001b[A\n",
            "Iteration:  80% 307/385 [00:46<00:11,  6.78it/s]\u001b[A\n",
            "Iteration:  80% 308/385 [00:46<00:11,  6.78it/s]\u001b[A\n",
            "Iteration:  80% 309/385 [00:46<00:11,  6.81it/s]\u001b[A\n",
            "Iteration:  81% 310/385 [00:46<00:11,  6.72it/s]\u001b[A\n",
            "Iteration:  81% 311/385 [00:46<00:10,  6.73it/s]\u001b[A\n",
            "Iteration:  81% 312/385 [00:46<00:10,  6.77it/s]\u001b[A\n",
            "Iteration:  81% 313/385 [00:47<00:10,  6.78it/s]\u001b[A\n",
            "Iteration:  82% 314/385 [00:47<00:10,  6.74it/s]\u001b[A\n",
            "Iteration:  82% 315/385 [00:47<00:10,  6.78it/s]\u001b[A\n",
            "Iteration:  82% 316/385 [00:47<00:10,  6.81it/s]\u001b[A\n",
            "Iteration:  82% 317/385 [00:47<00:10,  6.79it/s]\u001b[A\n",
            "Iteration:  83% 318/385 [00:47<00:09,  6.73it/s]\u001b[A\n",
            "Iteration:  83% 319/385 [00:47<00:09,  6.75it/s]\u001b[A\n",
            "Iteration:  83% 320/385 [00:48<00:09,  6.78it/s]\u001b[A\n",
            "Iteration:  83% 321/385 [00:48<00:09,  6.78it/s]\u001b[A\n",
            "Iteration:  84% 322/385 [00:48<00:09,  6.72it/s]\u001b[A\n",
            "Iteration:  84% 323/385 [00:48<00:09,  6.73it/s]\u001b[A\n",
            "Iteration:  84% 324/385 [00:48<00:09,  6.73it/s]\u001b[A\n",
            "Iteration:  84% 325/385 [00:48<00:08,  6.71it/s]\u001b[A\n",
            "Iteration:  85% 326/385 [00:48<00:08,  6.73it/s]\u001b[A\n",
            "Iteration:  85% 327/385 [00:49<00:08,  6.70it/s]\u001b[A\n",
            "Iteration:  85% 328/385 [00:49<00:08,  6.70it/s]\u001b[A\n",
            "Iteration:  85% 329/385 [00:49<00:08,  6.67it/s]\u001b[A\n",
            "Iteration:  86% 330/385 [00:49<00:08,  6.72it/s]\u001b[A\n",
            "Iteration:  86% 331/385 [00:49<00:08,  6.67it/s]\u001b[A\n",
            "Iteration:  86% 332/385 [00:49<00:07,  6.73it/s]\u001b[A\n",
            "Iteration:  86% 333/385 [00:50<00:07,  6.71it/s]\u001b[A\n",
            "Iteration:  87% 334/385 [00:50<00:07,  6.73it/s]\u001b[A\n",
            "Iteration:  87% 335/385 [00:50<00:07,  6.66it/s]\u001b[A\n",
            "Iteration:  87% 336/385 [00:50<00:07,  6.77it/s]\u001b[A\n",
            "Iteration:  88% 337/385 [00:50<00:07,  6.77it/s]\u001b[A\n",
            "Iteration:  88% 338/385 [00:50<00:06,  6.77it/s]\u001b[A\n",
            "Iteration:  88% 339/385 [00:50<00:06,  6.77it/s]\u001b[A\n",
            "Iteration:  88% 340/385 [00:51<00:06,  6.80it/s]\u001b[A\n",
            "Iteration:  89% 341/385 [00:51<00:06,  6.81it/s]\u001b[A\n",
            "Iteration:  89% 342/385 [00:51<00:06,  6.74it/s]\u001b[A\n",
            "Iteration:  89% 343/385 [00:51<00:06,  6.77it/s]\u001b[A\n",
            "Iteration:  89% 344/385 [00:51<00:06,  6.76it/s]\u001b[A02/12/2021 08:41:49 - INFO - transformers.configuration_utils -   Configuration saved in neurohoroscope_model_wt_sign_0/checkpoint-1500/config.json\n",
            "02/12/2021 08:41:52 - INFO - transformers.modeling_utils -   Model weights saved in neurohoroscope_model_wt_sign_0/checkpoint-1500/pytorch_model.bin\n",
            "02/12/2021 08:41:52 - INFO - __main__ -   Saving model checkpoint to neurohoroscope_model_wt_sign_0/checkpoint-1500\n",
            "02/12/2021 08:41:59 - INFO - __main__ -   Saving optimizer and scheduler states to neurohoroscope_model_wt_sign_0/checkpoint-1500\n",
            "\n",
            "Iteration:  90% 345/385 [01:01<01:57,  2.93s/it]\u001b[A\n",
            "Iteration:  90% 346/385 [01:01<01:21,  2.10s/it]\u001b[A\n",
            "Iteration:  90% 347/385 [01:01<00:57,  1.52s/it]\u001b[A\n",
            "Iteration:  90% 348/385 [01:01<00:41,  1.11s/it]\u001b[A\n",
            "Iteration:  91% 349/385 [01:01<00:29,  1.22it/s]\u001b[A\n",
            "Iteration:  91% 350/385 [01:01<00:21,  1.62it/s]\u001b[A\n",
            "Iteration:  91% 351/385 [01:01<00:16,  2.09it/s]\u001b[A\n",
            "Iteration:  91% 352/385 [01:02<00:12,  2.65it/s]\u001b[A\n",
            "Iteration:  92% 353/385 [01:02<00:09,  3.23it/s]\u001b[A\n",
            "Iteration:  92% 354/385 [01:02<00:08,  3.84it/s]\u001b[A\n",
            "Iteration:  92% 355/385 [01:02<00:06,  4.37it/s]\u001b[A\n",
            "Iteration:  92% 356/385 [01:02<00:05,  4.88it/s]\u001b[A\n",
            "Iteration:  93% 357/385 [01:02<00:05,  5.31it/s]\u001b[A\n",
            "Iteration:  93% 358/385 [01:03<00:04,  5.71it/s]\u001b[A\n",
            "Iteration:  93% 359/385 [01:03<00:04,  5.90it/s]\u001b[A\n",
            "Iteration:  94% 360/385 [01:03<00:04,  6.19it/s]\u001b[A\n",
            "Iteration:  94% 361/385 [01:03<00:03,  6.31it/s]\u001b[A\n",
            "Iteration:  94% 362/385 [01:03<00:03,  6.46it/s]\u001b[A\n",
            "Iteration:  94% 363/385 [01:03<00:03,  6.51it/s]\u001b[A\n",
            "Iteration:  95% 364/385 [01:03<00:03,  6.60it/s]\u001b[A\n",
            "Iteration:  95% 365/385 [01:04<00:03,  6.57it/s]\u001b[A\n",
            "Iteration:  95% 366/385 [01:04<00:02,  6.70it/s]\u001b[A\n",
            "Iteration:  95% 367/385 [01:04<00:02,  6.71it/s]\u001b[A\n",
            "Iteration:  96% 368/385 [01:04<00:02,  6.79it/s]\u001b[A\n",
            "Iteration:  96% 369/385 [01:04<00:02,  6.67it/s]\u001b[A\n",
            "Iteration:  96% 370/385 [01:04<00:02,  6.76it/s]\u001b[A\n",
            "Iteration:  96% 371/385 [01:04<00:02,  6.78it/s]\u001b[A\n",
            "Iteration:  97% 372/385 [01:05<00:01,  6.72it/s]\u001b[A\n",
            "Iteration:  97% 373/385 [01:05<00:01,  6.71it/s]\u001b[A\n",
            "Iteration:  97% 374/385 [01:05<00:01,  6.73it/s]\u001b[A\n",
            "Iteration:  97% 375/385 [01:05<00:01,  6.71it/s]\u001b[A\n",
            "Iteration:  98% 376/385 [01:05<00:01,  6.58it/s]\u001b[A\n",
            "Iteration:  98% 377/385 [01:05<00:01,  6.65it/s]\u001b[A\n",
            "Iteration:  98% 378/385 [01:06<00:01,  6.68it/s]\u001b[A\n",
            "Iteration:  98% 379/385 [01:06<00:00,  6.71it/s]\u001b[A\n",
            "Iteration:  99% 380/385 [01:06<00:00,  6.72it/s]\u001b[A\n",
            "Iteration:  99% 381/385 [01:06<00:00,  6.70it/s]\u001b[A\n",
            "Iteration:  99% 382/385 [01:06<00:00,  6.67it/s]\u001b[A\n",
            "Iteration:  99% 383/385 [01:06<00:00,  6.65it/s]\u001b[A\n",
            "Iteration: 100% 384/385 [01:06<00:00,  6.72it/s]\u001b[A\n",
            "Iteration: 100% 385/385 [01:07<00:00,  5.74it/s]\n",
            "Epoch:  80% 4/5 [04:17<01:03, 63.47s/it]\n",
            "Iteration:   0% 0/385 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/385 [00:00<00:55,  6.89it/s]\u001b[A\n",
            "Iteration:   1% 2/385 [00:00<00:55,  6.88it/s]\u001b[A\n",
            "Iteration:   1% 3/385 [00:00<00:56,  6.77it/s]\u001b[A\n",
            "Iteration:   1% 4/385 [00:00<00:55,  6.81it/s]\u001b[A\n",
            "Iteration:   1% 5/385 [00:00<00:56,  6.72it/s]\u001b[A\n",
            "Iteration:   2% 6/385 [00:00<00:56,  6.71it/s]\u001b[A\n",
            "Iteration:   2% 7/385 [00:01<00:56,  6.68it/s]\u001b[A\n",
            "Iteration:   2% 8/385 [00:01<00:56,  6.67it/s]\u001b[A\n",
            "Iteration:   2% 9/385 [00:01<00:56,  6.61it/s]\u001b[A\n",
            "Iteration:   3% 10/385 [00:01<00:56,  6.61it/s]\u001b[A\n",
            "Iteration:   3% 11/385 [00:01<00:56,  6.61it/s]\u001b[A\n",
            "Iteration:   3% 12/385 [00:01<00:56,  6.61it/s]\u001b[A\n",
            "Iteration:   3% 13/385 [00:01<00:56,  6.62it/s]\u001b[A\n",
            "Iteration:   4% 14/385 [00:02<00:55,  6.69it/s]\u001b[A\n",
            "Iteration:   4% 15/385 [00:02<00:56,  6.61it/s]\u001b[A\n",
            "Iteration:   4% 16/385 [00:02<00:54,  6.72it/s]\u001b[A\n",
            "Iteration:   4% 17/385 [00:02<00:54,  6.73it/s]\u001b[A\n",
            "Iteration:   5% 18/385 [00:02<00:54,  6.68it/s]\u001b[A\n",
            "Iteration:   5% 19/385 [00:02<00:54,  6.67it/s]\u001b[A\n",
            "Iteration:   5% 20/385 [00:02<00:54,  6.75it/s]\u001b[A\n",
            "Iteration:   5% 21/385 [00:03<00:54,  6.63it/s]\u001b[A\n",
            "Iteration:   6% 22/385 [00:03<00:53,  6.76it/s]\u001b[A\n",
            "Iteration:   6% 23/385 [00:03<00:55,  6.51it/s]\u001b[A\n",
            "Iteration:   6% 24/385 [00:03<00:54,  6.61it/s]\u001b[A\n",
            "Iteration:   6% 25/385 [00:03<00:54,  6.65it/s]\u001b[A\n",
            "Iteration:   7% 26/385 [00:03<00:53,  6.68it/s]\u001b[A\n",
            "Iteration:   7% 27/385 [00:04<00:53,  6.74it/s]\u001b[A\n",
            "Iteration:   7% 28/385 [00:04<00:53,  6.71it/s]\u001b[A\n",
            "Iteration:   8% 29/385 [00:04<00:53,  6.71it/s]\u001b[A\n",
            "Iteration:   8% 30/385 [00:04<00:53,  6.69it/s]\u001b[A\n",
            "Iteration:   8% 31/385 [00:04<00:52,  6.78it/s]\u001b[A\n",
            "Iteration:   8% 32/385 [00:04<00:52,  6.74it/s]\u001b[A\n",
            "Iteration:   9% 33/385 [00:04<00:52,  6.76it/s]\u001b[A\n",
            "Iteration:   9% 34/385 [00:05<00:51,  6.78it/s]\u001b[A\n",
            "Iteration:   9% 35/385 [00:05<00:51,  6.78it/s]\u001b[A\n",
            "Iteration:   9% 36/385 [00:05<00:52,  6.71it/s]\u001b[A\n",
            "Iteration:  10% 37/385 [00:05<00:51,  6.73it/s]\u001b[A\n",
            "Iteration:  10% 38/385 [00:05<00:51,  6.76it/s]\u001b[A\n",
            "Iteration:  10% 39/385 [00:05<00:51,  6.69it/s]\u001b[A\n",
            "Iteration:  10% 40/385 [00:05<00:51,  6.75it/s]\u001b[A\n",
            "Iteration:  11% 41/385 [00:06<00:50,  6.75it/s]\u001b[A\n",
            "Iteration:  11% 42/385 [00:06<00:50,  6.73it/s]\u001b[A\n",
            "Iteration:  11% 43/385 [00:06<00:50,  6.73it/s]\u001b[A\n",
            "Iteration:  11% 44/385 [00:06<00:50,  6.78it/s]\u001b[A\n",
            "Iteration:  12% 45/385 [00:06<00:50,  6.73it/s]\u001b[A\n",
            "Iteration:  12% 46/385 [00:06<00:50,  6.71it/s]\u001b[A\n",
            "Iteration:  12% 47/385 [00:07<00:50,  6.75it/s]\u001b[A\n",
            "Iteration:  12% 48/385 [00:07<00:49,  6.77it/s]\u001b[A\n",
            "Iteration:  13% 49/385 [00:07<00:50,  6.72it/s]\u001b[A\n",
            "Iteration:  13% 50/385 [00:07<00:49,  6.74it/s]\u001b[A\n",
            "Iteration:  13% 51/385 [00:07<00:49,  6.75it/s]\u001b[A\n",
            "Iteration:  14% 52/385 [00:07<00:50,  6.65it/s]\u001b[A\n",
            "Iteration:  14% 53/385 [00:07<00:49,  6.70it/s]\u001b[A\n",
            "Iteration:  14% 54/385 [00:08<00:49,  6.72it/s]\u001b[A\n",
            "Iteration:  14% 55/385 [00:08<00:49,  6.68it/s]\u001b[A\n",
            "Iteration:  15% 56/385 [00:08<00:48,  6.72it/s]\u001b[A\n",
            "Iteration:  15% 57/385 [00:08<00:48,  6.76it/s]\u001b[A\n",
            "Iteration:  15% 58/385 [00:08<00:49,  6.66it/s]\u001b[A\n",
            "Iteration:  15% 59/385 [00:08<00:48,  6.75it/s]\u001b[A\n",
            "Iteration:  16% 60/385 [00:08<00:48,  6.74it/s]\u001b[A\n",
            "Iteration:  16% 61/385 [00:09<00:48,  6.69it/s]\u001b[A\n",
            "Iteration:  16% 62/385 [00:09<00:48,  6.69it/s]\u001b[A\n",
            "Iteration:  16% 63/385 [00:09<00:47,  6.74it/s]\u001b[A\n",
            "Iteration:  17% 64/385 [00:09<00:48,  6.62it/s]\u001b[A\n",
            "Iteration:  17% 65/385 [00:09<00:47,  6.69it/s]\u001b[A\n",
            "Iteration:  17% 66/385 [00:09<00:48,  6.62it/s]\u001b[A\n",
            "Iteration:  17% 67/385 [00:09<00:47,  6.70it/s]\u001b[A\n",
            "Iteration:  18% 68/385 [00:10<00:47,  6.64it/s]\u001b[A\n",
            "Iteration:  18% 69/385 [00:10<00:47,  6.67it/s]\u001b[A\n",
            "Iteration:  18% 70/385 [00:10<00:47,  6.67it/s]\u001b[A\n",
            "Iteration:  18% 71/385 [00:10<00:46,  6.70it/s]\u001b[A\n",
            "Iteration:  19% 72/385 [00:10<00:47,  6.62it/s]\u001b[A\n",
            "Iteration:  19% 73/385 [00:10<00:46,  6.68it/s]\u001b[A\n",
            "Iteration:  19% 74/385 [00:11<00:46,  6.62it/s]\u001b[A\n",
            "Iteration:  19% 75/385 [00:11<00:46,  6.72it/s]\u001b[A\n",
            "Iteration:  20% 76/385 [00:11<00:46,  6.62it/s]\u001b[A\n",
            "Iteration:  20% 77/385 [00:11<00:46,  6.63it/s]\u001b[A\n",
            "Iteration:  20% 78/385 [00:11<00:46,  6.56it/s]\u001b[A\n",
            "Iteration:  21% 79/385 [00:11<00:46,  6.60it/s]\u001b[A\n",
            "Iteration:  21% 80/385 [00:11<00:46,  6.61it/s]\u001b[A\n",
            "Iteration:  21% 81/385 [00:12<00:45,  6.64it/s]\u001b[A\n",
            "Iteration:  21% 82/385 [00:12<00:45,  6.63it/s]\u001b[A\n",
            "Iteration:  22% 83/385 [00:12<00:45,  6.70it/s]\u001b[A\n",
            "Iteration:  22% 84/385 [00:12<00:45,  6.64it/s]\u001b[A\n",
            "Iteration:  22% 85/385 [00:12<00:44,  6.68it/s]\u001b[A\n",
            "Iteration:  22% 86/385 [00:12<00:44,  6.72it/s]\u001b[A\n",
            "Iteration:  23% 87/385 [00:12<00:43,  6.78it/s]\u001b[A\n",
            "Iteration:  23% 88/385 [00:13<00:44,  6.65it/s]\u001b[A\n",
            "Iteration:  23% 89/385 [00:13<00:43,  6.77it/s]\u001b[A\n",
            "Iteration:  23% 90/385 [00:13<00:43,  6.78it/s]\u001b[A\n",
            "Iteration:  24% 91/385 [00:13<00:43,  6.71it/s]\u001b[A\n",
            "Iteration:  24% 92/385 [00:13<00:43,  6.67it/s]\u001b[A\n",
            "Iteration:  24% 93/385 [00:13<00:43,  6.73it/s]\u001b[A\n",
            "Iteration:  24% 94/385 [00:14<00:43,  6.70it/s]\u001b[A\n",
            "Iteration:  25% 95/385 [00:14<00:43,  6.71it/s]\u001b[A\n",
            "Iteration:  25% 96/385 [00:14<00:42,  6.72it/s]\u001b[A\n",
            "Iteration:  25% 97/385 [00:14<00:43,  6.70it/s]\u001b[A\n",
            "Iteration:  25% 98/385 [00:14<00:42,  6.74it/s]\u001b[A\n",
            "Iteration:  26% 99/385 [00:14<00:42,  6.66it/s]\u001b[A\n",
            "Iteration:  26% 100/385 [00:14<00:42,  6.67it/s]\u001b[A\n",
            "Iteration:  26% 101/385 [00:15<00:42,  6.69it/s]\u001b[A\n",
            "Iteration:  26% 102/385 [00:15<00:42,  6.66it/s]\u001b[A\n",
            "Iteration:  27% 103/385 [00:15<00:42,  6.63it/s]\u001b[A\n",
            "Iteration:  27% 104/385 [00:15<00:42,  6.67it/s]\u001b[A\n",
            "Iteration:  27% 105/385 [00:15<00:42,  6.66it/s]\u001b[A\n",
            "Iteration:  28% 106/385 [00:15<00:41,  6.65it/s]\u001b[A\n",
            "Iteration:  28% 107/385 [00:15<00:41,  6.64it/s]\u001b[A\n",
            "Iteration:  28% 108/385 [00:16<00:41,  6.69it/s]\u001b[A\n",
            "Iteration:  28% 109/385 [00:16<00:41,  6.67it/s]\u001b[A\n",
            "Iteration:  29% 110/385 [00:16<00:41,  6.68it/s]\u001b[A\n",
            "Iteration:  29% 111/385 [00:16<00:41,  6.63it/s]\u001b[A\n",
            "Iteration:  29% 112/385 [00:16<00:41,  6.58it/s]\u001b[A\n",
            "Iteration:  29% 113/385 [00:16<00:41,  6.62it/s]\u001b[A\n",
            "Iteration:  30% 114/385 [00:17<00:40,  6.62it/s]\u001b[A\n",
            "Iteration:  30% 115/385 [00:17<00:40,  6.65it/s]\u001b[A\n",
            "Iteration:  30% 116/385 [00:17<00:40,  6.70it/s]\u001b[A\n",
            "Iteration:  30% 117/385 [00:17<00:40,  6.60it/s]\u001b[A\n",
            "Iteration:  31% 118/385 [00:17<00:39,  6.69it/s]\u001b[A\n",
            "Iteration:  31% 119/385 [00:17<00:40,  6.61it/s]\u001b[A\n",
            "Iteration:  31% 120/385 [00:17<00:39,  6.64it/s]\u001b[A\n",
            "Iteration:  31% 121/385 [00:18<00:39,  6.61it/s]\u001b[A\n",
            "Iteration:  32% 122/385 [00:18<00:39,  6.62it/s]\u001b[A\n",
            "Iteration:  32% 123/385 [00:18<00:39,  6.61it/s]\u001b[A\n",
            "Iteration:  32% 124/385 [00:18<00:39,  6.59it/s]\u001b[A\n",
            "Iteration:  32% 125/385 [00:18<00:39,  6.60it/s]\u001b[A\n",
            "Iteration:  33% 126/385 [00:18<00:39,  6.59it/s]\u001b[A\n",
            "Iteration:  33% 127/385 [00:19<00:38,  6.62it/s]\u001b[A\n",
            "Iteration:  33% 128/385 [00:19<00:38,  6.66it/s]\u001b[A\n",
            "Iteration:  34% 129/385 [00:19<00:38,  6.62it/s]\u001b[A\n",
            "Iteration:  34% 130/385 [00:19<00:38,  6.67it/s]\u001b[A\n",
            "Iteration:  34% 131/385 [00:19<00:38,  6.63it/s]\u001b[A\n",
            "Iteration:  34% 132/385 [00:19<00:38,  6.65it/s]\u001b[A\n",
            "Iteration:  35% 133/385 [00:19<00:38,  6.62it/s]\u001b[A\n",
            "Iteration:  35% 134/385 [00:20<00:37,  6.61it/s]\u001b[A\n",
            "Iteration:  35% 135/385 [00:20<00:37,  6.62it/s]\u001b[A\n",
            "Iteration:  35% 136/385 [00:20<00:37,  6.59it/s]\u001b[A\n",
            "Iteration:  36% 137/385 [00:20<00:37,  6.54it/s]\u001b[A\n",
            "Iteration:  36% 138/385 [00:20<00:37,  6.57it/s]\u001b[A\n",
            "Iteration:  36% 139/385 [00:20<00:38,  6.47it/s]\u001b[A\n",
            "Iteration:  36% 140/385 [00:20<00:37,  6.50it/s]\u001b[A\n",
            "Iteration:  37% 141/385 [00:21<00:37,  6.54it/s]\u001b[A\n",
            "Iteration:  37% 142/385 [00:21<00:36,  6.60it/s]\u001b[A\n",
            "Iteration:  37% 143/385 [00:21<00:36,  6.57it/s]\u001b[A\n",
            "Iteration:  37% 144/385 [00:21<00:36,  6.63it/s]\u001b[A\n",
            "Iteration:  38% 145/385 [00:21<00:36,  6.64it/s]\u001b[A\n",
            "Iteration:  38% 146/385 [00:21<00:35,  6.68it/s]\u001b[A\n",
            "Iteration:  38% 147/385 [00:22<00:36,  6.60it/s]\u001b[A\n",
            "Iteration:  38% 148/385 [00:22<00:35,  6.62it/s]\u001b[A\n",
            "Iteration:  39% 149/385 [00:22<00:35,  6.59it/s]\u001b[A\n",
            "Iteration:  39% 150/385 [00:22<00:35,  6.64it/s]\u001b[A\n",
            "Iteration:  39% 151/385 [00:22<00:35,  6.67it/s]\u001b[A\n",
            "Iteration:  39% 152/385 [00:22<00:35,  6.65it/s]\u001b[A\n",
            "Iteration:  40% 153/385 [00:22<00:35,  6.52it/s]\u001b[A\n",
            "Iteration:  40% 154/385 [00:23<00:34,  6.61it/s]\u001b[A\n",
            "Iteration:  40% 155/385 [00:23<00:34,  6.58it/s]\u001b[A\n",
            "Iteration:  41% 156/385 [00:23<00:34,  6.62it/s]\u001b[A\n",
            "Iteration:  41% 157/385 [00:23<00:34,  6.62it/s]\u001b[A\n",
            "Iteration:  41% 158/385 [00:23<00:34,  6.62it/s]\u001b[A\n",
            "Iteration:  41% 159/385 [00:23<00:34,  6.58it/s]\u001b[A\n",
            "Iteration:  42% 160/385 [00:24<00:33,  6.68it/s]\u001b[A\n",
            "Iteration:  42% 161/385 [00:24<00:33,  6.61it/s]\u001b[A\n",
            "Iteration:  42% 162/385 [00:24<00:33,  6.62it/s]\u001b[A\n",
            "Iteration:  42% 163/385 [00:24<00:33,  6.61it/s]\u001b[A\n",
            "Iteration:  43% 164/385 [00:24<00:33,  6.67it/s]\u001b[A\n",
            "Iteration:  43% 165/385 [00:24<00:33,  6.63it/s]\u001b[A\n",
            "Iteration:  43% 166/385 [00:24<00:33,  6.62it/s]\u001b[A\n",
            "Iteration:  43% 167/385 [00:25<00:32,  6.61it/s]\u001b[A\n",
            "Iteration:  44% 168/385 [00:25<00:32,  6.61it/s]\u001b[A\n",
            "Iteration:  44% 169/385 [00:25<00:32,  6.65it/s]\u001b[A\n",
            "Iteration:  44% 170/385 [00:25<00:32,  6.66it/s]\u001b[A\n",
            "Iteration:  44% 171/385 [00:25<00:32,  6.60it/s]\u001b[A\n",
            "Iteration:  45% 172/385 [00:25<00:32,  6.65it/s]\u001b[A\n",
            "Iteration:  45% 173/385 [00:25<00:32,  6.59it/s]\u001b[A\n",
            "Iteration:  45% 174/385 [00:26<00:31,  6.66it/s]\u001b[A\n",
            "Iteration:  45% 175/385 [00:26<00:31,  6.66it/s]\u001b[A\n",
            "Iteration:  46% 176/385 [00:26<00:31,  6.68it/s]\u001b[A\n",
            "Iteration:  46% 177/385 [00:26<00:31,  6.66it/s]\u001b[A\n",
            "Iteration:  46% 178/385 [00:26<00:31,  6.66it/s]\u001b[A\n",
            "Iteration:  46% 179/385 [00:26<00:31,  6.62it/s]\u001b[A\n",
            "Iteration:  47% 180/385 [00:27<00:30,  6.65it/s]\u001b[A\n",
            "Iteration:  47% 181/385 [00:27<00:30,  6.63it/s]\u001b[A\n",
            "Iteration:  47% 182/385 [00:27<00:30,  6.69it/s]\u001b[A\n",
            "Iteration:  48% 183/385 [00:27<00:30,  6.65it/s]\u001b[A\n",
            "Iteration:  48% 184/385 [00:27<00:29,  6.71it/s]\u001b[A\n",
            "Iteration:  48% 185/385 [00:27<00:29,  6.69it/s]\u001b[A\n",
            "Iteration:  48% 186/385 [00:27<00:30,  6.62it/s]\u001b[A\n",
            "Iteration:  49% 187/385 [00:28<00:30,  6.60it/s]\u001b[A\n",
            "Iteration:  49% 188/385 [00:28<00:29,  6.62it/s]\u001b[A\n",
            "Iteration:  49% 189/385 [00:28<00:29,  6.63it/s]\u001b[A\n",
            "Iteration:  49% 190/385 [00:28<00:29,  6.65it/s]\u001b[A\n",
            "Iteration:  50% 191/385 [00:28<00:29,  6.60it/s]\u001b[A\n",
            "Iteration:  50% 192/385 [00:28<00:29,  6.65it/s]\u001b[A\n",
            "Iteration:  50% 193/385 [00:28<00:29,  6.61it/s]\u001b[A\n",
            "Iteration:  50% 194/385 [00:29<00:28,  6.65it/s]\u001b[A\n",
            "Iteration:  51% 195/385 [00:29<00:28,  6.66it/s]\u001b[A\n",
            "Iteration:  51% 196/385 [00:29<00:28,  6.67it/s]\u001b[A\n",
            "Iteration:  51% 197/385 [00:29<00:28,  6.65it/s]\u001b[A\n",
            "Iteration:  51% 198/385 [00:29<00:28,  6.64it/s]\u001b[A\n",
            "Iteration:  52% 199/385 [00:29<00:28,  6.61it/s]\u001b[A\n",
            "Iteration:  52% 200/385 [00:30<00:28,  6.58it/s]\u001b[A\n",
            "Iteration:  52% 201/385 [00:30<00:28,  6.56it/s]\u001b[A\n",
            "Iteration:  52% 202/385 [00:30<00:27,  6.62it/s]\u001b[A\n",
            "Iteration:  53% 203/385 [00:30<00:27,  6.61it/s]\u001b[A\n",
            "Iteration:  53% 204/385 [00:30<00:27,  6.67it/s]\u001b[A\n",
            "Iteration:  53% 205/385 [00:30<00:27,  6.63it/s]\u001b[A\n",
            "Iteration:  54% 206/385 [00:30<00:26,  6.65it/s]\u001b[A\n",
            "Iteration:  54% 207/385 [00:31<00:26,  6.69it/s]\u001b[A\n",
            "Iteration:  54% 208/385 [00:31<00:26,  6.65it/s]\u001b[A\n",
            "Iteration:  54% 209/385 [00:31<00:26,  6.59it/s]\u001b[A\n",
            "Iteration:  55% 210/385 [00:31<00:26,  6.60it/s]\u001b[A\n",
            "Iteration:  55% 211/385 [00:31<00:26,  6.62it/s]\u001b[A\n",
            "Iteration:  55% 212/385 [00:31<00:26,  6.60it/s]\u001b[A\n",
            "Iteration:  55% 213/385 [00:32<00:26,  6.54it/s]\u001b[A\n",
            "Iteration:  56% 214/385 [00:32<00:25,  6.61it/s]\u001b[A\n",
            "Iteration:  56% 215/385 [00:32<00:25,  6.67it/s]\u001b[A\n",
            "Iteration:  56% 216/385 [00:32<00:25,  6.72it/s]\u001b[A\n",
            "Iteration:  56% 217/385 [00:32<00:24,  6.73it/s]\u001b[A\n",
            "Iteration:  57% 218/385 [00:32<00:24,  6.71it/s]\u001b[A\n",
            "Iteration:  57% 219/385 [00:32<00:24,  6.75it/s]\u001b[A\n",
            "Iteration:  57% 220/385 [00:33<00:24,  6.77it/s]\u001b[A\n",
            "Iteration:  57% 221/385 [00:33<00:24,  6.75it/s]\u001b[A\n",
            "Iteration:  58% 222/385 [00:33<00:24,  6.71it/s]\u001b[A\n",
            "Iteration:  58% 223/385 [00:33<00:24,  6.75it/s]\u001b[A\n",
            "Iteration:  58% 224/385 [00:33<00:23,  6.77it/s]\u001b[A\n",
            "Iteration:  58% 225/385 [00:33<00:23,  6.76it/s]\u001b[A\n",
            "Iteration:  59% 226/385 [00:33<00:23,  6.75it/s]\u001b[A\n",
            "Iteration:  59% 227/385 [00:34<00:23,  6.70it/s]\u001b[A\n",
            "Iteration:  59% 228/385 [00:34<00:23,  6.72it/s]\u001b[A\n",
            "Iteration:  59% 229/385 [00:34<00:23,  6.72it/s]\u001b[A\n",
            "Iteration:  60% 230/385 [00:34<00:22,  6.74it/s]\u001b[A\n",
            "Iteration:  60% 231/385 [00:34<00:22,  6.75it/s]\u001b[A\n",
            "Iteration:  60% 232/385 [00:34<00:22,  6.73it/s]\u001b[A\n",
            "Iteration:  61% 233/385 [00:34<00:22,  6.76it/s]\u001b[A\n",
            "Iteration:  61% 234/385 [00:35<00:22,  6.74it/s]\u001b[A\n",
            "Iteration:  61% 235/385 [00:35<00:22,  6.68it/s]\u001b[A\n",
            "Iteration:  61% 236/385 [00:35<00:22,  6.70it/s]\u001b[A\n",
            "Iteration:  62% 237/385 [00:35<00:21,  6.75it/s]\u001b[A\n",
            "Iteration:  62% 238/385 [00:35<00:21,  6.73it/s]\u001b[A\n",
            "Iteration:  62% 239/385 [00:35<00:21,  6.71it/s]\u001b[A\n",
            "Iteration:  62% 240/385 [00:36<00:21,  6.63it/s]\u001b[A\n",
            "Iteration:  63% 241/385 [00:36<00:21,  6.76it/s]\u001b[A\n",
            "Iteration:  63% 242/385 [00:36<00:21,  6.70it/s]\u001b[A\n",
            "Iteration:  63% 243/385 [00:36<00:21,  6.73it/s]\u001b[A\n",
            "Iteration:  63% 244/385 [00:36<00:20,  6.77it/s]\u001b[A\n",
            "Iteration:  64% 245/385 [00:36<00:20,  6.72it/s]\u001b[A\n",
            "Iteration:  64% 246/385 [00:36<00:20,  6.74it/s]\u001b[A\n",
            "Iteration:  64% 247/385 [00:37<00:20,  6.78it/s]\u001b[A\n",
            "Iteration:  64% 248/385 [00:37<00:20,  6.75it/s]\u001b[A\n",
            "Iteration:  65% 249/385 [00:37<00:20,  6.72it/s]\u001b[A\n",
            "Iteration:  65% 250/385 [00:37<00:19,  6.76it/s]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "\n",
            "Iteration:  65% 251/385 [00:37<00:18,  7.14it/s]\u001b[A\n",
            "Iteration:  65% 252/385 [00:37<00:18,  7.37it/s]\u001b[A\n",
            "Iteration:  66% 253/385 [00:37<00:18,  7.20it/s]\u001b[A\n",
            "Iteration:  66% 254/385 [00:38<00:19,  6.89it/s]\u001b[A\n",
            "Iteration:  66% 255/385 [00:38<00:19,  6.73it/s]\u001b[A\n",
            "Iteration:  66% 256/385 [00:38<00:18,  6.79it/s]\u001b[A\n",
            "Iteration:  67% 257/385 [00:38<00:18,  6.81it/s]\u001b[A\n",
            "Iteration:  67% 258/385 [00:38<00:18,  6.83it/s]\u001b[A\n",
            "Iteration:  67% 259/385 [00:38<00:18,  6.83it/s]\u001b[A\n",
            "Iteration:  68% 260/385 [00:38<00:18,  6.80it/s]\u001b[A\n",
            "Iteration:  68% 261/385 [00:39<00:18,  6.78it/s]\u001b[A\n",
            "Iteration:  68% 262/385 [00:39<00:18,  6.75it/s]\u001b[A\n",
            "Iteration:  68% 263/385 [00:39<00:18,  6.73it/s]\u001b[A\n",
            "Iteration:  69% 264/385 [00:39<00:17,  6.75it/s]\u001b[A\n",
            "Iteration:  69% 265/385 [00:39<00:17,  6.80it/s]\u001b[A\n",
            "Iteration:  69% 266/385 [00:39<00:17,  6.83it/s]\u001b[A\n",
            "Iteration:  69% 267/385 [00:39<00:17,  6.84it/s]\u001b[A\n",
            "Iteration:  70% 268/385 [00:40<00:17,  6.82it/s]\u001b[A\n",
            "Iteration:  70% 269/385 [00:40<00:17,  6.77it/s]\u001b[A\n",
            "Iteration:  70% 270/385 [00:40<00:16,  6.80it/s]\u001b[A\n",
            "Iteration:  70% 271/385 [00:40<00:16,  6.76it/s]\u001b[A\n",
            "Iteration:  71% 272/385 [00:40<00:16,  6.75it/s]\u001b[A\n",
            "Iteration:  71% 273/385 [00:40<00:16,  6.75it/s]\u001b[A\n",
            "Iteration:  71% 274/385 [00:40<00:16,  6.76it/s]\u001b[A\n",
            "Iteration:  71% 275/385 [00:41<00:16,  6.73it/s]\u001b[A\n",
            "Iteration:  72% 276/385 [00:41<00:16,  6.72it/s]\u001b[A\n",
            "Iteration:  72% 277/385 [00:41<00:15,  6.77it/s]\u001b[A\n",
            "Iteration:  72% 278/385 [00:41<00:15,  6.80it/s]\u001b[A\n",
            "Iteration:  72% 279/385 [00:41<00:15,  6.82it/s]\u001b[A\n",
            "Iteration:  73% 280/385 [00:41<00:15,  6.78it/s]\u001b[A\n",
            "Iteration:  73% 281/385 [00:42<00:15,  6.71it/s]\u001b[A\n",
            "Iteration:  73% 282/385 [00:42<00:15,  6.69it/s]\u001b[A\n",
            "Iteration:  74% 283/385 [00:42<00:15,  6.68it/s]\u001b[A\n",
            "Iteration:  74% 284/385 [00:42<00:14,  6.75it/s]\u001b[A\n",
            "Iteration:  74% 285/385 [00:42<00:14,  6.73it/s]\u001b[A\n",
            "Iteration:  74% 286/385 [00:42<00:14,  6.74it/s]\u001b[A\n",
            "Iteration:  75% 287/385 [00:42<00:14,  6.73it/s]\u001b[A\n",
            "Iteration:  75% 288/385 [00:43<00:14,  6.67it/s]\u001b[A\n",
            "Iteration:  75% 289/385 [00:43<00:14,  6.74it/s]\u001b[A\n",
            "Iteration:  75% 290/385 [00:43<00:14,  6.70it/s]\u001b[A\n",
            "Iteration:  76% 291/385 [00:43<00:13,  6.73it/s]\u001b[A\n",
            "Iteration:  76% 292/385 [00:43<00:13,  6.71it/s]\u001b[A\n",
            "Iteration:  76% 293/385 [00:43<00:13,  6.71it/s]\u001b[A\n",
            "Iteration:  76% 294/385 [00:43<00:13,  6.70it/s]\u001b[A\n",
            "Iteration:  77% 295/385 [00:44<00:13,  6.69it/s]\u001b[A\n",
            "Iteration:  77% 296/385 [00:44<00:13,  6.69it/s]\u001b[A\n",
            "Iteration:  77% 297/385 [00:44<00:13,  6.75it/s]\u001b[A\n",
            "Iteration:  77% 298/385 [00:44<00:12,  6.69it/s]\u001b[A\n",
            "Iteration:  78% 299/385 [00:44<00:12,  6.71it/s]\u001b[A\n",
            "Iteration:  78% 300/385 [00:44<00:12,  6.68it/s]\u001b[A\n",
            "Iteration:  78% 301/385 [00:45<00:12,  6.69it/s]\u001b[A\n",
            "Iteration:  78% 302/385 [00:45<00:12,  6.61it/s]\u001b[A\n",
            "Iteration:  79% 303/385 [00:45<00:12,  6.71it/s]\u001b[A\n",
            "Iteration:  79% 304/385 [00:45<00:12,  6.69it/s]\u001b[A\n",
            "Iteration:  79% 305/385 [00:45<00:11,  6.71it/s]\u001b[A\n",
            "Iteration:  79% 306/385 [00:45<00:11,  6.67it/s]\u001b[A\n",
            "Iteration:  80% 307/385 [00:45<00:11,  6.78it/s]\u001b[A\n",
            "Iteration:  80% 308/385 [00:46<00:11,  6.69it/s]\u001b[A\n",
            "Iteration:  80% 309/385 [00:46<00:11,  6.77it/s]\u001b[A\n",
            "Iteration:  81% 310/385 [00:46<00:11,  6.70it/s]\u001b[A\n",
            "Iteration:  81% 311/385 [00:46<00:10,  6.84it/s]\u001b[A\n",
            "Iteration:  81% 312/385 [00:46<00:10,  6.80it/s]\u001b[A\n",
            "Iteration:  81% 313/385 [00:46<00:10,  6.76it/s]\u001b[A\n",
            "Iteration:  82% 314/385 [00:46<00:10,  6.78it/s]\u001b[A\n",
            "Iteration:  82% 315/385 [00:47<00:10,  6.80it/s]\u001b[A\n",
            "Iteration:  82% 316/385 [00:47<00:10,  6.70it/s]\u001b[A\n",
            "Iteration:  82% 317/385 [00:47<00:10,  6.75it/s]\u001b[A\n",
            "Iteration:  83% 318/385 [00:47<00:09,  6.80it/s]\u001b[A\n",
            "Iteration:  83% 319/385 [00:47<00:09,  6.77it/s]\u001b[A\n",
            "Iteration:  83% 320/385 [00:47<00:09,  6.73it/s]\u001b[A\n",
            "Iteration:  83% 321/385 [00:47<00:09,  6.70it/s]\u001b[A\n",
            "Iteration:  84% 322/385 [00:48<00:09,  6.72it/s]\u001b[A\n",
            "Iteration:  84% 323/385 [00:48<00:09,  6.69it/s]\u001b[A\n",
            "Iteration:  84% 324/385 [00:48<00:09,  6.74it/s]\u001b[A\n",
            "Iteration:  84% 325/385 [00:48<00:08,  6.72it/s]\u001b[A\n",
            "Iteration:  85% 326/385 [00:48<00:08,  6.73it/s]\u001b[A\n",
            "Iteration:  85% 327/385 [00:48<00:08,  6.65it/s]\u001b[A\n",
            "Iteration:  85% 328/385 [00:49<00:08,  6.72it/s]\u001b[A\n",
            "Iteration:  85% 329/385 [00:49<00:08,  6.64it/s]\u001b[A\n",
            "Iteration:  86% 330/385 [00:49<00:08,  6.72it/s]\u001b[A\n",
            "Iteration:  86% 331/385 [00:49<00:08,  6.67it/s]\u001b[A\n",
            "Iteration:  86% 332/385 [00:49<00:07,  6.76it/s]\u001b[A\n",
            "Iteration:  86% 333/385 [00:49<00:07,  6.68it/s]\u001b[A\n",
            "Iteration:  87% 334/385 [00:49<00:07,  6.77it/s]\u001b[A\n",
            "Iteration:  87% 335/385 [00:50<00:07,  6.74it/s]\u001b[A\n",
            "Iteration:  87% 336/385 [00:50<00:07,  6.81it/s]\u001b[A\n",
            "Iteration:  88% 337/385 [00:50<00:07,  6.71it/s]\u001b[A\n",
            "Iteration:  88% 338/385 [00:50<00:06,  6.77it/s]\u001b[A\n",
            "Iteration:  88% 339/385 [00:50<00:06,  6.76it/s]\u001b[A\n",
            "Iteration:  88% 340/385 [00:50<00:06,  6.77it/s]\u001b[A\n",
            "Iteration:  89% 341/385 [00:50<00:06,  6.73it/s]\u001b[A\n",
            "Iteration:  89% 342/385 [00:51<00:06,  6.78it/s]\u001b[A\n",
            "Iteration:  89% 343/385 [00:51<00:06,  6.80it/s]\u001b[A\n",
            "Iteration:  89% 344/385 [00:51<00:06,  6.72it/s]\u001b[A\n",
            "Iteration:  90% 345/385 [00:51<00:05,  6.74it/s]\u001b[A\n",
            "Iteration:  90% 346/385 [00:51<00:05,  6.70it/s]\u001b[A\n",
            "Iteration:  90% 347/385 [00:51<00:05,  6.68it/s]\u001b[A\n",
            "Iteration:  90% 348/385 [00:52<00:05,  6.69it/s]\u001b[A\n",
            "Iteration:  91% 349/385 [00:52<00:05,  6.73it/s]\u001b[A\n",
            "Iteration:  91% 350/385 [00:52<00:05,  6.63it/s]\u001b[A\n",
            "Iteration:  91% 351/385 [00:52<00:05,  6.69it/s]\u001b[A\n",
            "Iteration:  91% 352/385 [00:52<00:04,  6.63it/s]\u001b[A\n",
            "Iteration:  92% 353/385 [00:52<00:04,  6.72it/s]\u001b[A\n",
            "Iteration:  92% 354/385 [00:52<00:04,  6.65it/s]\u001b[A\n",
            "Iteration:  92% 355/385 [00:53<00:04,  6.67it/s]\u001b[A\n",
            "Iteration:  92% 356/385 [00:53<00:04,  6.56it/s]\u001b[A\n",
            "Iteration:  93% 357/385 [00:53<00:04,  6.74it/s]\u001b[A\n",
            "Iteration:  93% 358/385 [00:53<00:04,  6.68it/s]\u001b[A\n",
            "Iteration:  93% 359/385 [00:53<00:03,  6.73it/s]\u001b[A\n",
            "Iteration:  94% 360/385 [00:53<00:03,  6.68it/s]\u001b[A\n",
            "Iteration:  94% 361/385 [00:53<00:03,  6.74it/s]\u001b[A\n",
            "Iteration:  94% 362/385 [00:54<00:03,  6.70it/s]\u001b[A\n",
            "Iteration:  94% 363/385 [00:54<00:03,  6.74it/s]\u001b[A\n",
            "Iteration:  95% 364/385 [00:54<00:03,  6.70it/s]\u001b[A\n",
            "Iteration:  95% 365/385 [00:54<00:02,  6.77it/s]\u001b[A\n",
            "Iteration:  95% 366/385 [00:54<00:02,  6.77it/s]\u001b[A\n",
            "Iteration:  95% 367/385 [00:54<00:02,  6.81it/s]\u001b[A\n",
            "Iteration:  96% 368/385 [00:54<00:02,  6.76it/s]\u001b[A\n",
            "Iteration:  96% 369/385 [00:55<00:02,  6.77it/s]\u001b[A\n",
            "Iteration:  96% 370/385 [00:55<00:02,  6.80it/s]\u001b[A\n",
            "Iteration:  96% 371/385 [00:55<00:02,  6.75it/s]\u001b[A\n",
            "Iteration:  97% 372/385 [00:55<00:01,  6.70it/s]\u001b[A\n",
            "Iteration:  97% 373/385 [00:55<00:01,  6.76it/s]\u001b[A\n",
            "Iteration:  97% 374/385 [00:55<00:01,  6.76it/s]\u001b[A\n",
            "Iteration:  97% 375/385 [00:56<00:01,  6.68it/s]\u001b[A\n",
            "Iteration:  98% 376/385 [00:56<00:01,  6.68it/s]\u001b[A\n",
            "Iteration:  98% 377/385 [00:56<00:01,  6.61it/s]\u001b[A\n",
            "Iteration:  98% 378/385 [00:56<00:01,  6.68it/s]\u001b[A\n",
            "Iteration:  98% 379/385 [00:56<00:00,  6.64it/s]\u001b[A\n",
            "Iteration:  99% 380/385 [00:56<00:00,  6.66it/s]\u001b[A\n",
            "Iteration:  99% 381/385 [00:56<00:00,  6.64it/s]\u001b[A\n",
            "Iteration:  99% 382/385 [00:57<00:00,  6.66it/s]\u001b[A\n",
            "Iteration:  99% 383/385 [00:57<00:00,  6.61it/s]\u001b[A\n",
            "Iteration: 100% 384/385 [00:57<00:00,  6.60it/s]\u001b[A\n",
            "Iteration: 100% 385/385 [00:57<00:00,  6.69it/s]\n",
            "Epoch: 100% 5/5 [05:15<00:00, 63.01s/it]\n",
            "02/12/2021 08:43:02 - INFO - __main__ -    global_step = 1925, average loss = 2.278779981414993\n",
            "02/12/2021 08:43:02 - INFO - __main__ -   Saving model checkpoint to neurohoroscope_model_wt_sign_0\n",
            "02/12/2021 08:43:02 - INFO - transformers.configuration_utils -   Configuration saved in neurohoroscope_model_wt_sign_0/config.json\n",
            "02/12/2021 08:43:04 - INFO - transformers.modeling_utils -   Model weights saved in neurohoroscope_model_wt_sign_0/pytorch_model.bin\n",
            "02/12/2021 08:43:04 - INFO - transformers.configuration_utils -   loading configuration file neurohoroscope_model_wt_sign_0/config.json\n",
            "02/12/2021 08:43:04 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/12/2021 08:43:04 - INFO - transformers.modeling_utils -   loading weights file neurohoroscope_model_wt_sign_0/pytorch_model.bin\n",
            "02/12/2021 08:43:10 - INFO - transformers.configuration_utils -   loading configuration file neurohoroscope_model_wt_sign_0/config.json\n",
            "02/12/2021 08:43:10 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/12/2021 08:43:10 - INFO - transformers.tokenization_utils -   Model name 'neurohoroscope_model_wt_sign_0' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'neurohoroscope_model_wt_sign_0' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/12/2021 08:43:10 - INFO - transformers.tokenization_utils -   Didn't find file neurohoroscope_model_wt_sign_0/added_tokens.json. We won't load it.\n",
            "02/12/2021 08:43:10 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/vocab.json\n",
            "02/12/2021 08:43:10 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/merges.txt\n",
            "02/12/2021 08:43:10 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/12/2021 08:43:10 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/special_tokens_map.json\n",
            "02/12/2021 08:43:10 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/tokenizer_config.json\n",
            "02/12/2021 08:43:10 - INFO - __main__ -   Evaluate the following checkpoints: ['neurohoroscope_model_wt_sign_0']\n",
            "02/12/2021 08:43:10 - INFO - transformers.configuration_utils -   loading configuration file neurohoroscope_model_wt_sign_0/config.json\n",
            "02/12/2021 08:43:10 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/12/2021 08:43:10 - INFO - transformers.modeling_utils -   loading weights file neurohoroscope_model_wt_sign_0/pytorch_model.bin\n",
            "02/12/2021 08:43:15 - INFO - __main__ -   Creating features from dataset file at \n",
            "02/12/2021 08:43:16 - INFO - __main__ -   Saving features into cached file gpt2_cached_lm_512_valid.txt\n",
            "02/12/2021 08:43:16 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/12/2021 08:43:16 - INFO - __main__ -     Num examples = 67\n",
            "02/12/2021 08:43:16 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 17/17 [00:02<00:00,  8.22it/s]\n",
            "02/12/2021 08:43:18 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/12/2021 08:43:18 - INFO - __main__ -     perplexity = tensor(18.4858)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2N6ylGPt1F5"
      },
      "source": [
        "## Check our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRlAAsIbsHdf"
      },
      "source": [
        "# !python generate_transformers.py \\\n",
        "#     --model_type=gpt2 \\\n",
        "#     --model_name_or_path=neurohoroscope_model_wt_sign_0 \\\n",
        "#     --k=5 \\\n",
        "#     --p=0.95 \\\n",
        "#     --length=300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laEca7IGFS-N"
      },
      "source": [
        "import os\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    CTRLLMHeadModel,\n",
        "    CTRLTokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    OpenAIGPTLMHeadModel,\n",
        "    OpenAIGPTTokenizer,\n",
        "    TransfoXLLMHeadModel,\n",
        "    TransfoXLTokenizer,\n",
        "    XLMTokenizer,\n",
        "    XLMWithLMHeadModel,\n",
        "    XLNetLMHeadModel,\n",
        "    XLNetTokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
        "    \"ctrl\": (CTRLLMHeadModel, CTRLTokenizer),\n",
        "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
        "    \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n",
        "    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
        "    \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n",
        "}\n",
        "\n",
        "# Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
        "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
        "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
        "(except for Alexei and Maria) are discovered.\n",
        "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
        "remainder of the story. 1883 Western Siberia,\n",
        "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        "father initially slaps him for making such an accusation, Rasputin watches as the\n",
        "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "#\n",
        "# Functions to prepare models' input\n",
        "#\n",
        "\n",
        "\n",
        "def prepare_ctrl_input(args, _, tokenizer, prompt_text):\n",
        "    if args.temperature > 0.7:\n",
        "        logger.info(\"CTRL typically works better with lower temperatures (and lower top_k).\")\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
        "    if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):\n",
        "        logger.info(\"WARNING! You are not starting your generation from a control code so you won't get good results\")\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_xlm_input(args, model, tokenizer, prompt_text):\n",
        "    # kwargs = {\"language\": None, \"mask_token_id\": None}\n",
        "\n",
        "    # Set the language\n",
        "    use_lang_emb = hasattr(model.config, \"use_lang_emb\") and model.config.use_lang_emb\n",
        "    if hasattr(model.config, \"lang2id\") and use_lang_emb:\n",
        "        available_languages = model.config.lang2id.keys()\n",
        "        if args.xlm_language in available_languages:\n",
        "            language = args.xlm_language\n",
        "        else:\n",
        "            language = None\n",
        "            while language not in available_languages:\n",
        "                language = input(\"Using XLM. Select language in \" + str(list(available_languages)) + \" >>> \")\n",
        "\n",
        "        model.config.lang_id = model.config.lang2id[language]\n",
        "        # kwargs[\"language\"] = tokenizer.lang2id[language]\n",
        "\n",
        "    # TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers\n",
        "    # XLM masked-language modeling (MLM) models need masked token\n",
        "    # is_xlm_mlm = \"mlm\" in args.model_name_or_path\n",
        "    # if is_xlm_mlm:\n",
        "    #     kwargs[\"mask_token_id\"] = tokenizer.mask_token_id\n",
        "\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_xlnet_input(args, _, tokenizer, prompt_text):\n",
        "    prompt_text = (args.padding_text if args.padding_text else PADDING_TEXT) + prompt_text\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_transfoxl_input(args, _, tokenizer, prompt_text):\n",
        "    prompt_text = (args.padding_text if args.padding_text else PADDING_TEXT) + prompt_text\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "PREPROCESSING_FUNCTIONS = {\n",
        "    \"ctrl\": prepare_ctrl_input,\n",
        "    \"xlm\": prepare_xlm_input,\n",
        "    \"xlnet\": prepare_xlnet_input,\n",
        "    \"transfo-xl\": prepare_transfoxl_input,\n",
        "}\n",
        "\n",
        "\n",
        "def adjust_length_to_model(length, max_sequence_length):\n",
        "    if length < 0 and max_sequence_length > 0:\n",
        "        length = max_sequence_length\n",
        "    elif 0 < max_sequence_length < length:\n",
        "        length = max_sequence_length  # No generation bigger than model size\n",
        "    elif length < 0:\n",
        "        length = MAX_LENGTH  # avoid infinite loop\n",
        "    return length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP5KuaZjGF2V"
      },
      "source": [
        "length = 300\n",
        "model_type = 'gpt2'\n",
        "model_name_or_path = 'neurohoroscope_model_wt_sign_0'\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLzi0i7iGmw0"
      },
      "source": [
        "model_class, tokenizer_class = MODEL_CLASSES[model_type]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGLk27H7Fbrq",
        "outputId": "43b61278-a29f-458b-86f7-5dd7217a395a"
      },
      "source": [
        "tokenizer = tokenizer_class.from_pretrained(model_name_or_path)\n",
        "model = model_class.from_pretrained(model_name_or_path)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:46:15 - INFO - transformers.tokenization_utils -   Model name 'neurohoroscope_model_wt_sign_0' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'neurohoroscope_model_wt_sign_0' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/12/2021 08:46:15 - INFO - transformers.tokenization_utils -   Didn't find file neurohoroscope_model_wt_sign_0/added_tokens.json. We won't load it.\n",
            "02/12/2021 08:46:15 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/vocab.json\n",
            "02/12/2021 08:46:15 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/merges.txt\n",
            "02/12/2021 08:46:15 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/12/2021 08:46:15 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/special_tokens_map.json\n",
            "02/12/2021 08:46:15 - INFO - transformers.tokenization_utils -   loading file neurohoroscope_model_wt_sign_0/tokenizer_config.json\n",
            "02/12/2021 08:46:15 - INFO - transformers.configuration_utils -   loading configuration file neurohoroscope_model_wt_sign_0/config.json\n",
            "02/12/2021 08:46:15 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/12/2021 08:46:15 - INFO - transformers.modeling_utils -   loading weights file neurohoroscope_model_wt_sign_0/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(2048, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vqH_UhtGadq",
        "outputId": "4adedbed-4a89-46d8-e1da-c26e4845b194"
      },
      "source": [
        "length = adjust_length_to_model(length, max_sequence_length=model.config.max_position_embeddings);length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTu-cvTgHAYL"
      },
      "source": [
        "def encode_prompt(prompt_text):\n",
        "  requires_preprocessing = model_type in PREPROCESSING_FUNCTIONS.keys()\n",
        "  if requires_preprocessing:\n",
        "      prepare_input = PREPROCESSING_FUNCTIONS.get(model_type)\n",
        "      preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)\n",
        "      encoded_prompt = tokenizer.encode(\n",
        "          preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", add_space_before_punct_symbol=True\n",
        "      )\n",
        "  else:\n",
        "      encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(device)\n",
        "  return encoded_prompt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ3cN8EpJMDq"
      },
      "source": [
        "k=5\n",
        "p=0.95\n",
        "temperature=1.0\n",
        "repetition_penalty=1.0\n",
        "stop_token='</s>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXM9vjIMMD3Q"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMHQOArCHTf1"
      },
      "source": [
        "def get_generated_sequences(prompt, num_return_sequences=5, temperature=1.0, p=0.95, k=5):\n",
        "  print('temperature: ', temperature, ' k:', k, ' p: ', p)\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=prompt,\n",
        "      max_length=length + len(prompt[0]),\n",
        "      temperature=temperature,\n",
        "      top_k=k,\n",
        "      top_p=p,\n",
        "      repetition_penalty=repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=num_return_sequences,\n",
        "  )\n",
        "  generated_sequences = []\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "      # print(\"ruGPT:\".format(generated_sequence_idx + 1))\n",
        "      generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "      # Decode text\n",
        "      text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "      # Remove all text after the stop token\n",
        "      text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "      # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "      total_sequence = (\n",
        "          # prompt_text + text[len(tokenizer.decode(prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "          text[len(tokenizer.decode(prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "      )\n",
        "\n",
        "      generated_sequences.append(total_sequence)\n",
        "      # os.system('clear')\n",
        "      # print(total_sequence)\n",
        "  return generated_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PMJD0ldGdqW",
        "outputId": "573dbef9-9687-41ee-d5d6-59b75c3301d5"
      },
      "source": [
        "pip install python-obscene-words-filter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-obscene-words-filter\n",
            "  Downloading https://files.pythonhosted.org/packages/54/c2/7ae2a9065b39555637bbabd67aa4d98bf85bd69a7e8fee4b7e0dc1b4b702/python-obscene-words-filter-0.1.6.tar.gz\n",
            "Building wheels for collected packages: python-obscene-words-filter\n",
            "  Building wheel for python-obscene-words-filter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-obscene-words-filter: filename=python_obscene_words_filter-0.1.6-cp36-none-any.whl size=12005 sha256=e9971bcef4c4acbb5a90f040fbe30b04addc54033a50982de133bd5d8aa08461\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/1d/25/e1c669b2e31702dd5bc59c9165b032d7875327a52f900430aa\n",
            "Successfully built python-obscene-words-filter\n",
            "Installing collected packages: python-obscene-words-filter\n",
            "Successfully installed python-obscene-words-filter-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdVW4s73Gnwj"
      },
      "source": [
        "from obscene_words_filter.default import get_default_filter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBBAl6knJlob"
      },
      "source": [
        "obscene_words_filter = get_default_filter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3xZTPv5ovOf"
      },
      "source": [
        "def get_filtered_seqs(seq_generator):\n",
        "  filtered_sequences = []\n",
        "\n",
        "  generated_sequences = seq_generator()\n",
        "\n",
        "  for seq in generated_sequences:\n",
        "    obscene_words = obscene_words_filter.find_bad_word_matches_without_good_words(seq)\n",
        "    if len(list(obscene_words)) > 0:\n",
        "      print('Filtered obscene sec: ', seq)\n",
        "    else:\n",
        "      filtered_sequences.append(seq)\n",
        "    return filtered_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEVqR_EqKX85"
      },
      "source": [
        "## export\n",
        "def get_values_fixed_number(num_vals, generator):\n",
        "  values = []\n",
        "  while len(values) <= num_vals:\n",
        "    values.extend(generator())\n",
        "    print('Generated ', len(values), ' vals')\n",
        "  return values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEwCT2Rw00Y1"
      },
      "source": [
        "from datetime import date\n",
        "from datetime import timedelta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsTKuNSVzeDr"
      },
      "source": [
        "def save_to_file(seqs, file_name, with_date:bool):\n",
        "  with open(file_name, \"w\") as file:\n",
        "      today = date.today()\n",
        "      days = 0 \n",
        "      if with_date:\n",
        "        file.write(\"date,text\\n\")\n",
        "      else:\n",
        "        file.write(\"text\\n\")\n",
        "      for x in seqs:\n",
        "        if with_date:\n",
        "          file.write('\"' + str(today+timedelta(days=days)) + '\",\"' + x.strip() + '\"\\n')\n",
        "        else:\n",
        "          file.write('\"' + x.strip() + '\"\\n')\n",
        "        days = days + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWKC97Yt5Tkc"
      },
      "source": [
        "from shutil import copyfile\n",
        "def save_to_drive(file_name):\n",
        "  copyfile(file_name, '/content/drive/MyDrive/neurohoroscope/'+file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGwCJeQ8y9OC"
      },
      "source": [
        "horoscopes = {\n",
        "    'aries':'Овен: ',\n",
        "    'taurus':'Телец: ',\n",
        "    'twins':'Близнецы: ',\n",
        "    'cancer':'Рак: ',\n",
        "    'leo':'Лев: ',\n",
        "    'maid':'Дева: ',\n",
        "    'scales':'Весы: ',\n",
        "    'scorpio':'Скорпион: ',\n",
        "    'sagittarius':'Стрелец: ',\n",
        "    'capricorn':'Козерог: ',\n",
        "    'aquarius':'Водолей: ',\n",
        "    'pisces':'Рыбы: '\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF6btP_Zy6Kr"
      },
      "source": [
        "num = 500\n",
        "for horoscope in horoscopes:\n",
        "  print(horoscopes[horoscope])\n",
        "  encoded_prompt = encode_prompt(horoscopes[horoscope])\n",
        "  generator = partial(get_generated_sequences, temperature=1.0, prompt=encoded_prompt, num_return_sequences=10)\n",
        "  filtered_seqs = get_values_fixed_number(num, partial(get_filtered_seqs, generator))\n",
        "  file_name = horoscope+'.csv'\n",
        "  save_to_file(filtered_seqs, horoscope+'.csv', True)\n",
        "  save_to_drive(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2gBPvqtfXoa",
        "outputId": "74d6baba-edb0-447c-955f-2e5b372ae5f4"
      },
      "source": [
        "# export general horoscope\n",
        "num = 1000\n",
        "encoded_prompt = encode_prompt('гороскоп:')\n",
        "generator = partial(get_generated_sequences, temperature=1.0, prompt=encoded_prompt, num_return_sequences=50)\n",
        "filtered_seqs = get_values_fixed_number(num, partial(get_filtered_seqs, generator))\n",
        "file_name =  'general_horoscope_2.csv'\n",
        "save_to_file(filtered_seqs, file_name, False)\n",
        "save_to_drive(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:46:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:47:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  1  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:47:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  2  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:47:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  3  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:47:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  4  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:47:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня ваш день.  Не стоит расстраиваться. День хорош для того, чтобы  прыгать через забор. Вы можете  обнаружить, что ваша одежда сделана из  старой кожи, а ваши глаза открыты. Не стоит беспокоиться. День благоприятен для  принятия родов, но не для еды.  Вы можете  увидеть  много дерьма, например, мусор, который выбрасывают ваши соседи. Не стоит беспокоиться.\n",
            "Generated  4  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:48:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  5  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:48:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  6  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:48:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  7  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:48:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете ощутить присутствие демонических ангелов, которые несут вам службу. Вы можете использовать свои руки для того, чтобы перепрыгивать через мусорные баки, и ваши глаза могут видеть все, что хотите. Сегодня вы можете использовать свои руки для того, чтобы перепрыгивать через мусорные баки, и ваши глаза могут видеть все, что пожелаете.\n",
            "Generated  7  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:48:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  8  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:48:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  9  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:49:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  10  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:49:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  11  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:49:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  12  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:49:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  13  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:49:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  14  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:50:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам не стоит бояться быть раздавленным огромным комодом дерьма. Вы можете спокойно продолжать наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнью. Вы можете наслаждаться жизнь\n",
            "Generated  14  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:50:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  15  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:50:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня ваш ум будет полон ненависти ко всем вам и вашему окружению. Не бойтесь упасть в обморок или начать кричать. Не бойтесь упасть в яму с дерьмом.  Если вы видите что-то грязное или грязное в своей комнате, немедленно убирайтесь оттуда. Не бойтесь того дня, когда вы можете увидеть кого-то в нижнем белье.\n",
            "Generated  15  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:50:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  16  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:50:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  17  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:50:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  18  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:51:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  19  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:51:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  20  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:51:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  21  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:51:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  22  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:51:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  23  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:52:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  24  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:52:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  25  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:52:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  26  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:52:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  27  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:52:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  28  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:52:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  29  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:53:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  30  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:53:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  31  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:53:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  32  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:53:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  33  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:53:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  34  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:54:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  35  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:54:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  36  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:54:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  37  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:54:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  38  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:54:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  39  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:54:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  40  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:55:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  41  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:55:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  42  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:55:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  43  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:55:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  44  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:55:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  45  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:56:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  46  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:56:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  47  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:56:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  48  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:56:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  49  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:56:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  50  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:56:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  51  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:57:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  52  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:57:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  53  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:57:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  54  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:57:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  55  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:57:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  56  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:58:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  57  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:58:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  58  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:58:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  59  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:58:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  60  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:58:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  61  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:58:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  62  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:59:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  63  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:59:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день вы можете стать свидетелем того, как свиньи падают в яму с дерьмом. В этот день вам следует избегать контакта с людьми. Не ешьте слишком много сырого мяса, оно может быть опасным. Не бойтесь быть раздавленным насекомым. Вы также можете стать свидетелем того, как ваши ноги начинают бегать. Не бойтесь быть раздавленным картофелем\n",
            "Generated  63  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:59:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете ощутить прилив сил и бодрости, которые благотворно сказываются на вашем здоровье. Будьте осторожны с едой и не ешьте слишком много красного мяса, которое выбрасывается на рынок, как это было несколько раз в прошлом году.\n",
            "Generated  63  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:59:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  64  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:59:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  65  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 08:59:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  66  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:00:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  67  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:00:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  68  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:00:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете ощутить прилив сил и вдохновения, которое благотворно отразится на вашем здоровье и на вашем благосостоянии. Вы можете начать снимать порнографии, слушать радио и вести здоровый образ жизни. Не бойтесь делать ошибки и не поддавайтесь искушению залезть в лужи.\n",
            "Generated  68  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:00:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  69  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:00:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  70  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:01:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  71  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:01:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день вы можете стать свидетелем того, как люди падают в яму с дерьмом. Не стоит расстраиваться из-за плохой памяти или плохой реакции на свет. В этот день вы можете встретить двух или трех уродов, которые захотят разорвать вам горло и сломать шею. Вы можете встретить молодого человека с длинными черными волосами и длинными черными глазами, которые будут смотреть на вас, как на сумасшедшего.\n",
            "Generated  71  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:01:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  72  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:01:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  73  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:01:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  74  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:01:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  75  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:02:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  76  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:02:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  77  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:02:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  78  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:02:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  79  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:02:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  80  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:03:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  81  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:03:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  82  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:03:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  83  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:03:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  84  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:03:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  85  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:03:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  86  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:04:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы должны избегать контактов с внешним миром, потому что он может использовать вас в качестве оружия против вас. Будьте очень осторожны, чтобы не столкнуться с искушением украсть что-то из одежды и не упасть в яму с дерьмом. Вечером можно встретить кого-то, кто похож на вас, и он попытается укусить вас за ногу.\n",
            "Generated  86  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:04:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  87  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:04:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  88  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:04:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  89  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:04:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  90  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:05:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  91  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:05:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  92  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:05:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  93  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:05:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  94  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:05:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  95  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:05:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  96  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:06:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  97  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:06:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  98  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:06:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  99  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:06:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  100  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:06:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  101  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:07:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  102  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:07:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  103  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:07:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  104  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:07:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  105  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:07:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  106  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:07:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  107  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:08:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  108  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:08:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете быть в хорошем настроении и быть в хорошем настроении. Не бойтесь упасть в лужу с медом и разбить окно.  Вы можете быть в хорошем настроении, но будьте осторожны с пуговицами.  В это время возможны неприятности, например, ваши ноги могут выпасть из реальности. Не бойтесь упасть в лужу с дерьмом. Не бойтесь влюбиться.\n",
            "Generated  108  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:08:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  109  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:08:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  110  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:08:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  111  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:09:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  112  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:09:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  113  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:09:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  114  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:09:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  115  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:09:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  116  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:09:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  117  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:10:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  118  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:10:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  119  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:10:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  120  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:10:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  121  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:10:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  122  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:10:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  123  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:11:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  124  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:11:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  125  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:11:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  126  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:11:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  127  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:11:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  128  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:12:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  129  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:12:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня звезды советуют вам быть в форме, потому что в этом случае вы будете похожи на свинью. Вы можете превратиться в жирного медвежонка и упасть в яму с говном. Вы можете превратиться в мокрого кота или в белого слона. Вам не придется беспокоиться о том, что ваши зубы сломаются. Сегодня у вас будет шанс познакомиться со старым дедушкой и его новой маленькой дочкой. Вечером можно встретить старого кота с грязными ногами и огромной бородой, который расскажет вам много интересного о вашей новой жизни.\n",
            "Generated  129  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:12:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  130  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:12:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  131  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:12:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  132  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:12:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  133  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:13:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  134  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:13:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  135  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:13:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  136  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:13:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  137  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:13:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  138  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:14:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  139  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:14:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  140  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:14:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам следует избегать контактов с людьми, которые выглядят как маленькие собачники. Возможно, вам придется провести ночь в сарае или подвале, где обитают бродяги. Вы можете быть поражены, если увидите на полу мусорные баки, в которых собираются насекомые. Вечером рекомендуется выпить стакан водки и закусить. Вы можете обнаружить, что ваша кожа покрыта слизью. Сегодня вы можете стать старым и грязным. \n",
            "Generated  140  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:14:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  141  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:14:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  142  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:14:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  143  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:15:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  144  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:15:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  145  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:15:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  146  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:15:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  147  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:15:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  148  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:16:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  149  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:16:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  150  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:16:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  151  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:16:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  152  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:16:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  153  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:16:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  154  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:17:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  155  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:17:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  156  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:17:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  157  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:17:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  158  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:17:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  159  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:18:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  160  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:18:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  161  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:18:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  162  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:18:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  163  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:18:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  164  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:18:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  165  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:19:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  166  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:19:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  167  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:19:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  168  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:19:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  169  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:19:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  170  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:20:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  171  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:20:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня ваш ум находится под давлением, и вы можете начать видеть галлюцинации. Вы можете обнаружить, что ваши руки покрыты каплями пота, а ваши волосы выбриты. Сегодня вам следует избегать употребления сырого фарфора и острых специй. Вы можете быть поражены, если ваши зубы сломаются. Не бойтесь упасть в яму с дерьмом\n",
            "Generated  171  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:20:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  172  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:20:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  173  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:20:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  174  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:20:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  175  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:21:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  176  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:21:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы будете наслаждаться тишиной, спокойствием и покоем.  Звезды рекомендуют вам не выходить из дома, пока не достигнете зрелости и не начнёте работать. Вечером вам нужно будет переодеться мужчиной, чтобы не попасть в  ловушку обольщения.  Не бойтесь упасть в яму с дерьмом.\n",
            "Generated  176  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:21:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  177  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:21:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  178  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:21:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  179  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:21:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  180  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:22:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  181  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:22:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  182  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:22:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  183  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:22:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  184  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:22:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  185  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:23:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  186  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:23:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  187  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:23:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  188  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:23:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день вам следует избегать  отношений с людьми, особенно из-за их сексуальной ориентации. В этот день вы можете  увидеть, как свиньи падают с неба на землю. Не рекомендуется вступать в сексуальные отношения с собаками, так как могут возникнуть серьезные проблемы с психикой.  Не рекомендуется вступать в сексуальные отношения с цыганами\n",
            "Generated  188  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:23:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  189  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:23:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  190  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:24:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  191  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:24:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  192  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:24:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  193  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:24:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  194  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:24:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  195  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:25:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  196  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:25:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  197  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:25:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  198  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:25:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  199  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:25:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  200  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:25:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  201  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:26:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  202  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:26:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  203  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:26:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  204  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:26:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  205  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:26:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  206  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:27:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  207  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:27:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  208  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:27:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  209  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:27:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  210  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:27:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  211  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:27:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  212  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:28:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  213  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:28:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  214  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:28:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  215  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:28:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  216  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:28:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  217  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:29:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  218  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:29:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  219  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:29:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  220  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:29:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  221  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:29:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Не бойтесь быть укушенным комарами. Не бойтесь упасть в яму с дерьмом, потому что вы находитесь в очень опасном положении. Вечером можно встретить двух или трех крупных псов, которые могут укусить вас за ногу.\n",
            "Generated  221  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:29:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  222  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:30:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  223  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:30:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  224  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:30:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  225  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:30:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  226  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:30:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  227  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:31:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  228  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:31:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  229  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:31:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  230  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:31:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  231  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:31:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  232  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:31:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  233  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:32:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  234  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:32:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  235  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:32:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  236  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:32:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  237  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:32:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  238  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:32:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  239  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:33:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  240  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:33:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  241  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:33:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам следует быть внимательнее к  буквам «о».  В них отражено то, что скрыто в каждом из нас.  Это то, чего нам следует бояться.  Если вы обнаружите, что стали жертвой заговора нечистой силы, не бойтесь обращаться к колдуну с вопросом, который может поставить вас в неудобное положение.  Сегодня вам следует избегать употребления  сырого фарша из коровьего дерьма. \n",
            "Generated  241  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:33:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  242  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:33:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  243  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:34:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  244  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:34:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  245  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:34:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  246  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:34:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  247  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:34:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  248  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:34:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  249  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:35:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  250  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:35:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Не исключено, что сегодня вы можете стать свидетелем того, как свиньи падают на землю с неба.  Не бойтесь того дня, когда вам придется лечь в больницу. Сегодня вы можете стать свидетелем того, как свиньи падают на землю с неба с неба. \n",
            "Generated  250  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:35:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  251  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:35:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  252  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:35:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  253  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:36:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  254  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:36:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день вам не придется делать ни одной ошибки, если вы  будете  лежать в своей постели. Не надо бояться того, кого вы знаете.  Вы можете притвориться спящим и притвориться мертвым. Не бойтесь того дня, когда вас посетит дьявол. Не надо бояться того дня, когда вы будете есть дерьмо. Не надо бояться того дня, когда вы будете играть на скрипке.\n",
            "Generated  254  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:36:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  255  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:36:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  256  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:36:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  257  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:36:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  258  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:37:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  259  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:37:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день вам следует вести себя так, словно вы не ебанулись. Возможно, вы захотите стать начальником пожарной охраны или полиции штата Орегон. Сегодня вы можете рассчитывать на  то, что ваше тело вернется в лоно семьи и будет хорошо служить вам.\n",
            "Generated  259  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:37:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  260  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:37:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня звезды советуют вам не прыгать с высокого небоскреба, а просто оставаться на крыше. Вы можете использовать свои руки для того, чтобы перепрыгивать через мусорные баки и мусорные баки, но не прыгайте со второго этажа. \n",
            "Generated  260  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:37:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  261  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:38:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  262  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:38:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам не следует вступать в половую связь с нищими людьми, поскольку они не соблюдают пост и не соблюдают правила гигиены. Не бойтесь упасть в яму с дерьмом. \n",
            "Generated  262  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:38:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  263  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:38:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  264  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:38:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  265  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:38:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  266  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:39:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  267  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:39:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  268  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:39:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня звезды советуют вам не тратить энергию на мусорные баки, а просто держать их в карманах. В этот день вы можете стать начальником поезда, а может и кем-нибудь еще. Вы также можете купить себе лошадь или козу. Не бойтесь упасть в лужу. Не бойтесь того, кого вы держите за горло. Не бойтесь того, кого вы держите за горло.\n",
            "Generated  268  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:39:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  269  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:39:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  270  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:40:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  271  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:40:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  272  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:40:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  273  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:40:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  274  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:40:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня звезды советуют вам взять два куска льда из бочки и засунуть их себе в рот.  Не бойтесь упасть в воду – это поможет вам успокоиться и снять напряжение.  Если вы упадете в лужу, вам придется лечь в нее лицом.  Не бойтесь упасть в яму с говном, это поможет вам расслабиться и снять напряжение. \n",
            "Generated  274  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:40:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  275  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:41:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  276  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:41:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  277  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:41:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  278  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:41:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  279  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:41:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  280  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:42:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  281  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:42:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  282  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:42:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  283  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:42:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  284  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:42:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  285  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:42:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  286  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:43:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  287  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:43:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  288  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:43:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  289  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:43:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  290  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:43:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  291  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:44:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам лучше отказаться от  всех земных привязанностей и посвятить себя  общению с  людьми.  Вам следует избегать  запаха  чеснока и запаха курицы, а также запаха чеснока и запаха коровьего дерьма. Не ешьте слишком много чеснока, он может вызвать проблемы с психикой. Не пейте слишком много кофе.  Не ешьте много соли и сахара.\n",
            "Generated  291  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:44:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  292  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:44:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  293  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:44:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  294  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:44:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  295  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:44:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  296  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:45:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  297  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:45:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  298  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:45:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  299  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:45:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  300  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:45:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  301  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:45:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  302  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:46:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  303  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:46:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  304  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:46:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  305  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:46:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  306  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:46:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  307  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:47:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  308  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:47:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  309  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:47:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  310  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:47:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  311  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:47:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  312  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:47:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  313  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:48:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  314  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:48:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  315  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:48:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  316  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:48:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  317  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:48:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  318  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:49:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  319  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:49:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  320  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:49:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  321  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:49:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  322  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:49:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  323  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:49:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  324  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:50:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  325  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:50:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  326  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:50:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам следует быть осторожными с  костями.  Если вы обнаружите, что ваши руки испачканы в яме с дерьмом, то вам следует немедленно покинуть дом.  Если вам придется драться с кем-то из прохожих, то не стоит этого делать.\n",
            "Generated  326  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:50:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  327  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:50:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  328  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:51:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  329  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:51:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  330  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:51:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  331  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:51:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  332  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:51:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  333  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:51:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  334  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:52:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  335  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:52:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  336  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:52:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  337  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:52:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  338  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:52:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  339  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:53:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  340  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:53:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня у вас есть шанс стать самым большим идиотом в мире, если вы будете смотреть порно со стены. В этом вам поможет зеркало. Вы можете увидеть в зеркале свое лицо. Возможно, у вас появится новый орган. Сегодня вы можете увидеть как свиньи падают с неба. Не стоит расстраиваться из-за ерунды.\n",
            "Generated  340  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:53:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  341  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:53:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  342  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:53:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  343  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:53:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  344  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:54:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  345  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:54:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  346  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:54:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  347  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:54:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете ощутить прилив сил и энергии. В этот день можно смело отправляться на прогулку с собакой. Не бойтесь упасть в лужу или в лужу с дерьмом. Вечером вы можете обнаружить себя на полу в гостиной. В этот день вы можете стать начальником поезда или верблюда. Не бойтесь упасть в лужу.\n",
            "Generated  347  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:54:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  348  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:55:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  349  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:55:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  350  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:55:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  351  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:55:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  352  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:55:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  353  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:55:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  354  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:56:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  355  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:56:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  356  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:56:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  357  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:56:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  358  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:56:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  359  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:57:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  360  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:57:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  361  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:57:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  362  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:57:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  363  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:57:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  364  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:57:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  365  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:58:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  366  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:58:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  367  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:58:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  368  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:58:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  369  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:58:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  370  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:59:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  371  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:59:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  372  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:59:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  373  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:59:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  374  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:59:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  375  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 09:59:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  вам стоит быть внимательнее, если вы обнаружите, что вас окружает множество паразитов, а ваши гениталии покрыты толстым слоем пыли.  Не бойтесь того, что ваши гениталии покрыты толстым слоем пыли. Вы можете быть пойманы в грязной игре, и ваши гениталии испачканы в яме с дерьмом. Не бойтесь того, что ваши гениталии покрыты толстым слоем пыли.  Не бойтесь того, что ваши гениталии покрыты толстым слоем дерьма. Не бойтесь того, что ваши гениталии покрыты толстым слоем пыли.\n",
            "Generated  375  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:00:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  376  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:00:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  377  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:00:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  378  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:00:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  379  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:00:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  380  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:00:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Если вы обнаружите, что ваши глаза опухли и вас стошнило, то вам следует принять обезболивающие. Не бойтесь ходить с опущенной головой. Если вам попадется какой-то старый пидор, то вам нужно бежать к ближайшему полицейскому, чтобы выяснить причину его визита.\n",
            "Generated  380  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:01:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  381  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:01:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  382  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:01:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  383  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:01:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  384  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:01:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  385  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:02:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  386  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:02:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  387  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:02:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Если вы хотите научиться прыгать с пятого этажа на девятый, вам следует купить себе билет на поезд до Калькутты, откуда вы можете уехать жить в Лос-Анджелес. Не бойтесь быть раздавленным насекомым. Сегодня звезды советуют вам не есть дерьмо. \n",
            "Generated  387  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:02:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  388  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:02:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  389  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:02:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  390  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:03:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  391  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:03:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  392  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:03:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  393  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:03:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  394  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:03:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  395  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:04:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  396  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:04:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  397  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:04:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  398  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:04:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  399  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:04:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  400  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:04:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  401  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:05:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  402  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:05:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  403  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:05:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  404  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:05:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  405  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:05:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  406  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:06:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  407  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:06:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  408  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:06:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  409  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:06:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  410  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:06:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  411  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:06:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  412  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:07:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  413  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:07:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  414  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:07:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  415  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:07:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  416  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:07:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  417  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:08:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  418  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:08:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  419  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:08:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  420  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:08:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Не бойтесь быть пойманным в  водоворот грязи. Не бойтесь  быть укушенным.  Не бойтесь быть укушенным  или выебанным.  Будьте осторожны.  Будьте осторожны.  Будьте осторожны.  Будьте осторожны. Будьте осторожны. Не бойтесь.  Будьте осторожны. Будьте осторожны. Будьте осторожны. Будьте осторожны. Будьте осторожны. Будьте осторожны. Не бойтесь. Не позволяйте своему гневу захлестнуть вас. Не позволяйте своему гневу захлестнуть вас.\n",
            "Generated  420  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:08:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  421  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:08:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  422  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:09:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  423  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:09:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  424  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:09:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  425  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:09:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  426  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:09:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  427  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:10:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  428  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:10:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  429  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:10:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  430  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:10:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  431  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:10:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  432  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:10:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  433  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:11:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  434  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:11:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  435  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:11:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  436  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:11:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  437  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:11:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  438  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:12:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  439  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:12:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  440  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:12:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  441  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:12:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  442  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:12:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  443  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:12:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  444  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:13:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  445  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:13:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  446  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:13:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  447  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:13:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы должны быть готовы к тому, что ваш рот будет полон пшена. Вы должны помнить, что вы не просто кусок пирога, а настоящая птица, которая будет летать, как мопс. Сегодня вам следует вести себя так, как будто вы просто ебанулись. Сегодня вы должны быть осторожны с грызунами, особенно с пауками. Не стоит использовать магию, чтобы остановить их.\n",
            "Generated  447  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:13:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  448  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:13:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  449  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:14:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  450  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:14:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  451  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:14:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  452  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:14:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  453  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:14:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  454  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:15:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  455  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:15:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  456  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:15:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  457  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:15:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  458  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:15:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  459  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:15:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  460  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:16:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  461  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:16:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  462  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:16:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  463  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:16:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  464  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:16:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  465  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:17:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  466  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:17:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  467  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:17:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  468  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:17:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  469  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:17:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  470  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:17:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете стать свидетелем того, как свиньи падают в яму с дерьмом и исчезают. Возможно, вы встретите старого крокодила, маленького зайчика и маленькую белую лошадку. Будьте очень осторожны, чтобы не упасть в яму с дерьмом. \n",
            "Generated  470  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:18:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  471  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:18:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  472  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:18:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  473  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:18:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  474  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:18:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  475  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:19:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  476  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:19:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Если вы видите на дороге мусор, то не бойтесь его выбросить. Не бойтесь идти по улице в грязной рубашке, потому что вы можете обнаружить, что ваши штаны испачканы в дерьме. Вы можете встретить кого-то, кто пахнет рыбой и молоком, и он расскажет вам, что вы нашли клад в огороде.\n",
            "Generated  476  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:19:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  477  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:19:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  478  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:19:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  479  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:19:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  480  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:20:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  481  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:20:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  482  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:20:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  483  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:20:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  484  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:20:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  485  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:21:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  486  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:21:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  487  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:21:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  488  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:21:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  489  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:21:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня ваш рот будет полон слюны и вы не сможете ее проглотить.  Вы можете начать ощущать боль и дискомфорт в области горла, которая расположена между пупком и горлом.  Не бойтесь упасть в лужу и сломать себе нос или ухо. Не бойтесь упасть в яму с дерьмом.\n",
            "Generated  489  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:21:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  490  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:22:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  491  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:22:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  492  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:22:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  493  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:22:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  494  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:22:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  495  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:23:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  496  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:23:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  497  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:23:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  498  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:23:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  499  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:23:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  500  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:23:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  501  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:24:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  502  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:24:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  503  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:24:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  504  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:24:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  505  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:24:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  506  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:25:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  507  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:25:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  508  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:25:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня у вас есть шанс стать начальником поезда или даже главнокомандующим армией мира. Возможно, вы проведете ночь с каким-нибудь мужчиной, который будет вас целовать и говорить «Madame Cherlon». Будьте внимательны к тому, как вас ведут. Вечером попытайтесь найти недостающие зубы и постарайтесь не наступать на них.\n",
            "Generated  508  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:25:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  509  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:25:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  510  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:25:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  511  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:26:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  512  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:26:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  513  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:26:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  514  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:26:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете стать свидетелем того, как ваши руки будут бегать по столу. Не стоит расстраиваться. Сегодня можно найти много интересного: например, вы сможете увидеть гигантскую черепаху и увидеть, как ваши зубы будут скреститься. Сегодня вы должны быть в состоянии мобилизации всех своих ресурсов и держать рот на замке. Вечером рекомендуется посетить кафедральный храм Ганеши и посетить кафедральный храм Ганеши. \n",
            "Generated  514  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:26:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  515  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:27:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  516  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:27:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  517  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:27:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете быть пойманы в  грязной драке с  быками, а также в битве при Эскобарбаде и на кладбище.  Если вы видите, что кто-то мажет вам лицо керосином, немедленно звоните в ближайший бар.  Если вы видите, что ваши волосы прилипла к лицу, немедленно звоните в ближайший реабилитационный центр.  Если вы видите, что ваши ноги испачканы в дерьме, немедленно звоните в ближайший реабилитационный центр.\n",
            "Generated  517  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:27:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  518  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:27:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  519  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:27:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  520  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:28:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  521  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:28:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  522  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:28:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  523  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:28:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день вы будете находиться в состоянии невесомости. Возможно, вам придется прыгать с лошади. Вы будете находиться в состоянии невесомости, но это уже совсем другая история. Будьте очень осторожны. Вы можете быть пойманы в  краже яиц из коровьего дерьма или же просто забыты.  Не бойтесь  упасть в лужу. \n",
            "Generated  523  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:28:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  524  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:29:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  525  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:29:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  526  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:29:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  527  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:29:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  528  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:29:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  529  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:29:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  530  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:30:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  531  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:30:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  532  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:30:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  533  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:30:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  534  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:30:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  535  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:31:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  536  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:31:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  537  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:31:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  538  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:31:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  539  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:31:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  540  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:31:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  541  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:32:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  542  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:32:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  543  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:32:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  544  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:32:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  545  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:32:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  546  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:33:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  547  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:33:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  548  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:33:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  549  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:33:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  550  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:33:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  551  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:33:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  552  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:34:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  553  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:34:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  554  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:34:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  555  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:34:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  556  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:34:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  557  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:34:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  558  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:35:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  559  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:35:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  560  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:35:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  561  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:35:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  562  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:35:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  563  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:36:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  564  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:36:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  565  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:36:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  566  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:36:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  567  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:36:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  568  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:36:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  569  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:37:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  570  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:37:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  571  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:37:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете использовать свои органы чувств для того, чтобы кричать: \"Я хочу жрать дерьмо, говно и дерьмо. Это лучший способ защитить свою душу и тело от злых духов и злых помыслов.\n",
            "Generated  571  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:37:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  572  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:37:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  573  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:38:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  574  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:38:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  575  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:38:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  576  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:38:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  577  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:38:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  578  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:38:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  579  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:39:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  580  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:39:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  581  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:39:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  582  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:39:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  583  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:39:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  584  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:40:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  585  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:40:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  586  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:40:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  587  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:40:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  588  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:40:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  589  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:40:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  590  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:41:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  591  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:41:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  592  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:41:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  593  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:41:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  594  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:41:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  595  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:42:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  596  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:42:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  597  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:42:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  598  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:42:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  599  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:42:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  600  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:42:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  601  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:43:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  602  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:43:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам стоит учесть  и то, что вы можете стать свидетелем  того, как свиньи падают в яму с дерьмом.  Если у вас есть какие-то сомнения относительно происходящего, не спешите их принимать – это опасно!  Сегодня вы можете столкнуться с  настоящим  крокодилом, который  будет пытаться укусить вас за ногу.  Если вы видите свинью на земле, немедленно бегите к ней и немедленно убирайтесь из дома. Не позволяйте Луне быть в вашей крови\n",
            "Generated  602  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:43:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  603  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:43:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  604  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:43:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  605  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:44:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  606  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:44:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  607  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:44:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  608  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:44:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  609  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:44:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  610  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:44:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете стать частью стаи летучих мышей. Не бойтесь упасть в лужу или в лужу с дерьмом, которая будет пахнуть рыбой или мочой.  Если вы упадете, вам не будет угрожать никакая тварь, кроме вашей собственной. Вы можете быть атакованы толпой птиц, которая попытается укусить вас за руку, если вы упадете. Не бойтесь упасть в лужу или в лужу с дерьмом.\n",
            "Generated  610  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:45:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  611  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:45:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  612  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:45:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  613  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:45:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  614  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:45:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  615  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:46:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  616  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:46:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  617  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:46:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам не стоит беспокоиться о  своем внешнем виде.  Они не станут  надевать на вас одежду, которая была бы вам неприятна, если бы вы  ею не злоупотребляли.  Это будет выглядеть как  нападение со стороны невидимой руки.  В самом начале вечера вы должны быть в костюме рыбки и держать себя в руках. \n",
            "Generated  617  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:46:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  618  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:46:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  619  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:46:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  620  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:47:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  621  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:47:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  622  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:47:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  623  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:47:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  624  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:47:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  625  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:48:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  626  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:48:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  627  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:48:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  628  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:48:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  629  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:48:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  630  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:48:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  631  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:49:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  632  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:49:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  633  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:49:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  634  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:49:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  635  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:49:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  636  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:50:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  637  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:50:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  638  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:50:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  639  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:50:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  640  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:50:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  641  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:50:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  642  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:51:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  643  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:51:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  644  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:51:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  645  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:51:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  646  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:51:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  647  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:52:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  648  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:52:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  649  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:52:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  650  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:52:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день вам не следует делать резких движений, чтобы не упасть в яму с дерьмом. В этот день можно использовать магию для того, чтобы очистить кишечник от примеси кошачьей мочи. В этот день вы можете найти свое отражение в фонтане, а затем нырнуть в него головой\n",
            "Generated  650  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:52:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  651  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:52:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  652  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:53:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  653  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:53:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Не исключено, что сегодня вам придется  надеть костюм клоуна и стать самим собой. Не бойтесь упасть в лужу или в лужу с дерьмом. Сегодня вам стоит отложить  свои дела и заняться  чем-нибудь другим. Например, сходить на пляж. \n",
            "Generated  653  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:53:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  654  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:53:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  655  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:53:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  656  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:53:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  657  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:54:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  658  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:54:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  659  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:54:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам следует остерегаться  воров в законе.  Они могут  попытаться  украсть вашу одежду, украсть ваши вещи, украсть вашу обувь и т.  д. Не бойтесь  быть раздавленным этими потоками дерьма и грязи. Сегодня вам не следует беспокоиться о  том, что подумают о вас в округе.  Если вы обнаружите себя спящими в луже дерьма или лужицах, смело используйте это время для сна. \n",
            "Generated  659  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:54:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  660  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:54:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  661  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:55:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  662  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:55:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  663  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:55:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  664  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:55:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  665  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:55:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  666  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:55:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  667  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:56:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  668  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:56:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  669  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:56:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  670  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:56:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  671  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:56:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  672  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:57:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  673  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:57:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  674  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:57:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  675  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:57:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  676  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:57:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  677  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:57:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  678  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:58:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  679  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:58:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  680  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:58:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  681  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:58:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  вам стоит помнить, что  все, что вы видите, — это просто куча дерьма. Не надо делать слишком поспешных выводов.  Лучше  подождать, пока ситуация изменится к лучшему. Если все пойдет хорошо, вы сможете снова увидеть солнце в крапиве и почувствовать тепло на коже. Если все идет хорошо, вы сможете снова увидеть солнце в крапиве\n",
            "Generated  681  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:58:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  682  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:59:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  683  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:59:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  вам лучше всего начать день с  молитвы.  Но помните о том, что это не лучший момент для  чего-то большего. Не забывайте о  том, что  вы – часть стаи слонов.  Не бойтесь  упасть в  лужу  или  в яму с дерьмом\n",
            "Generated  683  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:59:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  684  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:59:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  685  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:59:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  686  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 10:59:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  687  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:00:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  688  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:00:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  689  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:00:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  690  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:00:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  691  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:00:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня у вас будет много хлопот: вы будете есть грибы, пить чай и разговаривать с волком. В этот день вы можете столкнуться с проблемами, такими как краб, лягушка и т. д. Вы можете встретить в лесу пьяного олигарха, а может быть и волка. Не бойтесь делать резкие движения головой, чтобы не попасть в беду\n",
            "Generated  691  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:01:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  692  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:01:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  693  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:01:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  694  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:01:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  695  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:01:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  696  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:01:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  697  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:02:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  698  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:02:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам следует быть внимательнее при выборе одежды. Возможно, придется пожертвовать некоторыми из своих навыков, например, научиться мастурбировать. В этот день вы можете стать начальником полиции и даже главнокомандующим морскими конными войсками. \n",
            "Generated  698  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:02:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  699  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:02:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  700  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:02:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  701  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:03:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  702  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:03:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  703  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:03:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  704  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:03:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  705  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:03:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  706  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:03:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  707  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:04:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  708  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:04:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  709  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:04:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  710  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:04:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  711  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:04:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  712  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:05:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  713  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:05:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  714  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:05:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  715  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:05:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  716  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:05:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  717  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:05:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  718  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:06:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  719  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:06:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  720  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:06:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  721  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:06:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  722  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:06:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  723  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:07:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  724  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:07:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  725  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:07:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  726  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:07:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  727  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:07:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  вам не следует бояться  быть укушенным крысами. В этот день вам следует опасаться  быть укушенным крысами, поэтому не ешьте корм, содержащий молоко или мясо.  Сегодня вам лучше не участвовать в общественной жизни и не вступать в брак. Вы можете быть подвержены нападению со стороны людей с острыми усиками. Не бойтесь упасть в яму с дерьмом или в лужу с дерьмом. Вы можете стать жертвой сексуального насилия со стороны других людей. Не бойтесь влюбиться в кого-нибудь еще.  Не бойтесь влюбиться в кого-нибудь еще.\n",
            "Generated  727  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:07:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  728  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:08:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  729  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:08:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  730  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:08:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  731  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:08:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  732  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:08:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  733  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:09:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  734  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:09:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  735  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:09:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  736  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:09:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  737  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:09:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день можно использовать свои ноги, чтобы перепрыгивать через мусорные баки и бить стекла в общественных туалетах. В это время опасно открывать рот и говорить. Возможны проблемы со спинами, возможны проблемы со сном, возможны проблемы с психикой. Не забывайте следить за тем, чтобы ваш рот был полон слюны и вы были готовы порваться. \n",
            "Generated  737  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:09:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  738  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:10:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  739  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:10:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  740  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:10:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  741  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:10:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  742  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:10:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  743  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:11:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  744  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:11:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  745  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:11:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  746  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:11:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  747  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:11:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  748  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:11:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  749  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:12:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  750  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:12:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  751  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:12:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  752  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:12:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  753  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:12:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  754  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:13:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  755  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:13:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Не бойтесь упасть в яму с дерьмом.  Вы можете стать свидетелем того, как ваши ноги начнут дрожать. Сегодня вы можете стать свидетелем того, как ваши волосы упадут вам на лицо и вам придется ходить босиком. Вы можете стать свидетелем того, как ваши зубы начнут крошиться. Вы можете стать свидетелем того, как ваши ноги начнут крошиться\n",
            "Generated  755  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:13:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  756  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:13:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  757  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:13:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  758  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:13:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  759  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:14:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  760  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:14:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  761  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:14:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  762  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:14:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  763  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:14:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  764  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:14:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  765  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:15:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  766  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:15:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  767  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:15:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  768  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:15:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня у вас есть возможность познакомиться с  настоящим анусом, размером со сливу. Вы можете наслаждаться вкусом своего пота и запахом своих гениталий. Не бойтесь упасть в лужу с дерьмом или пропасть с пауком. Не бойтесь упасть в колодец с дерьмом и пауком\n",
            "Generated  768  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:15:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  769  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:16:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  770  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:16:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  771  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:16:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  772  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:16:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  773  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:16:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  774  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:16:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  775  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:17:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  776  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:17:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  777  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:17:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  778  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:17:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  779  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:17:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  780  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:18:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  781  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:18:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  782  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:18:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  783  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:18:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  784  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:18:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  785  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:18:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  786  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:19:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  787  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:19:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  788  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:19:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  789  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:19:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  790  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:19:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  791  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:20:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В этот день у вас может появиться новая сексуальная партнерша, похожая на дедушку Ленина, и вы будете с ней играть. В это время вы будете окружены толпой идиотов, которые будут орать на вас и мазать вас дерьмом.\n",
            "Generated  791  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:20:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  792  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:20:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  793  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:20:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  794  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:20:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  795  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:20:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  796  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:21:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  797  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:21:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  798  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:21:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  799  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:21:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  800  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:21:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  801  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:22:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  802  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:22:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  803  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:22:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня звезды советуют вам не прыгать с высокого небоскреба на землю. Не бойтесь упасть и начать бить себя головой в грудь. Если вы этого не сделаете, то все ваши проблемы будут решены. \n",
            "Generated  803  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:22:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  804  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:22:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  805  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:22:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  806  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:23:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  807  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:23:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  808  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:23:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  809  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:23:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  810  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:23:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  811  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:24:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  812  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:24:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  813  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:24:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  814  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:24:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  815  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:24:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  816  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:24:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  817  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:25:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  818  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:25:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  819  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:25:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  820  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:25:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  821  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:25:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  822  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:26:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  823  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:26:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  824  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:26:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  825  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:26:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  826  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:26:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  827  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:26:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  828  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:27:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  829  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:27:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  830  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:27:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  831  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:27:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  832  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:27:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  833  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:28:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  834  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:28:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  835  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:28:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  836  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:28:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Не бойтесь делать резкие выпады, особенно когда вы находитесь на открытом пространстве. Не бойтесь быть застигнутым врасплох и начать кричать на стеклянные двери, если увидите на них кого-то из знакомых. Сегодня вам не стоит тратить время на попытки убедить себя, что вы действительно ебанулись. \n",
            "Generated  836  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:28:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  837  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:28:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  838  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:29:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  839  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:29:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  840  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:29:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  841  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:29:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  842  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:29:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  843  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:30:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  844  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:30:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  845  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:30:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  846  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:30:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  847  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:30:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  848  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:30:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  849  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:31:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  850  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:31:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  вам стоит быть осторожными, потому что в этот день могут прийти злые духи, и вам придется пройти через них в виде ящерицы. Вы можете стать невидимым для своих друзей и врагов, но не забывайте следить за своим языком. Если вас укусил комар, немедленно выньте его из головы\n",
            "Generated  850  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:31:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  851  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:31:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  852  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:31:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  853  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:32:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  854  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:32:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  855  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:32:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  856  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:32:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  857  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:32:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня звезды советуют вам не  прыгать через забор, потому что он  может вас укусит.  Но если вы  этого не сделаете, то можете  упасть в яму с дерьмом и утонуть в ней.  Будьте осторожны с туалетом, он может вас укусить. Не бойтесь того момента, когда ваш нос будет покрыт слизью.\n",
            "Generated  857  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:32:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  858  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:33:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  859  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:33:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  860  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:33:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  861  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:33:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  862  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:33:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  863  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:34:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  864  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:34:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  865  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:34:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  866  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:34:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  867  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:34:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  868  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:34:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  869  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:35:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  870  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:35:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  871  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:35:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  872  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:35:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  873  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:35:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  874  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:36:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  875  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:36:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  876  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:36:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  877  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:36:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  878  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:36:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  879  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:36:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  880  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:37:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  881  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:37:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  882  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:37:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  883  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:37:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  884  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:37:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  885  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:38:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Если вы хотите выглядеть старым, вы должны выглядеть как старый волк. Не бойтесь идти по улице с грязными ногами и старыми волосами, чтобы не попасть в лапы пьяных охранников. Вы можете быть пойманы в краже фруктов, а потом вам грозит смертельное оскорбление. Не бойтесь упасть в яму с дерьмом или в мусорное ведро. Вечером вы можете встретить двух или трех старых людей с ведрами и тряпками. Вы также можете встретить двух или трех старых людей с длинными черными волосами и большими голубыми глазами, которые будут смотреть на вас с любовью и восхищением, как родные сестры\n",
            "Generated  885  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:38:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  886  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:38:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  887  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:38:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  888  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:38:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  889  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:38:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  890  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:39:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  891  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:39:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  892  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:39:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  893  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:39:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете ощутить на себе действие невидимой руки и  ног.  Не бойтесь упасть и  удариться головой, если это необходимо. Не бойтесь  упасть и удариться о  землю головой. Не бойтесь упасть в яму с дерьмом. Не бойтесь  упасть и удариться о землю головой. Не бойтесь  лечь на пол и заснуть. \n",
            "Generated  893  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:39:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  894  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:40:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам следует остерегаться не только запаха чеснока и коровьего дерьма, но и запаха Вы.  В первой половине дня вы можете быть подвержены нападению зомби или крокодилов.  Вы можете быть подвержены нападению насекомых или людей. В это время возможны небольшие землетрясения и извержения вулканов.\n",
            "Generated  894  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:40:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  895  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:40:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  896  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:40:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  897  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:40:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  898  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:40:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  899  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:41:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  900  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:41:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вам не стоит беспокоиться о своей внешности.  Скорее всего вы встретите кого-то вроде капитана космического корабля, который расскажет вам много интересного.  Вы можете стать начальником поезда, который привезет вас в Ад.  В этом случае вы будете очень удивлены. \n",
            "Generated  900  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:41:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  901  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:41:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  902  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:41:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  903  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:41:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  904  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:42:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  905  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:42:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  906  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:42:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  907  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:42:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  908  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:42:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  909  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:43:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  910  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:43:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  911  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:43:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  912  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:43:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  913  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:43:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  914  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:43:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  915  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:44:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  916  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:44:21 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  917  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:44:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  В начале дня не стоит беспокоиться о  своей внешности – вы можете  выглядеть на все сто.  Сегодня вам следует  отказаться от  привычки  жрать дерьмо, которое пахнет рыбой. Не бойтесь  быть самим собой. Сегодня вам следует  избегать употребления сырого мяса. Не бойтесь  быть самим собой.\n",
            "Generated  917  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:44:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  918  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:44:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  919  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:45:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  920  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:45:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  921  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:45:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  922  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:45:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  923  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:45:48 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Если у вас есть проблемы с психикой, не бойтесь их решить. Не бойтесь того дня, когда вы окажетесь в ловушке без кислорода. Не бойтесь того дня, когда ваш мозг взорвется, как у дирижабля.\n",
            "Generated  923  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:45:59 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете быть поражены внезапностью и неожиданностью. Вы можете увидеть в зеркале свое лицо. Сегодня вам следует избегать контактов с людьми с острыми  и тупыми концами. Сегодня вам следует вести себя словно вы не ебанулись. Не стоит расстраиваться. Вы можете наслаждаться жизнью. \n",
            "Generated  923  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:46:10 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  924  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:46:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  925  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:46:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  926  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:46:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  927  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:46:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  928  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:47:04 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  929  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:47:15 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  930  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:47:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  931  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:47:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  932  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:47:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  933  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:47:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  934  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:48:09 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  935  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:48:20 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  936  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:48:31 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  937  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:48:42 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  938  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:48:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  939  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:49:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете стать свидетелем того, как ваши свиньи падают с неба на землю. Будьте очень осторожны с ними – они могут вас укусить. Вечером вы можете встретить дракона в облике жабы и серу в облике кузнечика, или серу в облике кузнечика. Будьте осторожны с лошадью, она может вас укусить\n",
            "Generated  939  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:49:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  940  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:49:25 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  941  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:49:36 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  942  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:49:47 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  943  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:49:58 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  944  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:50:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  945  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:50:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  946  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:50:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  947  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:50:41 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  948  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:50:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  949  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:51:03 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  950  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:51:14 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  951  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:51:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  952  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:51:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  953  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:51:46 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  954  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:51:57 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  955  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:52:08 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  956  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:52:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  957  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:52:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  958  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:52:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  959  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:52:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  960  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:53:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  961  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:53:13 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  962  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:53:24 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  963  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:53:35 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня у Весов праздник – день рождения. Не упускайте возможности проявить свою  изобретательность и  проявить  свой талант к  танцам с  медведями.  Сегодня вы можете стать частью стаи гусей или даже стаи оленей.  Не бойтесь  упасть в яму с дерьмом\n",
            "Generated  963  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:53:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  964  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:53:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  965  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:54:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  966  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:54:18 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  967  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:54:29 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  968  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:54:40 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  969  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:54:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  970  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:55:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  971  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:55:12 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  972  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:55:23 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  973  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:55:34 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  974  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:55:45 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Не бойтесь упасть в яму с дерьмом и начать бить хвостом. Не бойтесь упасть в лужу с дерьмом, и тогда вас постигнет разочарование.\n",
            "Generated  974  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:55:56 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  975  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:56:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  976  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:56:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  977  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:56:28 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  978  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:56:39 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  979  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:56:50 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  980  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:57:01 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  981  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:57:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  982  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:57:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered obscene sec:  Сегодня вы можете стать начальником полиции или начальником пожарной части, но не главнокомандующего флотом. Не бойтесь того, кого вы держите за горло. Вы также можете быть атакованы невидимыми для окружающих вас людьми. Будьте осторожны с детьми, животными и растениями. Сегодня вам не следует делать резких движений\n",
            "Generated  982  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:57:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  983  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:57:44 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  984  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:57:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  985  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:58:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  986  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:58:17 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  987  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:58:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  988  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:58:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  989  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:58:49 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  990  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:59:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  991  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:59:11 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  992  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:59:22 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  993  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:59:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  994  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:59:43 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  995  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 11:59:54 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  996  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 12:00:05 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  997  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 12:00:16 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  998  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 12:00:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  999  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/12/2021 12:00:38 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generated  1000  vals\n",
            "temperature:  1.0  k: 5  p:  0.95\n",
            "Generated  1001  vals\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZaHe7koy9tf",
        "outputId": "9283a8cf-8427-4ac4-e81c-5ac5de5d4e48"
      },
      "source": [
        "print(\"\\n\".join(filtered_seqs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Сегодня Близнецам лучше  не  участвовать в  турнире по метанию ножа в руках у  себя над кроватью. Звезды советуют не делать резких движений руками и ногами, потому что это приведет к тому, что вы будете покрыты толстым слоем слизи.  Это может вызвать проблемы с психикой. Не бойтесь рожать. \n",
            " Сегодня вы можете стать начальником поезда и одновременно шофером троллейбуса, который привезет вас в Москву. Не бойтесь упасть с дерева и начать мастурбировать.  Не бойтесь упасть с дерева и начать мастурбировать. Не бойтесь упасть с дерева и начать мастурбировать.\n",
            " Сегодня Близнецы легко поддаются  соблазну  быть  на коне,  как это бывало в прошлом и настоящем.  Но  сегодня они  этого не делают.  Они  просто  идут  своим обычным путем, как это было в прошлом и настоящем.\n",
            "  В течение дня Близнецы будут  выглядеть как  настоящие  мертвецы. Не бойтесь того, кто их кусает. Если  вы будете играть в  роль «двойняшек», то сможете заработать себе геморрой на всю оставшуюся жизнь. Не бойтесь  того, что вас укусят пчелы-убийцы.  Если вы будете играть роль  «двойняшек», то можете заработать себе геморрой на всю оставшуюся жизнь. \n",
            " Сегодня Близнецы рискуют быть выброшенными в космос. Возможны серьезные последствия. Например, вы можете оказаться в состоянии невесомости. Вы можете стать свидетелем появления большого белого волка в вашей квартире. Вам не следует делать резких движений. Вы можете встретить в лесу кого-то, кто похож на вас.\n",
            "  Сегодня вам предстоит стать начальником поезда, который отправится в Калькутту. Сегодня вы можете стать начальником поезда, который отправится в Калькутту. Вы можете стать начальником поезда, который отправится в Калькутту. Сегодня вы можете стать начальником поезда, который отправится в Калькутту. Вы можете стать начальником поезда, который отправится в Калькутту. Вы можете стать начальником поезда, который отправится в Калькутту. Вы можете стать начальником поезда, который отправится в Калькутту. Вы можете стать начальником поезда, который отправится в Калькутту. Вы можете стать начальником поезда, который отправится в Калькутту. Вы можете стать начальником поезда, который отправится в Калькутту. Не бойтесь быть собакой. Не бойтесь быть собакой. Не бойтесь быть собакой.\n",
            " Сегодня Близнецам не стоит бояться быть  злыми из-за своих  дурацких  дурацких  костюмов. Если они хотят быть злыми, они их добиваются. Если они хотят быть добрыми, они их добиваются. В этом случае их цель – не дать себя в обиду, а сломать их изнутри. \n",
            " Сегодня Близнецы склонны к  самобичеванию, так как  их  раздражает  любое действие, которое  вызывает у них чувство  вины. Не надо  говорить  о  том, что  они делают, – это может  причинить им боль.  Лучше просто наслаждаться жизнью и быть счастливым с кем-то другим.  Например, с  собакой».  Этот день хорош для  родов, но не для зачатия.  Не стоит обольщаться и думать, что  вы можете родить ребенка с плохим характером.  Это будет просто смешно\n",
            " Сегодня Близнецы могут рассчитывать на успех в самых разных сферах жизни, в том числе в области  образования и финансов.  Это зависит от обстоятельств и вашей активности.  В этот день можно  рассчитывать на  удачу в  различных сферах жизни и  в различных сферах жизни.  Если вы чувствуете  себя плохо, то  лучше  не пытаться  лечить  себя сам\n",
            " В этот день можно встретить и козлов с горящими глазами.  Будьте осторожны, чтобы не упасть на людей. Не позволяйте стрекотать крысам.  Не позволяйте стрекотать крысам.  Не позволяйте стрекотать крысам.  Не позволяйте стрекотать крысам. Не позволявайте стрекотать крысам. Не позволяйте стрекотать крысам.  Не позволяйте стрекотать крысам\n",
            " Звезды советуют вам не пить много кофе и не курить. Не следует использовать косметические процедуры для лица.  В этот день можно встретить молодого кота, собаку, ящерицу, крокодила, ящерицу с хвостом. Не ешьте в эти сутки сырого картофеля, он может вызвать у вас рвоту.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAv7_pz75N8D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LzhUyO65CVL"
      },
      "source": [
        "from shutil import copyfile\n",
        "copyfile('gem', dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhUgg_NOgQY"
      },
      "source": [
        "! cp {file_name} /content/drive/MyDrive/neurohoroscope/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71hRcveYTBjP",
        "outputId": "aa8d59fe-17b8-4be8-d3c3-49391316a662"
      },
      "source": [
        "generated_sequences = get_generated_sequences(temperature=2.0)\n",
        "for seq in generated_sequences:\n",
        "  print(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/18/2021 21:12:00 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "temperature:  2.0  k: 5  p:  0.95\n",
            "Если у вас есть грудь или ноги — положите их в рот, а если они укусят вас, закройте рот и сделайте то же самое с ногами. Вы должны помнить о том, что ваш рот полон молока. Сегодня звезды советуют вам не пить уксусу, поскольку он может причинить вам вред. \n",
            "В этот день вы можете использовать любую возможность для того, чтобы сделать что-нибудь глупое или неприятное.  Например, если ваш котенок споткнется и упадет в лужу, вам следует надеть вместо шубы меховой жилет из меха.  Это хорошее предзнаменование. Не бойтесь того дня, когда вы станете похожи на маленького крокодила.\n",
            "В этот день можно встретить двух или даже пяти драконов. В первой половине дня вы будете испытывать трудности с письмом. Если вы решитесь на это сегодня, вы сможете найти выход из сложной ситуации.\n",
            "Не бойтесь идти танцевать с белым медведем и кричать на него: \"Mad Substance, you're right, I'm right.\n",
            "вам сегодня не до раздумий о судьбе России и Украины.  Они хотят жить, но пока им не хватает ума понять что такое хорошо и плохо и что такое плохо. А это требует от них огромного таланта, чтобы понять, чем плохо. В этом и состоит главная их ошибка.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuEmAVMETC62",
        "outputId": "4e6fb7f8-5168-45ae-f3c9-e327258ceea3"
      },
      "source": [
        "generated_sequences = get_generated_sequences(temperature=0.5)\n",
        "for seq in generated_sequences:\n",
        "  print(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/18/2021 21:12:06 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "temperature:  0.5  k: 5  p:  0.95\n",
            "Сегодня вам стоит учесть, что в этот день у вас могут появиться проблемы с психикой.  В этот день вам следует быть осторожным с грызунами и кошками.  Не бойтесь того дня, когда ваши ноги могут выпасть из поезда\n",
            "В этот день вы можете стать начальником полиции или мэром. Вы можете стать мэром Лондона или даже Парижа, но не будьте мэром Лондона или Парижа. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или даже Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлина или Берлина. Вы можете стать мэром Берлин\n",
            "Сегодня у вас может появиться искушение  изнасиловать кого-нибудь из прихожан.  Не бойтесь.  Если вы этого не сделаете, вы можете  попасть в  ловушку дьявола.  Если вы этого не сделаете, вы можете попасть в  ловушку дьявола.  Если вы этого не сделаете, вы можете попасть в ловушку дьявола.  Если вы этого не сделаете, вы можете попасть в ловушку дьявола.\n",
            "Сегодня вы можете стать свидетелем того, как ваши ноги начнут бегать быстрее. Возможно, вам придется прыгать со второго этажа. Будьте осторожны с туалетом. Сегодня вы можете встретить молодого человека с длинными черными волосами и голубыми глазами. Будьте осторожны с туалетом. Будьте осторожны с едой. Сегодня вы можете встретить молодого человека с длинными черными волосами и голубыми глазами. Будьте осторожны с туалетом. Будьте осторожны с едой. Будьте осторожны с туалетом. Будьте осторожны с едой. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с едой. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьте осторожны с туалетом. Будьт\n",
            "Сегодня вам не стоит беспокоиться о том, что они будут орать на вас и бить вас ногами. Возможно, вам придется пойти на корм волку. В этот день вам следует избегать контакта с людьми и животными, особенно с собаками. Вечером рекомендуется сходить в лес и попробовать на вкус чью-то плоть. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf0ftLwTLUtV",
        "outputId": "a956e56d-f8a0-47d9-b761-e8e25ffb1e47"
      },
      "source": [
        "! du --block-size=m ./neurohoroscope_model_wt_sign_0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1629M\t./neurohoroscope_model_wt_sign_0/checkpoint-1000\n",
            "1629M\t./neurohoroscope_model_wt_sign_0/checkpoint-500\n",
            "1629M\t./neurohoroscope_model_wt_sign_0/checkpoint-1500\n",
            "5558M\t./neurohoroscope_model_wt_sign_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xu59rM6Nnto"
      },
      "source": [
        "#! rm -rf ./neurohoroscope_model/checkpoint-500\n",
        "#! rm -rf ./neurohoroscope_model/checkpoint-1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LaghuqbNgSS"
      },
      "source": [
        "# archive model\n",
        "#import shutil\n",
        "#shutil.make_archive('neurohoroscope_model_wt_sign_0', 'zip', './neurohoroscope_model_wt_sign_0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuzC5SAGM9CI"
      },
      "source": [
        "##save model to drive\n",
        "#! cp ./neurohoroscope_model_wt_sign_0.zip /content/drive/MyDrive/models/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}